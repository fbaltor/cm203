{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5dcc5d19",
   "metadata": {},
   "source": [
    "**Instituto Tecnológico de Aeronáutica – ITA**\n",
    "\n",
    "**Visão Computacional - CM-203**\n",
    "\n",
    "**Professores:** \n",
    "\n",
    "Marcos Ricardo Omena de Albuquerque Maximo\n",
    "\n",
    "Gabriel Adriano de Melo\n",
    "\n",
    "\n",
    "**Orientações padrão:**\n",
    "\n",
    "Antes de você entregar o Lab, tenha certeza de que tudo está rodando corretamente (sequencialmente): Primeiro, **reinicie o kernel** (`Runtime->Restart Runtime` no Colab ou `Kernel->Restart` no Jupyter), depois rode todas as células (`Runtime->Run All` no Colab ou `Cell->Run All` no Jupyter) e verifique que as células rodem sem erros, principalmente as de correção automática que apresentem os `assert`s.\n",
    "\n",
    "É muito importante que vocês não apaguem as células de resposta para preenchimento, isto é, as que contenham o `ESCREVA SEU CÓDIGO AQUI` ou o \"ESCREVA SUA RESPOSTA AQUI\", além das células dos `assert`, pois elas contém metadados com o id da célula para os sistemas de correção automatizada e manual. O sistema de correção automatizada executa todo o código do notebook, adicionando testes extras nas células de teste. Não tem problema vocês criarem mais células, mas não apaguem as células de correção. Mantenham a solução dentro do espaço determinado, por organização. Se por acidente acontecer de apagarem alguma célula que deveria ter a resposta, recomendo iniciar de outro notebook (ou dar um `Undo` se possível), pois não adianta recriar a célula porque perdeu o ID. Ou então você baixa e abre o notebook como texto (é um JSON) e readiciona o campo de ID. Neste ano nós também colocamos um comentário nessas células que é igual ao ID delas, para ser um failsafe em caso de sumirem com o ID das células, então NÃO apaguem esse comentário com ID (ele é um fallback caso vocês percam o ID da célula, ele deve ficar na primeira linha).\n",
    "\n",
    "Os notebooks vocês podem alterar à vontade, podem criar novas células, modificar as existentes, apagar (a menos das células de correção). O corretor automático executará todas as células e verificará a presença de erro nos `asserts`, depois haverá a correção manual das questões com apreciação da resposta e comentários gerados em HTML. Se ele não achar a célula com os asserts, fica sem a nota da questão, se ele não achar a célular com a questão, fica sem os comentários. Mas vocês podem escreve sim código fora dos espaço delemitado pelo `ESCREVA SEU CÓDIGO AQUI` sem problemas, só não altera a assinatura da função. Esse espaço foi pensado para facilitar a sua implementação.\n",
    "\n",
    "Os Notebooks foram programados para serem compatíveis com o Google Colab, instalando as dependências necessárias automaticamente a baixando os datasets necessários a cada Lab. Os comandos que se inicial por ! (ponto de exclamação) são de bash e também podem ser executados no terminal linux, que justamente instalam as dependências."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc2a98a9",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0156fd95",
   "metadata": {},
   "source": [
    "## Laboratório 3 - Neural Radiance Fields (NeRFs)\n",
    "\n",
    "Neste laboratório iremos treinar nossa própria NeRF (Neural Radiance Field). Essa é uma reprodução simplificada do trabalho de Ben Mildenhall, Pratul P. Srinivasan, Matthew Tancik, Jonathan T. Barron, Ravi Ramamoorthi e Ren Ng entitulado [Representing Scenes as Neural Radiance Fields for View Synthesis](https://www.matthewtancik.com/nerf), no qual uma rede neural é treinada para reproduzir uma cena a partir de fotos com posições conhecidas. Essencialmente a rede neural aprende a cor e a densidade (opacidade por 'comprimento') de cada ponto X, Y, Z no espaço.\n",
    "\n",
    "Dessa forma há diversos códigos na internet sobre esse assunto, e peço que não copiem. Podem assistir os vídeos e aulas do youtube a respeito, mas não copiem o código como resposta direta. O mesmo vale para as LLMs, não pergunte diretamente a resposta para elas, mas pode usá-las.\n",
    "\n",
    "Esse laboratório em 5 partes, que compreendem as operações básicas com PyTorch (motivadas pelas aulas 1 e 2); o entendimento e a estruturação do conjunto de dados a ser utilizado na NeRF; a definição da arquitetura neural; o treinamento; e a inferência. Qualquer dúvida, não hesitem de perguntar no grupo do WhatsApp. Esse deve ser um Lab bem divertido e recompensador. Eu tentei simplificar ao máximo, sem que a NeRF 'parasse de funcionar'.\n",
    "\n",
    "**ATENÇÃO**: É recomendável utilizar uma GPU Nvidia para o treinamento da NeRF neste laboratório. Habilite a GPU gratuitamente no Colab ([aula 1 slide 137](https://docs.google.com/presentation/d/1PYRArTIBh9vQ5r7DvEwGgbZp8FCkrwZnkQuFZ-z-snc)) ou utilize a imagem docker com GPU ([jupytergpu.yml](https://github.com/Gabrui/cm203))."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9be01ec6",
   "metadata": {},
   "source": [
    "**Exemplo de prompt que pode ser utilizado no ChatGPT**\n",
    "\n",
    "Pode usar LLMs desde que elas não gerem nenhum código e não falem a resposta diretamente.\n",
    "\n",
    "Por favor, tenha cuidado para não deixá-lo falar nenhum código, senão ele vai dar a resposta de cara e você não vai aprender nada ...\n",
    "\n",
    "```\n",
    "I am a computer vision graduate student and I am currently implementing the following code. Could you help me. My professor said that you shouldn't give me the answer right away, but it is okay if you guide me towards the answer so that I can discover it by myself. You should not output any code, as requested by the professor. Here is the function I need to implement (it is in portuguese but I hope you can understand):\n",
    "\n",
    "def entrada_senos_cossenos(x, escala=potencias_2) -> torch.Tensor:\n",
    "    \"\"\" Realiza a seguinte operação em um tensor de entrada:\n",
    "           x =[x1, ..., xn]\n",
    "        dado um vetor de escala de frequências\n",
    "           escala = [f1, f2, ..., fe]\n",
    "        Calcula:\n",
    "           senos = [sin(f1*x1), sin(f2*x1), ..., sin(fe*x1), ... ... ... , sin(f1*xn), sin(f2*xn), ..., sin(fe*xn)]\n",
    "           cosse = [cos(f1*x1), cos(f2*x1), ..., cos(fe*x1), ... ... ... , cos(f1*xn), cos(f2*xn), ..., cos(fe*xn)]\n",
    "        e retorna a concatenação do vetor de entrada e de seus senos e cossenos na escala de frequências recebida, nessa ordem.\n",
    "           retorna [x, senos, cossenos]\n",
    "\n",
    "      Args:\n",
    "        x (torch.Tensor): Tensor de entrada a ser operado, tem shape (..., N)\n",
    "        escala (optional, torch.Tensor): Tensor com shape (E, ) que representa os fatores de escala \n",
    "            de frequência a serem utilizados na transformação, (default: potencias_2).\n",
    "\n",
    "      Returns:\n",
    "        (torch.Tensor): Resultado da operação, tem shape (..., N + 2 * N * E)\n",
    "      \"\"\"\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f695ecd",
   "metadata": {},
   "source": [
    "## Imports e Baixar dados"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa2c0857",
   "metadata": {},
   "source": [
    "Temos as nossas bibliotecas conhecidas, já utilizadas nos labs 1 e 2, o NumPy e o Matplotlib. Além dessas temos o tqdm (*te quiero demasiado*) para mostrar uma barra de progresso durante o treino.\n",
    "\n",
    "Em seguida temos a nossa estrela desse lab: O PyTorch ! Por enquanto vamos utilizá-lo em sua forma pura, mas nos próximos lab, utilizaremos o FastAI que é tem uma interface de mais alto nível. Mas é importante saber fazer as operações de mais baixo nível no PyTorch para saber como o FastAI funciona de fato. Além disso, vocês vão perceber que o PyTorch nada mais é do que o nanoGrad que fizemos na aula passado, só que glorificado: tem muito mais operações, e consegue usar GPU eficientemente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "2e5dc47b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from typing import Callable, Tuple, List, Iterable\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "import torch\n",
    "from torch.nn.functional import sigmoid, relu, leaky_relu, softplus, pad\n",
    "\n",
    "from IPython.display import Image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13442bc7",
   "metadata": {},
   "source": [
    "Para não ter dúvidas e nem dores de cabeça de ficar trazendo tensores de CPU para GPU e vice-versa, já definimos a GPU como o dispositivo padrão para instanciarmos nossos tensores além de nosso tipo padrão float com 32 bits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "1af40cfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "torch.set_default_device(device)\n",
    "torch.set_default_dtype(torch.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "490de53c",
   "metadata": {},
   "source": [
    "Verifica se já foram baixadas os dados do drive (ou do *fallback*), baixando-as e descompactando se necessário."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "41bc2fa5",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "e31039d5dc0a91a44a8ce8a7976ec8bc",
     "grade": false,
     "grade_id": "dataset_lab",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/home/fbaltor/fbaltor/ita/2023/CM-203/cm203/.venv/bin/gdown\", line 8, in <module>\n",
      "    sys.exit(main())\n",
      "  File \"/home/fbaltor/fbaltor/ita/2023/CM-203/cm203/.venv/lib/python3.10/site-packages/gdown/cli.py\", line 156, in main\n",
      "    filename = download(\n",
      "  File \"/home/fbaltor/fbaltor/ita/2023/CM-203/cm203/.venv/lib/python3.10/site-packages/gdown/download.py\", line 275, in download\n",
      "    for file in os.listdir(osp.dirname(output) or \".\"):\n",
      "FileNotFoundError: [Errno 2] No such file or directory: '/content'\n",
      "--2023-08-30 14:36:52--  http://ia.gam.dev/cm203/23/lab03/nerfs.zip\n",
      "Resolving ia.gam.dev (ia.gam.dev)... 89.117.75.63, 2605:a143:2138:6535::1\n",
      "Connecting to ia.gam.dev (ia.gam.dev)|89.117.75.63|:80... connected.\n",
      "HTTP request sent, awaiting response... 301 Moved Permanently\n",
      "Location: https://ia.gam.dev/cm203/23/lab03/nerfs.zip [following]\n",
      "--2023-08-30 14:36:53--  https://ia.gam.dev/cm203/23/lab03/nerfs.zip\n",
      "Connecting to ia.gam.dev (ia.gam.dev)|89.117.75.63|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 4260460 (4.1M) [application/zip]\n",
      "/content: Permission denied\n",
      "/content/nerfs.zip: No such file or directory\n",
      "\n",
      "Cannot write to ‘/content/nerfs.zip’ (Success).\n",
      "[Errno 2] No such file or directory: '/content'\n",
      "/home/fbaltor/fbaltor/ita/2023/CM-203/cm203/notebooks/nerfs_lab03/content/content/content/content\n"
     ]
    }
   ],
   "source": [
    "# dataset_lab read_only\n",
    "\n",
    "! [ ! -d \"/content/nerfs\" ] && gdown -O /content/nerfs.zip \"1VCtH2tt7h8XP8yKjYCKrIx8jQ0QiaCa-\" &&  unzip -q /content/nerfs.zip -d /content && rm /content/nerfs.zip\n",
    "! [ ! -d \"/content/nerfs\" ] && wget -P /content/ \"http://ia.gam.dev/cm203/23/lab03/nerfs.zip\" &&  unzip -q /content/nerfs.zip -d /content  && rm /content/nerfs.zip\n",
    "base_path = Path(\"/content/nerfs\")\n",
    "%cd /content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "5ce8966d-fe7c-4946-ac0e-11bc8df39bdb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/fbaltor/fbaltor/ita/2023/CM-203/cm203/notebooks/nerfs_lab03/content/content/content/content\n",
      "Downloading...\n",
      "From: https://drive.google.com/uc?id=1VCtH2tt7h8XP8yKjYCKrIx8jQ0QiaCa-\n",
      "To: /home/fbaltor/fbaltor/ita/2023/CM-203/cm203/notebooks/nerfs_lab03/content/content/content/content/nerfs.zip\n",
      "100%|███████████████████████████████████████| 4.26M/4.26M [00:27<00:00, 156kB/s]\n",
      "/home/fbaltor/fbaltor/ita/2023/CM-203/cm203/notebooks/nerfs_lab03/content/content/content/content/content\n"
     ]
    }
   ],
   "source": [
    "# local data setup\n",
    "import os\n",
    "jupyter_path = os.path.abspath(\"\")\n",
    "print(jupyter_path)\n",
    "nerfs_path = jupyter_path + '/content/nerfs'\n",
    "\n",
    "! [ ! -d \"./content/nerfs\" ] && gdown -O nerfs.zip \"1VCtH2tt7h8XP8yKjYCKrIx8jQ0QiaCa-\" &&  unzip -q nerfs.zip -d ./content && rm nerfs.zip\n",
    "! [ ! -d \"./content/nerfs\" ] && wget -P ./content \"http://ia.gam.dev/cm203/23/lab03/nerfs.zip\" &&  unzip -q nerfs.zip -d ./content  && rm nerfs.zip\n",
    "base_path = Path(nerfs_path)\n",
    "%cd ./content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e325010",
   "metadata": {},
   "source": [
    "E vamos carregar os dados:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "ed75a8de",
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN = True\n",
    "\n",
    "dados = np.load(base_path / 'dados.npz')\n",
    "\n",
    "distancia_focal = float(dados['distancia_focal'])\n",
    "imagens_treino = torch.tensor(dados['imagens_treino'])\n",
    "poses_treino = torch.tensor(dados['poses_treino'])\n",
    "\n",
    "imagens_valida = torch.tensor(dados['imagens_valida'])\n",
    "poses_valida = torch.tensor(dados['poses_valida'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "971bd056-db00-44ee-832b-ae838a897aca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7fb8700d4af0>"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "imagens = dados['imagens_treino']\n",
    "\n",
    "def denormalize_image(image: np.ndarray):\n",
    "    denormalized = image.copy()\n",
    "    denormalized[:, :, [0, 1, 2]] = 255*denormalized[:, :, [0, 1, 2]]\n",
    "    return denormalized.astype(int)\n",
    "\n",
    "plt.imshow(denormalize_image(imagens[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a32cd9df",
   "metadata": {},
   "source": [
    "## Operações Básicas PyTorch (1 ponto)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32e3ee4e",
   "metadata": {},
   "source": [
    "**Explicação sobre o assunto**\n",
    "\n",
    "O pyTorch tem a mesma essência de operação que o nosso nanoGrad que implementamos na aula passada. Além disso, ele implementa todas as operações que você também consegue fazer em NumPy, sobre tudo as técnicas de slicing, broadcasting e as operações que aprendemos na aula 1 sobre o NumPy. Dessa forma temos um framework com a mesma flexibilidade de NumPy que é capaz de calcular os gradientes automaticamente além de poder operar eficientemente em GPU.\n",
    "\n",
    "Veja com é fácil criar um tensor, lembre-se que ele é essencialmente a mesma coisa que o nosso Nó Computacional (`NoComp`) do lab passado, mas que ele também tem muito mais operações"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f69013be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 2, 3])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.tensor([1, 2, 3])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f16f61dc",
   "metadata": {},
   "source": [
    "O `torch.Tensor` maiúsculo é a mesma coisa, só que ele usa o tipo especificado lá em cima como padrão (`torch.float32`) e não o tipo do dado de origem, na célula acima era int64, mas aqui já é float32."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "28280bba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1., 2., 3.])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.Tensor([1, 2, 3])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aadf6cf3",
   "metadata": {},
   "source": [
    "Você também pode criar um tensor a partir de uma matriz numpy, sendo que ele herda o tipo e o mesmo shape da matriz numpy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b1b5d589",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.2271,  0.7856, -1.2149,  0.4869], dtype=torch.float64)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.tensor(np.random.randn(4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81ddefc8",
   "metadata": {},
   "source": [
    "Quando você cria um Tensor do PyTorch inicialmente, ele não tem o cálculo dos gradientes habilitado por default, para poder habilitar esse funcionalidade, você tem que setar o parâmetro `requires_grad`, pois dessa forma, todos os tensores resultantes de um que tem isso habilitado também terá o cálculo dos gradientes habilitado. Veja que ele até aparede a `grad_fn`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1bd739f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([8.1000], grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.tensor([3.1], requires_grad=True) + torch.tensor([5])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b3ceffa",
   "metadata": {},
   "source": [
    "Assim a função `backward` é responsável por implementar o `retropropaga` de todos os nós que tem `requires_grad=True`. Os gradientes ficam armazenados no atributo `.grad` dos tensores, tal qual fizemos no Lab 2:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f62bc7db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([2.])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor0 = torch.tensor([3.1], requires_grad=True)\n",
    "tensor1 = 2 * tensor0 + 8\n",
    "tensor1.backward()\n",
    "tensor0.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b59f3845",
   "metadata": {},
   "source": [
    "E veja como o PyTorch te adverte quando você tenta acessar o gradiente de um tensor que não é raíz da computação (folha para ele). Isso acontece pois ele é esperto e nem guarda o gradiente para tensores intermediários pois a princípio eles não são parâmetros otimizáveis (são resultados internos de contas)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4b3fb4b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/site-packages/torch/utils/_device.py:62: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the .grad field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations. (Triggered internally at aten/src/ATen/core/TensorBody.h:486.)\n",
      "  return func(*args, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "tensor1.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff58d44c",
   "metadata": {},
   "source": [
    "Mas veremos essa questão dos gradiente depois no treinamento.\n",
    "\n",
    "Agora vamos implementar uma nova operação que será utilizada como camada inicial para a nossa NeRF, que é expandir as nossas entradas, que são simplesmente pontos 3d (X, Y, Z) em mais dimenões em frequência, isto é, aplicar senos e cossenos para obtermos uma codificação posicional (*positional encoding*) em uma dimensão maior que 3. Nas aulas futuras de Transformers, veremos mais uma outra motivação para essa operação (spoiler: é como se fosse uma codificação binária só que contínua, por exemplo se em binário representamos o número 99 como (0, 1, 1, 0,   0, 0, 1, 1) usamos os senos e cossenos com frequência exponencialmente maiores para criar uma representação contínua disso).\n",
    "\n",
    "(E tudo isso depois do Gabriel ter falado que Deep Learning fazia um 'automatic feature engineering' sendo que temos que manualmente ajudar a rede neural com features que são senos e cossenos dos valores de entrada 🤡)\n",
    "\n",
    "Primeiro vamos definir a quantidade de frequências que iremos utilizar e calcular os seus valores com base na exponenciação binária:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45bca9ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "qtd_frequencias = 6\n",
    "potencias_2 = 2.**torch.arange(qtd_frequencias)\n",
    "potencias_2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8baee63b",
   "metadata": {},
   "source": [
    "**Enunciado da Questão**\n",
    "\n",
    "Implemente a função `entrada_senos_cossenos` abaixo. Utilize as funções `torch.sin` e `torch.cos` para aplicar o seno e o cosseno. Faça uso de operações vetorizadas, não use for-loops explícitos que poderiam ser implementados de forma vetorizada (se não depois a NeRF fica um pouco mais lenta). Veja a [aula 1 slide 100](https://docs.google.com/presentation/d/1PYRArTIBh9vQ5r7DvEwGgbZp8FCkrwZnkQuFZ-z-snc) sobre o broadcasting e também um pouco antes sobre o uso de elipsis e `None` para indexar matrizes, é a mesma coisa do NumPy no PyTorch.\n",
    "\n",
    "**NÃO** use LLMs (ChatGPT) para pegar a resposta pronta. **NÃO** pesquise a resposta pronta na internet.\n",
    "\n",
    "**Pode** olhar a documentação das biblitocas (PyTorch mas todas as funções que você precisa está nas **dicas**) e **pode** (aconselhado) olhar o material de aula (slides e referências).\n",
    "\n",
    "<details><summary><b>Dica para a resposta</b></summary>\n",
    "<p>\n",
    "Utilize as função torch.cat e torch.flatten além de indexação com elipse e elemento None. Use índices negativos.\n",
    "\n",
    "A minha solução ficou com três linhas (mas poderia ter sido apenas uma), pois realizei primeiro o reshape do tensor de entrada para o formato correto, depois fiz a multiplicação na dimensão correta (apenas com o operador *) e por último apliquei a concatenação dos resultados calculados com o seno e o cosseno do produto, realizando o flatten para a dimensão correta.\n",
    "</p>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8f3bbc2",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "176d5fd24dc20f5c864060784c5a66d7",
     "grade": false,
     "grade_id": "basico_pytorch",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# basico_pytorch autograded_answer\n",
    "\n",
    "def entrada_senos_cossenos(x: torch.tensor, escala: torch.tensor = potencias_2) -> torch.tensor:\n",
    "    \"\"\" Realiza a seguinte operação em um tensor de entrada usando operações VETORIZADAS.\n",
    "           x =[x1, ..., xn]\n",
    "        dado um vetor de escala de frequências\n",
    "           escala = [f1, f2, ..., fe]\n",
    "        Calcula:\n",
    "           senos = [sin(f1*x1), sin(f2*x1), ..., sin(fe*x1), ... ... ... , sin(f1*xn), sin(f2*xn), ..., sin(fe*xn)]\n",
    "           cosse = [cos(f1*x1), cos(f2*x1), ..., cos(fe*x1), ... ... ... , cos(f1*xn), cos(f2*xn), ..., cos(fe*xn)]\n",
    "        e retorna a concatenação do vetor de entrada e de seus senos e cossenos na escala de frequências recebida, nessa ordem.\n",
    "           retorna [x, senos, cossenos]\n",
    "\n",
    "      Args:\n",
    "        x (torch.Tensor): Tensor de entrada a ser operado, tem shape (..., N)\n",
    "        escala (optional, torch.Tensor): Tensor com shape (E, ) que representa os fatores de escala \n",
    "            de frequência a serem utilizados na transformação, (default: potencias_2).\n",
    "\n",
    "      Returns:\n",
    "        (torch.Tensor): Resultado da operação, tem shape (..., N + 2 * N * E)\n",
    "      \"\"\"\n",
    "    # ESCREVA SEU CÓDIGO AQUI (pode apagar este comentário, mas não apague esta célula para não perder o ID)\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7690a4b5",
   "metadata": {},
   "source": [
    "Se você usou uma LLM, escreva a sua conversa com ela aqui nesta própria célula de texto (copie a conversa inteira) ou exporte o link da conversa:\n",
    "\n",
    "**Escreva aqui**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d362f962",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "2cc2293c7824c68f41de90586eac7570",
     "grade": true,
     "grade_id": "testa_basico_pytorch",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# testa_basico_pytorch autograder_tests 1\n",
    "\n",
    "qtd_frequencias = 6\n",
    "potencias_2 = 2.**torch.arange(qtd_frequencias)\n",
    "x0 = torch.tensor([3.])\n",
    "assert entrada_senos_cossenos(x0).shape == (13,)\n",
    "assert entrada_senos_cossenos(x0).dtype == torch.float32\n",
    "assert entrada_senos_cossenos(x0).requires_grad == False\n",
    "assert torch.linalg.norm(entrada_senos_cossenos(x0) - torch.tensor([ 3,  0.1411, -0.2794, -0.5366, -0.9056, -0.7683,  0.9836, -0.9900, 0.9602,  0.8439,  0.4242, -0.6401, -0.1804])) < 0.001\n",
    "x1 = torch.tensor([[3.]])\n",
    "assert entrada_senos_cossenos(x1).shape == (1, 13)\n",
    "assert torch.linalg.norm(entrada_senos_cossenos(x1) - torch.tensor([[ 3,  0.1411, -0.2794, -0.5366, -0.9056, -0.7683,  0.9836, -0.9900, 0.9602,  0.8439,  0.4242, -0.6401, -0.1804]])) < 0.001\n",
    "x2 = torch.tensor([[3, 1, 0.],\n",
    "                   [0, 2, -1]])\n",
    "assert entrada_senos_cossenos(x2).shape == (2, 39)\n",
    "assert torch.linalg.norm(entrada_senos_cossenos(x2) - torch.tensor(\n",
    "    [[ 3.0000,  0.1411, -0.2794, -0.5366, -0.9056, -0.7683,  0.9836, -0.9900,  0.9602,  0.8439,  0.4242, -0.6401, -0.1804,\n",
    "       1.0000,  0.8415,  0.9093, -0.7568,  0.9894, -0.2879,  0.5514,  0.5403, -0.4161, -0.6536, -0.1455, -0.9577,  0.8342,\n",
    "       0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000],\n",
    "     [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,\n",
    "       2.0000,  0.9093, -0.7568,  0.9894, -0.2879,  0.5514,  0.9200, -0.4161, -0.6536, -0.1455, -0.9577,  0.8342,  0.3919,\n",
    "      -1.0000, -0.8415, -0.9093,  0.7568, -0.9894,  0.2879, -0.5514,  0.5403, -0.4161, -0.6536, -0.1455, -0.9577,  0.8342]])) < 0.001\n",
    "assert abs(entrada_senos_cossenos(torch.zeros(5, 2, 3)).sum().item() - 180) < 0.001\n",
    "assert entrada_senos_cossenos(torch.zeros(5, 2, 7)).shape == (5, 2, 91)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe5706ee",
   "metadata": {},
   "source": [
    "## Estruturação do Conjunto de Dados (2 pontos)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26caa5d6",
   "metadata": {},
   "source": [
    "**Explicação sobre o assunto**\n",
    "\n",
    "Veja o início do vídeo abaixo, pelo menos até o primeiro minuto 01:00.\n",
    "\n",
    "[![Video Explaining NeRFs](https://ia.gam.dev/cm203/23/lab03/nerfs_video.png)](https://youtu.be/JuH79E8rdKc \"Video Explaining NeRFs - Click to Watch!\")\n",
    "\n",
    "Veja que o nosso conjunto de dados são apenas as imagens 2d tiradas de uma câmera e as poses da câmera, isto é, a posição 6d no espaço da câmera: translação (x, y, z) e rotação (os três ângulos de rotação  ψ, θ, φ por exemplo, mas essa rotação já é dada por uma matriz de rotação para não ter perigo de gimbal-lock).\n",
    "\n",
    "Você pode se perguntar, as fotos eu consigo capturar facilmente, mas como nós conseguimos essa pose das câmeras? Isso nós veremos no próximo bimestre em visão 3d, por enquanto você pode pensar que alguém mediu essas posições antes de tirar as fotos (um braço robótico por exemplo), ou no nosso caso, com um dataset sintético que foi renderizado no Blender, é justamente as coordenadas da câmera.\n",
    "\n",
    "Mas a ideia central da NeRF é ter uma rede neural cuja entrada é um ponto 3d no espaço, com as suas coordenadas X, Y, Z, e cuja saída é o R, G, B desse ponto e a sua 'opacidade' 𝜎 (que na realidade é uma densidade linear). <sub><sup>(OBS: Na formulação completa da NeRF, além do ponto 3d com X, Y, Z, temos os dois ângulos de visada desse ponto (θ, φ) que permite modelar o reflexo de luz (que depende de como você olha para o ponto). Mas para simplificar, deixamos de fora.)</sup></sub>\n",
    "\n",
    "Então temos que transformar esses pixels e essa pose da câmera em uma sequência de pontos 3d no espaço que poderá servir de entrada para a rede. E depois no final, temos que pegar essa sequência de pontos e reduzir."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afdf0977",
   "metadata": {},
   "source": [
    "Vamos primeiro gerar duas matrizes de coordenadas X e Y para cada pixel da nossa imagem, chamei de `pos_x` e `pos_y`. Perceba que eles se iniciam do ponto superior esquerdo e crescem da esquerda para a direita e de cima para baixo, conforme a nossa convenção de indexação da imagem, veja por exemplo o `pos_x` abaixo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "044170ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "H, W = imagens_treino.shape[1:3]\n",
    "pos_x, pos_y = torch.meshgrid(torch.arange(W), torch.arange(H), indexing='xy')\n",
    "pos_x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a6dc2f3",
   "metadata": {},
   "source": [
    "Se quiser ver o `pos_y`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53198e5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ef4583e",
   "metadata": {},
   "source": [
    "Então para cada pixel, vamos pegar um vetor que sai da origem do sistema de coordenadas da câmera, tido como $(0, 0, 0)$, e passa pelo plano de formação da imagem da câmera, tido como $(x-x_c, y-y_c, f)$, onde $x_c$ e $y_c$ são o centro de formação da imagem em pixels e $f$ é a distância focal da câmera, também medida em pixels. Estamos desprezando efeitos de distorção da lente.  Essa conta que você não precisa se preocupar agora (veremos a explicação no próximo bimestre em visão 3d). <sub><sup>OBS: (Aqui a nossa implementação difere um pouco do paper, eles não deixam norma 1, apenas dividem por $f$. A diferença é que a nossa amostra de distância vai ficar um segmento de esfera, em vez de um frostum)</sup></sub>\n",
    "\n",
    "Mas atente para o `shape` do tensor resultante: para cada pixel temos uma direção com `dir_x, dir_y, dir_z` normalizada, tudo isso no referencial da própria câmera (mas cuja direção dos eixos já foi invertida para ficar compatível com a rotação que será efetuada posteriormente)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a06bd2f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "centro_x, centro_y = W / 2, H / 2\n",
    "raios_camera = torch.stack([pos_x-centro_x, -(pos_y-centro_y), -distancia_focal * torch.ones_like(pos_x)], -1)\n",
    "raios_camera = raios_camera / raios_camera.norm(dim=-1, keepdim=True)\n",
    "raios_camera.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b91049e7",
   "metadata": {},
   "source": [
    "Veja que é bem semelhante ao `pos_y` acima, só que inverteu a direção, o centro ficou no meio e os valores máximos e mínimos dependem da distância focal da câmera (o FoV, Field of View, ângulo de abertura da câmera)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94049174",
   "metadata": {},
   "outputs": [],
   "source": [
    "raios_camera[:, :, 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e20e97f",
   "metadata": {},
   "source": [
    "Para termos um tensor com os valores de distância amostrados linearmente basta usarmos a função `torch.linspace` que a mesma do `np.linspace`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b27345e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "distancia_minima=2.\n",
    "distancia_maxima=6.\n",
    "n_amostras=64\n",
    "valores_distancia = torch.linspace(distancia_minima, distancia_maxima, n_amostras)\n",
    "valores_distancia"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beb07c0c",
   "metadata": {},
   "source": [
    "Então para cada raio normalizado nas coordenadas da câmera, iremos levá-lo para as coordenadas globais por meio da pose da câmera (é uma simples multiplicação pela matriz de rotação). Também a origem de cada um desses raios será a origem da própria câmera nas coordenadas globais (da sua pose). \n",
    "\n",
    "Assim, basta amostrarmos os pontos ao longo dos valores de distância da origem da câmera (já tudo em coordenadas globais).\n",
    "\n",
    "Uma questão importante que me fez perder tempo no treinamento que não convergia muito bem da NeRF era a necessidade de perturbar a distância desses pontos, para que eles não sejam sempre os mesmos durante o treinamento, por isso temos o parâmetro aleatoriza que é o valor máximo de uma distribuição uniforme $U_{uniforme}[0, aleatoriza)$.\n",
    "\n",
    "Assim a saída final dessa etapa de pré-processamento são os pontos 3d e também a distância entre os pontos vizinho (pois nós usaremos esse valor quando formos reduzir os pontos ao longo do raio na renderização para calcular a transparência).\n",
    "\n",
    "Para facilitar a implementação se vocês e também o entendimento, separei a função mostrada abaixo `amostra_pontos3d` em outra `amostra_pontos3d_inner`, para detalhar melhor o seu funcionamento que você irá implementar mais a diante (só o `inner`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9e10ddf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def amostra_pontos3d(raios_camera: torch.tensor, pose_camera: torch.tensor, valores_distancia: torch.tensor, aleatoriza: float = 0) -> Tuple[torch.tensor, torch.tensor]:\n",
    "    \"\"\" Amostra pontos 3d a valores de distância ao longo do raio especificado a partir da câmera\n",
    "    com a sua pose definida. Retorna esses pontos 3d e as distâncias entre eles (pontos vizinhos).\n",
    "\n",
    "    Args:\n",
    "      raios_camera (torch.Tensor): Tensor que determina o ponto de origem do raio, tem shape (..., 3)\n",
    "      pose_camera (torch.Tensor): Tensor que determina a pose da câmera [R | T], tem shape (..., 3, 4)\n",
    "      valores_distancia (torch.Tensor): Valores de distância entre os pontos, shape (..., n_amostras)\n",
    "      aleatoriza (float): valor aleatório máximo a ser somado sobre os valores_distância\n",
    "\n",
    "    Returns:\n",
    "      (torch.Tensor): Pontos 3d amostrados, tem shape (..., n_amostras, 3)\n",
    "      (torch.Tensor): Distância entre o ponto atual e o anterior, tem shape (..., n_amostras)\n",
    "    \"\"\"\n",
    "    direcao_vetor = raios_camera @ pose_camera[..., :3, :3].T\n",
    "    origem_vetor = pose_camera[..., :3, -1]\n",
    "    if aleatoriza:\n",
    "        valores_distancia = valores_distancia + torch.rand(direcao_vetor.shape[:-1]+valores_distancia.shape[-1:]) * aleatoriza\n",
    "    return amostra_pontos3d_inner(origem_vetor, direcao_vetor, valores_distancia, 1e10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c7c637c",
   "metadata": {},
   "source": [
    "**Enunciado da Questão**\n",
    "\n",
    "Implemente a função `amostra_pontos3d_inner` abaixo de acordo com a sua documentação.\n",
    "\n",
    "**NÃO** pesquise a resposta pronta na internet (**NÃO** copie o código de repositórios de NeRFs e seus derivados ...). Pode usar LLMs (ChatGPT) mas sem pedir para ele responder na cara dura, isto é, você pode pedir uma explicação mas não a resposta para a LLM, utilize por exemplo prompt de exemplo lá de cima (pode melhorar se quiser, mas nunca peça a resposta direta):\n",
    "\n",
    "**Pode** olhar a documentação das biblitocas (NumPy, PyTorch, FastAI, mas todas as funções que você precisa está nas **dicas**) e **pode** (aconselhado) olhar o material de aula (slides e referências).\n",
    "\n",
    "<details><summary><b>Dica para a resposta</b></summary>\n",
    "<p>\n",
    "Use indexação com ..., com None, com :, com número negativo. Utilize a função `pad` já importada, para acrescentar um valor (argumento value) com pad vezes no pad=(n_começo, n_final).\n",
    "\n",
    "A minha solução ficou com duas linhas, uma para calcular os pontos3d por meio das operações de adição, multiplicação, e indexação inteligente; e a outra linha  para calcular a distância entre os pontos por meio da indexação inteligente e a função pad para acrescentar o valor_infinito no final.\n",
    "</p>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8618bb9",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "b779f998574131fe34c304b59a967015",
     "grade": false,
     "grade_id": "estrutura_dados",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# estrutura_dados autograded_answer\n",
    "\n",
    "def amostra_pontos3d_inner(origem: torch.tensor, direcao: torch.tensor, valores_distancia: torch.tensor, valor_infinito: float) -> Tuple[torch.tensor, torch.tensor]:\n",
    "    \"\"\" Amostra pontos 3d a valores de distância ao longo do raio que parte de um ponto de origem\n",
    "        e que tem uma direção especificada. Retorna esses pontos 3d e as distâncias entre eles (pontos vizinhos).\n",
    "        Use operações VETORIZADAS.\n",
    "        Observe o seguinte exemplo com as dimensões maiores omitidas (broadcastable)\n",
    "           origem = [x0, y0, z0]\n",
    "           direcao = [dir_x, dir_y, dir_z]\n",
    "           valores_distancia = [dist0, dist1, ..., dist_n]\n",
    "        Calcula:\n",
    "           pontos3d = [[x0+dir_x*dist0,  y0+dir_y*dist0,  z0+dir_z*dist0],\n",
    "                       [x0+dir_x*dist1,  y0+dir_y*dist0,  z0+dir_z*dist0],\n",
    "                             ...              ...              ...\n",
    "                       [x0+dir_x*dist_n, y0+dir_y*dist_n, z0+dir_z*dist_n]]\n",
    "          distancia_entre_pontos = [dist1-dist0, dist2-dist1, ..., dist_n-dist_n1, valor_infinito]\n",
    "        e retorna tanto os pontos3d quando as distância entre os pontos vizinho, sendo que o último é o valor infinito.\n",
    "\n",
    "      Args:\n",
    "        origem (torch.Tensor): Tensor que determina o ponto de origem do raio, tem shape (..., 3)\n",
    "        direcao (torch.Tensor): Tensor que determina a direção do raio que parte da origem, tem norma garantidamente\n",
    "            unitária, tem shape (..., 3)\n",
    "        valores_distancia (torch.Tensor): Tensor que determina os valores de distância da origem que os pontos a serem\n",
    "            amostrado estão (na direção do raio), tem shape (..., n_amostras)\n",
    "        valor_infinito (float): Valor a ser utilizado na última distância entre pontos\n",
    "\n",
    "      Returns:\n",
    "        (torch.Tensor): Pontos 3d amostrados, tem shape (..., n_amostras, 3)\n",
    "        (torch.Tensor): Distância entre o ponto atual e o anterior, tem shape (..., n_amostras)\n",
    "    \"\"\"\n",
    "    # ESCREVA SEU CÓDIGO AQUI (pode apagar este comentário, mas não apague esta célula para não perder o ID)\n",
    "    raise NotImplementedError()\n",
    "    return pontos3d, distancia_entre_pontos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8b480c5",
   "metadata": {},
   "source": [
    "Se você usou uma LLM, escreva a sua conversa com ela aqui nesta própria célula de texto (copie a conversa inteira) ou exporte o link da conversa:\n",
    "\n",
    "**Escreva aqui**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de145e5c",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "ef4df290d50d4197267bba022993de3c",
     "grade": true,
     "grade_id": "testa_estrutura_dados",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# testa_estrutura_dados autograder_tests 2\n",
    "\n",
    "pts3d_test0, dists_test0 = amostra_pontos3d_inner(torch.tensor([9, 7, 0.]), torch.tensor([0, 0, 1.]), torch.tensor([0, 1, 2, 3, 8.]), 1e9)\n",
    "assert pts3d_test0.requires_grad == False\n",
    "assert pts3d_test0.dtype == torch.float32\n",
    "assert pts3d_test0.shape == (5, 3)\n",
    "assert torch.linalg.norm(pts3d_test0 - torch.tensor([[9., 7, 0], [9, 7, 1], [9, 7, 2], [9, 7, 3], [9, 7, 8]])) < 1e-5\n",
    "assert dists_test0.requires_grad == False\n",
    "assert dists_test0.dtype == torch.float32\n",
    "assert dists_test0.shape == (5,)\n",
    "assert torch.linalg.norm(dists_test0 - torch.tensor([1, 1, 1, 5, 1e9])) < 1e-5\n",
    "\n",
    "origem_testes = torch.tensor([0.0000, 2.7373, 2.9593])\n",
    "direcao_testes = torch.tensor([\n",
    "         [[ 0.3208, -0.8406, -0.4364],\n",
    "          [ 0.3150, -0.8424, -0.4373],\n",
    "          [ 0.3092, -0.8440, -0.4381],\n",
    "          [ 0.3034, -0.8457, -0.4390]],\n",
    "         [[ 0.3215, -0.8376, -0.4416],\n",
    "          [ 0.3157, -0.8394, -0.4425],\n",
    "          [ 0.3099, -0.8410, -0.4434],\n",
    "          [ 0.3040, -0.8427, -0.4443]]])\n",
    "val_dists_testes = torch.tensor([\n",
    "        [[ 0.4538,  3.3146,  6.8625,  8.1907, 10.6819],\n",
    "         [ 0.3811,  3.4473,  5.3367,  8.7293, 10.0798],\n",
    "         [ 0.8941,  2.6826,  6.9399,  8.8870, 10.7653],\n",
    "         [ 0.9996,  4.2814,  5.1150,  8.0507, 10.5449]],\n",
    "        [[ 0.3916,  4.3886,  6.9209,  7.5023, 10.9344],\n",
    "         [ 1.2990,  3.5211,  5.1325,  9.3391, 10.9775],\n",
    "         [ 1.3107,  3.8841,  6.1250,  9.2003, 10.8153],\n",
    "         [ 0.1952,  3.2972,  6.8838,  8.7427, 10.0683]]])\n",
    "pts3d_test1, dists_test1 = amostra_pontos3d_inner(origem_testes, direcao_testes, val_dists_testes, 1e8)\n",
    "assert pts3d_test1.dtype == torch.float32\n",
    "assert pts3d_test1.shape == (2, 4, 5, 3)\n",
    "assert torch.linalg.norm(pts3d_test1[:, :, [0,2,4], :] - torch.tensor([\n",
    "        [[[ 0.1456,  2.3558,  2.7613],\n",
    "          [ 2.2015, -3.0313, -0.0355],\n",
    "          [ 3.4268, -6.2419, -1.7023]],\n",
    "         [[ 0.1200,  2.4163,  2.7926],\n",
    "          [ 1.6811, -1.7583,  0.6256],\n",
    "          [ 3.1751, -5.7539, -1.4486]],\n",
    "         [[ 0.2765,  1.9827,  2.5676],\n",
    "          [ 2.1458, -3.1200, -0.0811],\n",
    "          [ 3.3286, -6.3486, -1.7570]],\n",
    "         [[ 0.3033,  1.8919,  2.5205],\n",
    "          [ 1.5519, -1.5885,  0.7138],\n",
    "          [ 3.1993, -6.1805, -1.6699]]],\n",
    "        [[[ 0.1259,  2.4093,  2.7864],\n",
    "          [ 2.2251, -3.0596, -0.0970],\n",
    "          [ 3.5154, -6.4214, -1.8693]],\n",
    "         [[ 0.4101,  1.6469,  2.3845],\n",
    "          [ 1.6203, -1.5709,  0.6882],\n",
    "          [ 3.4656, -6.4772, -1.8982]],\n",
    "         [[ 0.4062,  1.6350,  2.3781],\n",
    "          [ 1.8981, -2.4138,  0.2435],\n",
    "          [ 3.3517, -6.3584, -1.8362]],\n",
    "         [[ 0.0593,  2.5728,  2.8726],\n",
    "          [ 2.0927, -3.0637, -0.0992],\n",
    "          [ 3.0608, -5.7473, -1.5140]]]])) < 1e-3\n",
    "assert dists_test1.dtype == torch.float32\n",
    "assert dists_test1.shape == (2, 4, 5)\n",
    "assert torch.linalg.norm(dists_test1 - torch.tensor([\n",
    "        [[2.8608e+00, 3.5479e+00, 1.3282e+00, 2.4912e+00, 1.0000e+08],\n",
    "         [3.0662e+00, 1.8894e+00, 3.3926e+00, 1.3505e+00, 1.0000e+08],\n",
    "         [1.7885e+00, 4.2573e+00, 1.9471e+00, 1.8783e+00, 1.0000e+08],\n",
    "         [3.2818e+00, 8.3360e-01, 2.9357e+00, 2.4942e+00, 1.0000e+08]],\n",
    "        [[3.9970e+00, 2.5323e+00, 5.8140e-01, 3.4321e+00, 1.0000e+08],\n",
    "         [2.2221e+00, 1.6114e+00, 4.2066e+00, 1.6384e+00, 1.0000e+08],\n",
    "         [2.5734e+00, 2.2409e+00, 3.0753e+00, 1.6150e+00, 1.0000e+08],\n",
    "         [3.1020e+00, 3.5866e+00, 1.8589e+00, 1.3256e+00, 1.0000e+08]]])) < 1e-4\n",
    "\n",
    "pts3d, dists = amostra_pontos3d(raios_camera, poses_treino[0], valores_distancia)\n",
    "assert pts3d.shape == (100, 100, 64, 3)\n",
    "assert dists.shape == (64,)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "880c1438",
   "metadata": {},
   "source": [
    "Essa formulação das amostra é extremamente simplificada, o paper original realiza uma segunda amostragem baseada justamente no valor de densidade para poder amostrar os pontos de forma mais concentrada nos lugares de maior densidade (onde ele vai bater) pois não adianta nada ter um monte de pontos que ficam no vácuo, ou atrás do objeto.\n",
    "\n",
    "Formulações mais recentes, como a [instant-NGP](https://github.com/NVlabs/instant-ngp) utiliza formas ainda mais inteligentes de amostragem e uma implementação otimizada em CUDA para deixar o treinamento ainda 100x mais rápido.\n",
    "\n",
    "Fica a sugestão para o projeto do exame: implementar/aplicar uma NeRF original completa ou ainda modelos mais recentes de NeRFs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc3f3f19",
   "metadata": {},
   "source": [
    "## Definição da Arquitetura (3 pontos)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9264135a",
   "metadata": {},
   "source": [
    "**Explicação sobre o assunto**\n",
    "\n",
    "![Arquitetura da nossa Pequena NeRF a ser implementada](https://ia.gam.dev/cm203/23/lab03/nerf.png)\n",
    "\n",
    "Neste lab implementaremos uma arquitetura menor da NeRF apresentada no artigo original (sem as entradas dos ângulos de visada) e com menor largura e profundidade.\n",
    "\n",
    "A sua arquitetura é extremamente simples, sendo apenas camadas densas (completamente conectadas) sequenciais, a menos de uma conexão intermediária que concatena o resultado da ativação anterior com a entrada codificada.\n",
    "\n",
    "Em PyTorch podemos definir a nossa rede neural como sendo uma `nn.Module` que apresenta algumas conviências simples de registar os parâmetros treináveis.\n",
    "\n",
    "Abaixo, definimos uma PequenaNerf que conta com apenas 7 camadas treináveis. Escolhemos a função de ativação `leaky_relu` pois o `relu` estava ficando por muitas vezes preso em regiões negativas e não conseguia treinar (morria). Essa arquitetura tem algo interessante que é reinjetar a codificaçãoda entrada em uma camada posterior.\n",
    "\n",
    "Nessa nossa arquitetura, temos uma entrada com tamanho 3 no último eixo que representa o ponto 3d (X, Y, Z) e uma saída com tamanho 4 (que na realidade é quebrada em 3 e 1) no último eixo que representa o valor (R, G, B, 𝜎), nessa ordem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8fd16a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PequenaNerf(torch.nn.Module):\n",
    "    def __init__(self,\n",
    "                 largura: int = 128,\n",
    "                 codificador_posicional: Callable[[torch.tensor], torch.tensor] = entrada_senos_cossenos,\n",
    "                 funcao_ativacao: Callable[[torch.tensor], torch.tensor] = leaky_relu):\n",
    "        super().__init__()\n",
    "        self.largura = largura\n",
    "        self.codificador_posicional = codificador_posicional\n",
    "        self.funcao_ativacao = funcao_ativacao\n",
    "        dimensoes_apos_codifica = codificador_posicional(torch.zeros(3)).shape[0]\n",
    "        self.camada1 = torch.nn.Linear(dimensoes_apos_codifica, largura)\n",
    "        self.camada2 = torch.nn.Linear(largura, largura)\n",
    "        self.camada3 = torch.nn.Linear(largura, largura)\n",
    "        self.camada4 = torch.nn.Linear(largura + dimensoes_apos_codifica, largura)\n",
    "        self.camada5 = torch.nn.Linear(largura, largura)\n",
    "        self.camada6 = torch.nn.Linear(largura, largura)\n",
    "        self.camada_saida = torch.nn.Linear(largura, 4)\n",
    "        self.camadas = [self.camada1, self.camada2, self.camada3, self.camada4, self.camada5, self.camada6, self.camada_saida]\n",
    "\n",
    "    def forward(self, x: torch.tensor):\n",
    "        return executa_rede(x, self.codificador_posicional, self.funcao_ativacao, self.camadas)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc74cdbd",
   "metadata": {},
   "source": [
    "A esperteza do PyTorch com essa `nn.Module` se restringe a outros `nn.Module`s que sejam instanciados como atributos do primeiro ou ainda registrados manualmente por meio da função `.add_module`.\n",
    "\n",
    "Perceba que tem a mesma essência do método `.parametros()` da `RedeNeuralSequencial` que foi implementada no Lab 2.\n",
    "\n",
    "Veja por exemplo os parâmetros (que na realidade estão internos a esses módulos) que o PyTorch detecta."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7af03c4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "p = PequenaNerf()\n",
    "p"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0819024",
   "metadata": {},
   "source": [
    "Na realidade, ele consegue acessar os tensores simplesmente acessando cada módulo recursivamente e agregando os seus tensores, por exemplo, o módulo `Linear` já termina com apenas dois parâmetros `weight` e `bias` (é só a combinção linear, uma multiplicação matricial.\n",
    "\n",
    "Veja abaixo todos os tensores que são treináveis. O `state_dict` é usado como uma forma de salvar e de carregar os parâmetros de um `nn.Module`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6f47246",
   "metadata": {},
   "outputs": [],
   "source": [
    "p.state_dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d96f749",
   "metadata": {},
   "source": [
    "**Enunciado da Questão**\n",
    "\n",
    "Implemente a função `executa_rede` abaixo de acordo com a sua documentação.\n",
    "\n",
    "**NÃO** pesquise a resposta pronta na internet (**NÃO** copie o código de repositórios de NeRFs e seus derivados ...). Pode usar LLMs (ChatGPT) mas sem pedir para ele responder na cara dura, isto é, você pode pedir uma explicação mas não a resposta para a LLM, utilize por exemplo prompt de exemplo lá de cima (pode melhorar se quiser, mas nunca peça a resposta direta):\n",
    "\n",
    "**Pode** olhar a documentação das biblitocas (NumPy, PyTorch, FastAI, mas todas as funções que você precisa está nas **dicas**) e **pode** (aconselhado) olhar o material de aula (slides e referências).\n",
    "\n",
    "<details><summary><b>Dica para a resposta</b></summary>\n",
    "<p>\n",
    "Você deve chamar as funções, seja do codificador_posicional, seja da funcao_ativacao, ou seja um elemento da camadas_rede normalmente, tal qual chamamos as funções implementadas anteirormente, elas receber um tensor como entrada e geram um outro tensor de saída realizando diversas operações internamente, que são todas diferenciáveis e podem ser retropropagadas.\n",
    "\n",
    "Use `torch.cat` para concatenar os valores da codificação com o da camada anterior nessa ordem. Use as função sigmoid e softplus já importadas.\n",
    "\n",
    "Essa eu acabei levando 8 linhas para implementar, mas são todas bem simples que consistem apenas na aplicação das funções da camada (camadas_rede que é a combinação linear) seguida pela função de ativação. Você pode usar um for-loop ou descrever manualmente as operações com as camadas. Esse não tem como vetorizar pois as operações ocorrem sequencialmente e não em paralelo.\n",
    "    \n",
    "Um dos erros que cometi quando fui implementar pela primeira vez foi a de trocar as funções de ativação, usando uma sigmoide para a densidade, o que a limitava e deixava o trator semi-transparente. Muito atenção nessa função de ativação da última camada, isso pode prejudicar bastante o treino.\n",
    "</p>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de474234",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "7001059d20e9ca804044aa61e0c6561c",
     "grade": false,
     "grade_id": "define_arquitetura",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# define_arquitetura autograded_answer\n",
    "\n",
    "def executa_rede(tensor_entrada: torch.tensor, codificador_posicional: Callable, funcao_ativacao: Callable, camadas_rede: List[Callable]):\n",
    "    \"\"\" Realiza a propagação direta da NeRF, que consiste primeiro na codificação da entrada (codificador_posicional)\n",
    "        depois na aplicação sequencial das camadas da rede, cada qual com a sua própria combinação linear (camadas_rede)\n",
    "        no qual sempre devem passar pela funcao_ativacao, a menos da última camada que deve ter uma função de ativação\n",
    "        sigmoid para as três primeiras saídas e função de ativação softplus para o último elemento da saída.\n",
    "        A concatenação deverá ser realizada na quarta camada interna da rede, sendo que a codificação posicional deve\n",
    "        ser colocada antes do resultado da ativação da camada anterior, [x_codificado, atv_anterior] nessa ordem.\n",
    "\n",
    "      Args:\n",
    "        tensor_entrada (torch.tensor): Tensor que representa um ponto 3d no espaço, tem shape (..., 3)\n",
    "        codificador_posicional (Callable): Função que computa a codificação da entrada\n",
    "        funcao_ativacao (Callable): Função de ativação a ser aplicada a toda camada na rede (menos a última)\n",
    "        camadas_rede (List[Callable]): Lista com 7 funções que representam a combinação linear de uma camada da rede\n",
    "\n",
    "      Returns:\n",
    "        (torch.Tensor): Saída de RGB, deve ser entre 0 e 1 (resultado de sigmoid), tem shape (..., 3)\n",
    "        (torch.Tensor): Saída de densidade 𝜎, deve ser positivo (resultado de softplus), tem shape (..., )\n",
    "    \"\"\"\n",
    "    # ESCREVA SEU CÓDIGO AQUI (pode apagar este comentário, mas não apague esta célula para não perder o ID)\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "321c260f",
   "metadata": {},
   "source": [
    "Se você usou uma LLM, escreva a sua conversa com ela aqui nesta própria célula de texto (copie a conversa inteira) ou exporte o link da conversa:\n",
    "\n",
    "**Escreva aqui**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65b05ef2",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "ead2ee102ddd168dfb2dbdcd36a20f16",
     "grade": true,
     "grade_id": "testa_define_arquitetura",
     "locked": true,
     "points": 3,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# testa_define_arquitetura autograder_tests 3\n",
    "\n",
    "pts3d_test0 = torch.tensor([[0, 0, 0], [-999, -999, -999], [999, 999, 999.], [1, 2, 3], [-1, 8, -6]])\n",
    "cores, densidades = executa_rede(pts3d_test0, lambda x: torch.cat([x, x], dim=-1), lambda x: x, [lambda x: x for i in range(6)]+[lambda x: x[..., :4]])\n",
    "assert cores.dtype == torch.float32\n",
    "assert cores.shape == (5, 3)\n",
    "assert torch.linalg.norm(cores - torch.tensor(\n",
    "        [[0.5000, 0.5000, 0.5000],\n",
    "         [0.0000, 0.0000, 0.0000],\n",
    "         [1.0000, 1.0000, 1.0000],\n",
    "         [0.7311, 0.8808, 0.9526],\n",
    "         [0.2689, 0.9997, 0.0025]])) < 1e-3\n",
    "assert densidades.dtype == torch.float32\n",
    "assert densidades.shape == (5,)\n",
    "assert torch.linalg.norm(densidades - torch.tensor([0.69315, 0, 999, 1.3133, 0.31326])) < 1e-4\n",
    "\n",
    "cores, densidades = executa_rede(pts3d_test0, lambda x: torch.cat([x/2, x], dim=-1), lambda x: x+1, [lambda x: x for i in range(6)]+[lambda x: x[..., :4]])\n",
    "assert torch.linalg.norm(cores - torch.tensor(\n",
    "        [[0.9526, 0.9526, 0.9526],\n",
    "         [0.0000, 0.0000, 0.0000],\n",
    "         [1.0000, 1.0000, 1.0000],\n",
    "         [0.9707, 0.9820, 0.9890],\n",
    "         [0.9241, 0.9991, 0.5000]])) < 1e-3\n",
    "assert torch.linalg.norm(densidades - torch.tensor([3.0486, 0.0000, 1002.0000, 4.0181, 2.1269])) < 1e-4\n",
    "\n",
    "modelo_nerf = PequenaNerf()\n",
    "modelo_nerf.load_state_dict(torch.load(base_path / 'aleatoria.pt', map_location=device))\n",
    "pts3d, dists = amostra_pontos3d(raios_camera[:2, :2],  poses_treino[0], torch.linspace(0, 5, 7))\n",
    "\n",
    "cores, densidades = modelo_nerf(pts3d)\n",
    "assert cores.shape == (2, 2, 7, 3)\n",
    "assert cores.requires_grad == True\n",
    "assert densidades.shape == (2, 2, 7)\n",
    "assert densidades.requires_grad == True\n",
    "assert torch.linalg.norm(cores - torch.tensor([\n",
    "         [[[0.7953, 0.4961, 0.5148],\n",
    "           [0.7108, 0.3646, 0.6647],\n",
    "           [0.8353, 0.3493, 0.5688],\n",
    "           [0.7661, 0.3283, 0.6333],\n",
    "           [0.6062, 0.4618, 0.6932],\n",
    "           [0.7543, 0.3719, 0.3533],\n",
    "           [0.8085, 0.4326, 0.4280]],\n",
    "          [[0.7953, 0.4961, 0.5148],\n",
    "           [0.7006, 0.3631, 0.6645],\n",
    "           [0.8324, 0.3365, 0.5950],\n",
    "           [0.7625, 0.3179, 0.6299],\n",
    "           [0.5739, 0.4561, 0.6880],\n",
    "           [0.7773, 0.3852, 0.3391],\n",
    "           [0.7382, 0.3707, 0.4519]]],\n",
    "         [[[0.7953, 0.4961, 0.5148],\n",
    "           [0.7031, 0.3730, 0.6575],\n",
    "           [0.8291, 0.3453, 0.5678],\n",
    "           [0.7587, 0.3482, 0.6528],\n",
    "           [0.5746, 0.4547, 0.7069],\n",
    "           [0.7038, 0.3423, 0.3991],\n",
    "           [0.8258, 0.4944, 0.4465]],\n",
    "          [[0.7953, 0.4961, 0.5148],\n",
    "           [0.6944, 0.3735, 0.6606],\n",
    "           [0.8263, 0.3322, 0.6081],\n",
    "           [0.7580, 0.3278, 0.6505],\n",
    "           [0.5700, 0.4707, 0.6980],\n",
    "           [0.7135, 0.3595, 0.3701],\n",
    "           [0.7903, 0.4171, 0.4178]]]])) < 1e-3\n",
    "assert torch.linalg.norm(densidades - torch.tensor([\n",
    "         [[0.1150, 0.1859, 0.1180, 0.3546, 0.3164, 0.3694, 0.2446],\n",
    "          [0.1150, 0.1861, 0.1298, 0.3080, 0.3537, 0.2966, 0.2130]],\n",
    "         [[0.1150, 0.1804, 0.1198, 0.3267, 0.2936, 0.3396, 0.3401],\n",
    "          [0.1150, 0.1794, 0.1301, 0.2792, 0.3456, 0.3108, 0.2732]]])) < 1e-3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18fd4e80",
   "metadata": {},
   "source": [
    "Pronto, se passar nos testes, a sua arquitetura está exatamente igual ao que implementamos, o que possibilita carregar os pesos já treinados e obter exatamente o mesmo resultado esperado.\n",
    "\n",
    "Perceba que você também poderia implementar de formas diferentes e conseguir treinar uma NeRF até melhor, só que a arquitetura foi fixada por limitações da correção."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcb72c3b",
   "metadata": {},
   "source": [
    "## Treinamento (2 pontos)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ca1c88a",
   "metadata": {},
   "source": [
    "**Explicação sobre o assunto**\n",
    "\n",
    "Para podermos treinar a rede neural, temos que primeiro renderizar a imagem a partir das cores e densidades de cada ponto amostrados ao longo do raio. Para isso implementamos a função `renderiza_imagem` abaixo. Ela recebe as cores e as densidades de cada ponto ao longo de um raio além das distâncias lineares entre esses pontos vizinhos, calculadas mais facilmente no passo anterior.\n",
    "\n",
    "Como a densidade foi definida do ponto de vista da 'quantidade de matéria por elemento de profundidade' linear, nós temos que multiplicá-la justamente pela nossa distância entre os pontos vizinhos para obter uma grandeza que seja correlacionada com a opacidade do material entre esses dois pontos vizinhos. Um detalhe aqui é o uso da exponencial $(1-exp(-x))$ que garante que a opacidade esteja sempre entre 0 e 1 para valores de densidade positivos.\n",
    "\n",
    "Depois calculamos a transmitância que a fração de luz que passa localmente entre os pontos, isto é, a transparência (o complementar da opacidade). Nós multiplicamos esse valor de transmitância consecutivamente ao longo do raio para saber o quanto de luz de chega até a câmera partindo daquele ponto por meio da função `torch.cumprod` que calcula o produto cumulativo, exemplo: `[a, b, c, d] -> [a, a*b, a*b*c, a*b*c*d]` (aqui você pode pensar equivalentemente da fração de luz que se estivesse saindo da câmera chegaria ao ponto). Para garantir que sempre pelo menos o primeiro ponto será amostrado, nós fazemos um padding com o valor unitário para ficar da forma `[1, a, a*b, a*b*c]` e o valor que seria da opacidade amostrada no infinito vai embora também.\n",
    "\n",
    "Finalmente podemos calcula a contribuição de cada cor que é justamente a sua própria opacidade vezes o tanto de luz que chega até o observador partindo-se daquele ponto, isto é, a transmitância (transparência acumulada). Todo esse processo na realidade é uma discretização do que seria uma integral da NeRF ao longo do raio (ray).\n",
    "\n",
    "Aqui estamos trabalhando com operações aditivas das cores ([ver slide 53 da aula 1](https://docs.google.com/presentation/d/1PYRArTIBh9vQ5r7DvEwGgbZp8FCkrwZnkQuFZ-z-snc)), só que ponderadas pela opacidade e pela 'transmitância acumulada' do ponto de interesse até a câmera. A intuição aqui é que são os pontos que emitem luz.\n",
    "\n",
    "Ela foi dividia com a função `reduz_cores` que você implementará na célula pontuada desta seção. Atente-se para o shape dos tensores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1448a32d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def renderiza_imagem(cores: torch.tensor, densidades: torch.tensor, distancia_entre_pontos: torch.tensor) -> torch.tensor:\n",
    "    \"\"\" Realiza a combinação das cores de acordo com a densidade do ponto ao longo do raio (ray)\n",
    "\n",
    "      Args:\n",
    "        cores (torch.Tensor): Tensor que representa a cor de um ponto no espaço em um mesmo\n",
    "            raio (ray), tem shape (..., N, 3) e valores entre 0 e 1\n",
    "        densidades (torch.Tensor): Tensor que representa a densidade amostrada em pontos \n",
    "            consecutivos em um raio (ray) no espaço, tem shape (..., N) e valores positivos\n",
    "        distancia_entre_pontos (torch.Tensor): Tensor que representa a distância consecutiva \n",
    "            entre os pontos de um mesmo raio, tem shape (..., N)\n",
    "\n",
    "      Returns:\n",
    "        (torch.Tensor): Saída de RGB, com valores entre entre 0 e 1, tem shape (..., 3)\n",
    "    \"\"\"\n",
    "    opacidade = 1. - torch.exp(-densidades * distancia_entre_pontos)\n",
    "    transmitancia = 1 - opacidade + 1e-10\n",
    "    transmitancia_acumulada = torch.cumprod(transmitancia, -1)[..., :-1]\n",
    "    transmitancia_acumulada = pad(transmitancia_acumulada, (1,0), value=1)\n",
    "    pesos_de_cada_cor = opacidade * transmitancia_acumulada\n",
    "    return reduz_cores(cores, pesos_de_cada_cor)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6431371f",
   "metadata": {},
   "source": [
    "**Enunciado da Questão**\n",
    "\n",
    "Implemente as funções `reduz_cores` e `train` abaixo de acordo com a sua documentação.\n",
    "\n",
    "**NÃO** pesquise a resposta pronta na internet (**NÃO** copie o código de repositórios de NeRFs e seus derivados ...). Pode usar LLMs (ChatGPT) mas sem pedir para ele responder na cara dura, isto é, você pode pedir uma explicação mas não a resposta para a LLM, utilize por exemplo prompt de exemplo lá de cima (pode melhorar se quiser, mas nunca peça a resposta direta):\n",
    "\n",
    "**Pode** olhar a documentação das biblitocas (NumPy, PyTorch, FastAI, mas todas as funções que você precisa está nas **dicas**) e **pode** (aconselhado) olhar o material de aula (slides e referências).\n",
    "\n",
    "<details><summary><b>Dica para a resposta</b></summary>\n",
    "<p>\n",
    "A função `reduz_cores` pode ser implementada apenas com uma linha de código. Use a função `torch.sum` somando na dimensão `dim` correta. Basta fazer uma multiplicação e uma indexação com elemento None (reshape).\n",
    "\n",
    "A função `train` pode ser implementada com com apenas 4 linhas de código. Sendo que as três primeiras são apenas chamar as funções corretamente que foram recebidas como argumentos. Se tiver dúvida de como essas funções são chamadas, observe os testes do exercício anterior (TESTES TAMBÉM SÃO DOCUMENTAÇÃO).\n",
    "\n",
    "É importantíssimo você chamar o método `.backward()` do seu tensor custo para poder retropropagar os gradientes, lembra do lab2 da função retropropaga?\n",
    "</p>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83518ba8",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "bae0e8710b90a8c6997c4f09aa47e465",
     "grade": false,
     "grade_id": "treinamento",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# treinamento autograded_answer\n",
    "\n",
    "def reduz_cores(cores: torch.tensor, pesos_de_cada_cor: torch.tensor) -> torch.tensor:\n",
    "    \"\"\" Realiza a média ponderada das cores ao longo do raio (ray) de acordo com a sua importância\n",
    "        que já foi calculada anteriormente pela sua opacidade e pela fração da luz que chega daquele ponto.\n",
    "\n",
    "      Args:\n",
    "        cores (torch.Tensor): Tensor que representa a cor de um ponto no espaço em um mesmo raio (ray),\n",
    "            tem shape (..., N, 3) e valores entre 0 e 1\n",
    "        pesos_de_cada_cor (torch.Tensor): Tensor que representa os pesos a ser usado na combinação linear das\n",
    "            cores consecutivas em um raio (ray) no espaço, tem shape (..., N) e valores entre 0 e 1, soma unitária\n",
    "\n",
    "      Returns:\n",
    "        (torch.Tensor): Saída de RGB da imagem formada, com valores entre entre 0 e 1, tem shape (..., 3)\n",
    "    \"\"\"\n",
    "    # ESCREVA SEU CÓDIGO AQUI (pode apagar este comentário, mas não apague esta célula para não perder o ID)\n",
    "    raise NotImplementedError()\n",
    "\n",
    "def train(neural_model: Callable, funcao_custo: Callable, dataloader: Iterable, optimizer: torch.optim.Optimizer, scheduler: torch.optim.lr_scheduler.LRScheduler) -> List[float]:\n",
    "    \"\"\" Realiza o treinamento da rede neural, \n",
    "\n",
    "      Args:\n",
    "        neural_model (Callable): Modelo da rede neural a ser treinada\n",
    "        funcao_custo (Callable): Função custo que recebe dois tensores e retorna um tensor de rank 0.\n",
    "            Passe a saída esperada como primeiro argumento e a saída estimada pela rede como segundo argumento.\n",
    "        dataloader (Iterable): Objeto iterável que retorna uma tupla com valores de entrada e de saída esperada\n",
    "        optimizer (torch.optim.Optimizer): Tem referência os parâmetros treináveis da rede e os atualiza\n",
    "        scheduler (torch.optim.lr_scheduler.LRScheduler): Controla os hiper-parâmetros do optimizer\n",
    "\n",
    "      Returns:\n",
    "        (List[float]): Lista com os custos que são MSE, mean-squared error\n",
    "    \"\"\"\n",
    "    custos: List[float]  = []\n",
    "    dataloader = tqdm(dataloader) # Para imprimir a barra de progresso verde\n",
    "    for (pts3d, dists), imagem_rgb in dataloader:\n",
    "        optimizer.zero_grad() # Zera os gradientes, por causa do += do lab2\n",
    "        # ESCREVA SEU CÓDIGO AQUI (pode apagar este comentário, mas não apague esta célula para não perder o ID)\n",
    "        raise NotImplementedError()\n",
    "        optimizer.step() # Aqui que os parâmetros da rede são atualizados\n",
    "        scheduler.step()\n",
    "        dataloader.set_postfix(Custo=custo.item())\n",
    "        custos.append(custo.item())\n",
    "    return custos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6801e04",
   "metadata": {},
   "source": [
    "Se você usou uma LLM, escreva a sua conversa com ela aqui nesta própria célula de texto (copie a conversa inteira) ou exporte o link da conversa:\n",
    "\n",
    "**Escreva aqui**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad0e8db8",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "800d79a11d99b387509ca01b6f223ce9",
     "grade": true,
     "grade_id": "testa_treinamento",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# testa_treinamento autograder_tests 2\n",
    "\n",
    "\n",
    "imagem_gerada = reduz_cores(torch.tensor([[0.3, 0.8, 0.6],[0, 0, 0]]), torch.tensor([0.5, 0.5]))\n",
    "assert imagem_gerada.dtype == torch.float32\n",
    "assert imagem_gerada.shape == (3,)\n",
    "assert torch.linalg.norm(imagem_gerada - torch.tensor([0.15, 0.4, 0.3])) < 1e-5\n",
    "imagem_gerada = reduz_cores(torch.tensor([[0.3, 0.8, 0.6],[0, 0, 0]]), torch.tensor([0, 1]))\n",
    "assert torch.linalg.norm(imagem_gerada - torch.tensor([0, 0., 0])) < 1e-5\n",
    "imagem_gerada = reduz_cores(torch.tensor([[0.3, 0.8, 0.6],[0, 0, 0]]), torch.tensor([1, 0]))\n",
    "assert torch.linalg.norm(imagem_gerada - torch.tensor([0.3, 0.8, 0.6])) < 1e-5\n",
    "imagem_gerada = reduz_cores(torch.ones((55, 66, 10, 3)), torch.ones((55, 66, 10))*0.1)\n",
    "assert imagem_gerada.shape == (55, 66, 3)\n",
    "assert torch.linalg.norm(imagem_gerada -  torch.ones((55, 66, 3))) < 1e-5\n",
    "\n",
    "modelo_nerf = PequenaNerf()\n",
    "modelo_nerf.load_state_dict(torch.load(base_path / 'aleatoria.pt', map_location=device))\n",
    "pts3d, dists = amostra_pontos3d(raios_camera[:2, :2],  poses_treino[0], torch.linspace(0, 5, 7))\n",
    "cores, densidades = modelo_nerf(pts3d)\n",
    "imagem_gerada = renderiza_imagem(cores, densidades, dists)\n",
    "assert imagem_gerada.requires_grad == True\n",
    "assert imagem_gerada.shape == (2, 2, 3)\n",
    "assert torch.linalg.norm(imagem_gerada -  torch.tensor([\n",
    "        [[0.7585, 0.4017, 0.5384],\n",
    "         [0.7306, 0.3832, 0.5476]],\n",
    "        [[0.7543, 0.4230, 0.5489],\n",
    "         [0.7389, 0.4010, 0.5410]]])) < 1e-3\n",
    "\n",
    "\n",
    "torch.manual_seed(42)\n",
    "\n",
    "funcao_custo_mse = lambda ŷ, y: torch.mean((ŷ - y)**2)\n",
    "\n",
    "amostra_pts0 = amostra_pontos3d(raios_camera, poses_treino[0], torch.linspace(2, 6, 16))\n",
    "dataloader0 = [(amostra_pts0, imagens_treino[0]) for i in range(10)]\n",
    "\n",
    "optim0 = torch.optim.Adam(modelo_nerf.parameters())\n",
    "sched0 = torch.optim.lr_scheduler.OneCycleLR(optim0, 1e-3, 10)\n",
    "\n",
    "custos = train(modelo_nerf, funcao_custo_mse, dataloader0, optim0, sched0)\n",
    "assert custos[-1] <= 0.027\n",
    "assert torch.linalg.norm(modelo_nerf.camada_saida.bias - torch.tensor([-0.0037, -0.0036, -0.0034, -0.0042])) < 1e-3\n",
    "assert torch.linalg.norm(modelo_nerf.camada_saida.bias.grad - torch.tensor([0.0023, 0.0036, 0.0013, 0.0008])) < 1e-3\n",
    "del dataloader0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8747fa3a",
   "metadata": {},
   "source": [
    "Esse não foi o treinamento de verdade, foi apenas para testar o seu código (a mesma imagem 10 vezes com poucas amostras do ray). Você irá treinar de fato a NeRF mais embaixo, depois do próximo exercício pontuado."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72bacf98",
   "metadata": {},
   "source": [
    "## Validação e Síntese de Novas Cenas (2 pontos)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40ee6b93",
   "metadata": {},
   "source": [
    "**Explicação sobre o assunto**\n",
    "\n",
    "Parabéns, se você chegou até aqui é porque praticamente já está acabando a parte avaliada do lab e logo após você treinará a sua própria NeRF do zero!\n",
    "\n",
    "Agora vamos apenas chamar a rede com as funções apropriadas que já utilizamos anteriormente.\n",
    "\n",
    "Mas como também a inferência não faz parte do treinamento, não precisamos calcular os gradiente, o que economiza memória, assim usamos a função `torch.no_grad()` para desabilitar o cálculo dos gradientes dentro de um escopo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "506a9341",
   "metadata": {},
   "outputs": [],
   "source": [
    "t0 = torch.tensor([3.], requires_grad=True)\n",
    "with torch.no_grad():\n",
    "    t1 = 2*t0 + 9\n",
    "t1, t1.requires_grad"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77125214",
   "metadata": {},
   "source": [
    "**Enunciado da Questão**\n",
    "\n",
    "Implemente as funções `sintetiza_nova` de acordo com a sua documentação.\n",
    "\n",
    "**NÃO** pesquise a resposta pronta na internet (**NÃO** copie o código de repositórios de NeRFs e seus derivados ...). Pode usar LLMs (ChatGPT) mas sem pedir para ele responder na cara dura, isto é, você pode pedir uma explicação mas não a resposta para a LLM, utilize por exemplo prompt de exemplo lá de cima (pode melhorar se quiser, mas nunca peça a resposta direta):\n",
    "\n",
    "**Pode** olhar a documentação das biblitocas (NumPy, PyTorch, FastAI, mas todas as funções que você precisa está nas **dicas**) e **pode** (aconselhado) olhar o material de aula (slides e referências).\n",
    "\n",
    "<details><summary><b>Dica para a resposta</b></summary>\n",
    "<p>\n",
    "\n",
    "A função `train` pode ser implementada com com apenas 4 linhas de código. Sendo que são apenas chamar as funções corretamente que foram implementada anteriormente. Não se esqueça do `torch.no_grad()`\n",
    "\n",
    "</p>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a818ae69",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "b64c1fd9703b93e2bef270e5aa0c9bd7",
     "grade": false,
     "grade_id": "validacao_sintese",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# validacao_sintese autograded_answer\n",
    "\n",
    "def sintetiza_nova(modelo_nerf: Callable, raios_camera: torch.tensor, pose_camera: torch.tensor, valores_distancias: torch.tensor) -> torch.tensor:\n",
    "    \"\"\" Amostra pontos 3d a valores de distância ao longo do raio especificado a partir da câmera\n",
    "    com a sua pose definida. Retorna esses pontos 3d e as distâncias entre eles (pontos vizinhos).\n",
    "\n",
    "    Args:\n",
    "      modelo_nerf (Callable): Rede Neural\n",
    "      raios_camera (torch.Tensor): Tensor que determina o ponto de origem do raio, tem shape (..., 3)\n",
    "      pose_camera (torch.Tensor): Tensor que determina a pose da câmera [R | T], tem shape (..., 3, 4)\n",
    "      valores_distancia (torch.Tensor): Valores de distância entre os pontos, shape (..., n_amostras)\n",
    "\n",
    "    Returns:\n",
    "        (torch.Tensor): Saída de RGB, com valores entre entre 0 e 1, tem shape (..., 3)\n",
    "    \"\"\"\n",
    "    # ESCREVA SEU CÓDIGO AQUI (pode apagar este comentário, mas não apague esta célula para não perder o ID)\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bea3f9e",
   "metadata": {},
   "source": [
    "Se você usou uma LLM, escreva a sua conversa com ela aqui nesta própria célula de texto (copie a conversa inteira) ou exporte o link da conversa:\n",
    "\n",
    "**Escreva aqui**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83566d4f",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "a1ff3a07d3e431a5d14c4bfeb46df0a8",
     "grade": true,
     "grade_id": "testa_validacao_sintese",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# testa_validacao_sintese autograder_tests 2\n",
    "\n",
    "\n",
    "modelo_nerf = PequenaNerf()\n",
    "modelo_nerf.load_state_dict(torch.load(base_path / 'treinada.pt', map_location=device))\n",
    "n_amostras0 = 256 # Diminua essa linha se a sua GPU reclamar de memória\n",
    "img0 = sintetiza_nova(modelo_nerf, raios_camera, poses_valida[1], torch.linspace(2, 6, n_amostras0))\n",
    "assert img0.requires_grad == False\n",
    "assert img0.shape == (H, W, 3)\n",
    "assert torch.linalg.norm(img0[50:55, 50:55] - torch.tensor([\n",
    "        [[0.2988, 0.2144, 0.0685],\n",
    "         [0.3396, 0.2407, 0.0739],\n",
    "         [0.3654, 0.2630, 0.0777],\n",
    "         [0.3996, 0.2907, 0.0830],\n",
    "         [0.4343, 0.3110, 0.0801]],\n",
    "        [[0.3106, 0.2353, 0.0742],\n",
    "         [0.3208, 0.2316, 0.0709],\n",
    "         [0.3539, 0.2520, 0.0723],\n",
    "         [0.3644, 0.2723, 0.0739],\n",
    "         [0.3935, 0.2955, 0.0727]],\n",
    "        [[0.3630, 0.2703, 0.0834],\n",
    "         [0.3621, 0.2685, 0.0743],\n",
    "         [0.3768, 0.2754, 0.0747],\n",
    "         [0.3994, 0.2906, 0.0766],\n",
    "         [0.3657, 0.2873, 0.0737]],\n",
    "        [[0.3786, 0.2704, 0.0733],\n",
    "         [0.3903, 0.2794, 0.0764],\n",
    "         [0.4062, 0.2888, 0.0841],\n",
    "         [0.4312, 0.3090, 0.0858],\n",
    "         [0.4103, 0.3089, 0.0849]],\n",
    "        [[0.3643, 0.2552, 0.0677],\n",
    "         [0.3884, 0.2796, 0.0762],\n",
    "         [0.4132, 0.2855, 0.0860],\n",
    "         [0.4202, 0.3040, 0.0951],\n",
    "         [0.4596, 0.3417, 0.1223]]]\n",
    ")) < 0.2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f53da30",
   "metadata": {},
   "source": [
    "A imagem resultante deve ser parecida com a segunda imagem abaixo do conjunto de validação."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef06ca81",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(img0.cpu())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b5659e7",
   "metadata": {},
   "source": [
    "## Juntando tudo e treinando de verdade!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebf67969",
   "metadata": {},
   "source": [
    "Vamos dar uma olhada no nosso conjunto de treinamento, são 100 imagens com o tamanho de 100 pixels de altura e largura de um trator lego:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65177628",
   "metadata": {},
   "outputs": [],
   "source": [
    "if TRAIN:\n",
    "    fig, axs = plt.subplots(10, 10, figsize=(12,12))\n",
    "    for ax, img in zip(axs.flatten(), imagens_treino.cpu().numpy()):\n",
    "        ax.imshow(img)\n",
    "        ax.axis('off')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "imagens_treino.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f186687",
   "metadata": {},
   "source": [
    "Já as imagens de validação são apenas 5 (poderiam ter tido mais, mas acabou sendo uma limitação desse dataset que já peguei formatado):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b20716a",
   "metadata": {},
   "outputs": [],
   "source": [
    "if TRAIN:\n",
    "    fig, axs = plt.subplots(1, 6, figsize=(18,6))\n",
    "    for ax, img in zip(axs.flatten(), imagens_valida.cpu().numpy()):\n",
    "        ax.imshow(img)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51f827ca",
   "metadata": {},
   "source": [
    "E se fôssemos visualizar a saída de nossa rede:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecc75133",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mostra_predicao_valida(modelo_nerf, n_amostras = 128, plots = None):\n",
    "    valores = torch.linspace(2, 6, n_amostras)\n",
    "    fig, axs = plt.subplots(1, 6, figsize=(12,3))\n",
    "    for i, ax in enumerate(axs.flatten()):\n",
    "        if plots is not None and i == 5:\n",
    "            plt.plot(plots, 'x-')\n",
    "            break\n",
    "        img = sintetiza_nova(modelo_nerf, raios_camera, poses_valida[i], valores)\n",
    "        ax.imshow(img.cpu())\n",
    "        ax.set_title('PSNR %.4f'%(-10 * torch.log10(torch.nn.functional.mse_loss(img, imagens_valida[i])).item()))\n",
    "        ax.axis('off')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "if TRAIN:\n",
    "    mostra_predicao_valida(modelo_nerf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f08337f",
   "metadata": {},
   "source": [
    "Então só para juntar todas as contas que tinham ficadas espalhadas pelas questões do notebook temos essa classe do Dataset, que gera a entrada e a saída esperada para a rede neural da NeRF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "099a4e87",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset:\n",
    "    def __init__(self, imagens, poses, focal, valores_distancias):\n",
    "        self.imagens = imagens\n",
    "        self.poses = poses\n",
    "        self.focal = focal\n",
    "        self.valores_distancias = valores_distancias\n",
    "        self.aleatoriza = (valores_distancias.max().item() - valores_distancias.min().item()) / len(valores_distancias)\n",
    "        H, W = imagens_treino.shape[1:3]\n",
    "        pos_x, pos_y = torch.meshgrid(torch.arange(W), torch.arange(H), indexing='xy')\n",
    "        centro_x, centro_y = W / 2, H / 2\n",
    "        raios_camera = torch.stack([pos_x-centro_x, -(pos_y-centro_y), -focal * torch.ones_like(pos_x)], -1)\n",
    "        self.raios_camera = raios_camera / raios_camera.norm(dim=-1, keepdim=True)\n",
    "\n",
    "    def dataloaders(self, batch_size=1):\n",
    "        return torch.utils.data.DataLoader(self, batch_size=batch_size)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.imagens)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        if idx > len(self.imagens):\n",
    "            raise IndexError(\"Index out of range\")\n",
    "        pts3d_dists = amostra_pontos3d(self.raios_camera, self.poses[idx], self.valores_distancias, self.aleatoriza)\n",
    "        return pts3d_dists, self.imagens[idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32eba8e2",
   "metadata": {},
   "source": [
    "Então vamos instanciar o nosso dataset de treino e o nosso dataloader, lembra do lab1? Aqui o dataloader simplesmente aglutinaria várias imagens para formar um tamanho de batch maior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "014a496b",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_treino = Dataset(imagens_treino, poses_treino, distancia_focal, torch.linspace(2, 6, 64))\n",
    "dataloader_treino = dataset_treino.dataloaders()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc37321a",
   "metadata": {},
   "source": [
    "Assim podemos instanciar o modelo e carregar pesos aleatórios (só porque o PyTorch gosta de iniciar os bias com valores aleatórios e eu gosto de zeros, além do desvio padrão dos pesos um pouco maior)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2d3cb5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "if TRAIN:\n",
    "    modelo_nerf = PequenaNerf()\n",
    "    modelo_nerf.load_state_dict(torch.load(base_path/'aleatoria.pt', map_location=device))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "722367d0",
   "metadata": {},
   "source": [
    "Veja como é o modelo aleatório:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "868d4ee6",
   "metadata": {},
   "outputs": [],
   "source": [
    "if TRAIN:\n",
    "    mostra_predicao_valida(modelo_nerf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2298fa5",
   "metadata": {},
   "source": [
    "E agora fazemos o loop final do treinamento, sinta-se a vontade para ficar treinando mais e ver como os resultados evoluem, altere a quantidade de épocas e lembre-se de diminuir a learning_rate se você for rodar novamente por mais épocas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "113011d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "epocas = 10\n",
    "learning_rate = 2e-3\n",
    "\n",
    "optim0 = torch.optim.Adam(modelo_nerf.parameters())\n",
    "sched0 = torch.optim.lr_scheduler.OneCycleLR(optim0, learning_rate, epochs=epocas, steps_per_epoch=len(dataloader_treino), pct_start=0.01)\n",
    "psnrs = []\n",
    "\n",
    "for epoca in range(epocas):\n",
    "  if TRAIN:\n",
    "    custos = train(modelo_nerf, torch.nn.functional.mse_loss, dataloader_treino, optim0, sched0)\n",
    "    psnrs.append(-10 * np.log10(np.mean(custos)))\n",
    "    mostra_predicao_valida(modelo_nerf, plots=psnrs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba0298d3",
   "metadata": {},
   "source": [
    "Para salvar a sua NeRF:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d962b0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "if TRAIN:\n",
    "    torch.save(modelo_nerf.state_dict(), 'nova_treinada.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44c46e6e",
   "metadata": {},
   "source": [
    "Para carregar:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03e6e13d",
   "metadata": {},
   "outputs": [],
   "source": [
    "if TRAIN:\n",
    "    modelo_nerf.load_state_dict(torch.load('nova_treinada.pt', map_location=device))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d52acec",
   "metadata": {},
   "source": [
    "Agora mexa nas barrinhas abaixo para mudar a sua posição de renderização (não se preocupe com o código abaixo):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d13be2e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial.transform import Rotation\n",
    "from ipywidgets import interactive, widgets\n",
    "\n",
    "n_amostrasx = 128\n",
    "\n",
    "def pose_de_angulos(phi, theta, distancia):\n",
    "    rot = Rotation.from_euler('ZYX', (theta, 0, phi), degrees=True).as_matrix()\n",
    "    pos = (np.array([0, 0, distancia]) @ rot.T)[:,None]\n",
    "    return torch.tensor(np.hstack([rot, pos]), dtype=torch.float32)\n",
    "\n",
    "def mostra(**kwargs):\n",
    "    pose_camera = pose_de_angulos(**kwargs)\n",
    "    img = sintetiza_nova(modelo_nerf, raios_camera, pose_camera, torch.linspace(1, 8, n_amostrasx)).cpu()\n",
    "    plt.figure(9, figsize=(6,6))\n",
    "    plt.axis('off')\n",
    "    plt.imshow(img)\n",
    "    plt.show()\n",
    "\n",
    "interactive_plot=None\n",
    "if TRAIN:\n",
    "    interactive_plot = interactive(mostra, \n",
    "          phi = widgets.FloatSlider(value=40, min=-90, max=90, step=.01),\n",
    "          theta = widgets.FloatSlider(value=60, min=0, max=360, step=.01),\n",
    "          distancia = widgets.FloatSlider(value=3, min=2, max=6, step=.01))\n",
    "interactive_plot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "750e007b",
   "metadata": {},
   "source": [
    "E agora vamos gerar um vídeo dessa visualização girando em torno do objeto:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf9d9184",
   "metadata": {},
   "outputs": [],
   "source": [
    "html = None\n",
    "if TRAIN:\n",
    "    from IPython.display import HTML\n",
    "    from base64 import b64encode\n",
    "    import imageio\n",
    "\n",
    "    video = imageio.get_writer('output.mp4')\n",
    "    for theta in tqdm(np.linspace(0., 360, 120, endpoint=False)):\n",
    "        pose_camera = pose_de_angulos(60, theta, 3.5)\n",
    "        img = sintetiza_nova(modelo_nerf, raios_camera, pose_camera, torch.linspace(2, 6, n_amostrasx)).cpu().numpy()\n",
    "        video.append_data((255*img).astype(np.uint8))\n",
    "    video.close()\n",
    "\n",
    "    mp4 = open('output.mp4','rb').read()\n",
    "    data_url = \"data:video/mp4;base64,\" + b64encode(mp4).decode()\n",
    "    html = HTML(\"\"\"\n",
    "    <video width=400 controls autoplay loop>\n",
    "          <source src=\"%s\" type=\"video/mp4\">\n",
    "    </video>\n",
    "    \"\"\" % data_url)\n",
    "html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ba8745c",
   "metadata": {},
   "source": [
    "Usando essa forma bronca de amostragem precisariam-se muito tempo (várias horas) para poder treinar uma NeRF igual ao do paper original com a resolução maior. Mas essas imagens pequena e com essa abordagem simplificada dá para ver bem o que a rede consegue treinar nesse tempo."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "123782df",
   "metadata": {},
   "source": [
    "# Seus dados e feedback aqui:\n",
    "\n",
    "Coloque o seu feedback sobre o lab aqui para podermos melhorá-lo para as próximas turmas e também 'calibrar' os próximos labs (idealmente os 80% dos alunos terminar em bem menos de 3h)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a48dd344",
   "metadata": {},
   "source": [
    "Preencha as seguintes variáveis com a quantidade de horas gasta no lab, a dificuldade percebida e a nota esperada (pode apagar o `raise` e o comentário):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e09baf2b",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "bd7bcb89e16451eac905de60a4e9033b",
     "grade": true,
     "grade_id": "meta_eval",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# meta_eval manual_graded_answer 0\n",
    "\n",
    "horas_gastas = None    # 1.5   - Número float com a quantidade de horas \n",
    "dificuldade_lab = None # 0     - Número float de 0 a 10 (inclusive)\n",
    "nota_esperada = None   # 10    - Número float de 0 a 10 (inclusive)\n",
    "\n",
    "# ESCREVA SEU CÓDIGO AQUI (pode apagar este comentário, mas não apague esta célula para não perder o ID)\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c7890cc",
   "metadata": {},
   "source": [
    "Escreva abaixo (na célula discursiva) outros comentários e feedbacks sobre o lab, pode ser em termos gerais, ou específico sobre alguma questão. Se tiver alguma dúvida que restou também pode colocar aqui.\n",
    "\n",
    "Quaisquer erros, por menor que forem (português, o jupyter não tem corretor gramatical para português e a extensão do navegador não pega na célula), pode comentar abaixo para podermos melhorar e corrigir."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bff57274",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "1f2631795cfc36bf2f1ec9ad84af8d32",
     "grade": true,
     "grade_id": "meta_eval_discursivo",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "ESCREVA A SOLUÇÃO ABAIXO (não mude essa primeira linha):\n",
    "**ATENÇÃO**\n",
    "\n",
    "**ATENÇÃO**\n",
    "\n",
    "**ATENÇÃO**\n",
    "\n",
    "**ATENÇÃO**\n",
    "\n",
    "**QUESTÃO DISCURSIVA**\n",
    "\n",
    "ESCREVA SUA RESPOSTA AQUI (não apague esta célula para não perder o ID)\n",
    "\n",
    "**ATENÇÃO**\n",
    "\n",
    "**ATENÇÃO**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bc1f42a",
   "metadata": {},
   "source": [
    "Fim do laboratório."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
