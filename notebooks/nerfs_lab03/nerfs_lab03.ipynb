{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5dcc5d19",
   "metadata": {},
   "source": [
    "**Instituto Tecnol√≥gico de Aeron√°utica ‚Äì ITA**\n",
    "\n",
    "**Vis√£o Computacional - CM-203**\n",
    "\n",
    "**Professores:** \n",
    "\n",
    "Marcos Ricardo Omena de Albuquerque Maximo\n",
    "\n",
    "Gabriel Adriano de Melo\n",
    "\n",
    "\n",
    "**Orienta√ß√µes padr√£o:**\n",
    "\n",
    "Antes de voc√™ entregar o Lab, tenha certeza de que tudo est√° rodando corretamente (sequencialmente): Primeiro, **reinicie o kernel** (`Runtime->Restart Runtime` no Colab ou `Kernel->Restart` no Jupyter), depois rode todas as c√©lulas (`Runtime->Run All` no Colab ou `Cell->Run All` no Jupyter) e verifique que as c√©lulas rodem sem erros, principalmente as de corre√ß√£o autom√°tica que apresentem os `assert`s.\n",
    "\n",
    "√â muito importante que voc√™s n√£o apaguem as c√©lulas de resposta para preenchimento, isto √©, as que contenham o `ESCREVA SEU C√ìDIGO AQUI` ou o \"ESCREVA SUA RESPOSTA AQUI\", al√©m das c√©lulas dos `assert`, pois elas cont√©m metadados com o id da c√©lula para os sistemas de corre√ß√£o automatizada e manual. O sistema de corre√ß√£o automatizada executa todo o c√≥digo do notebook, adicionando testes extras nas c√©lulas de teste. N√£o tem problema voc√™s criarem mais c√©lulas, mas n√£o apaguem as c√©lulas de corre√ß√£o. Mantenham a solu√ß√£o dentro do espa√ßo determinado, por organiza√ß√£o. Se por acidente acontecer de apagarem alguma c√©lula que deveria ter a resposta, recomendo iniciar de outro notebook (ou dar um `Undo` se poss√≠vel), pois n√£o adianta recriar a c√©lula porque perdeu o ID. Ou ent√£o voc√™ baixa e abre o notebook como texto (√© um JSON) e readiciona o campo de ID. Neste ano n√≥s tamb√©m colocamos um coment√°rio nessas c√©lulas que √© igual ao ID delas, para ser um failsafe em caso de sumirem com o ID das c√©lulas, ent√£o N√ÉO apaguem esse coment√°rio com ID (ele √© um fallback caso voc√™s percam o ID da c√©lula, ele deve ficar na primeira linha).\n",
    "\n",
    "Os notebooks voc√™s podem alterar √† vontade, podem criar novas c√©lulas, modificar as existentes, apagar (a menos das c√©lulas de corre√ß√£o). O corretor autom√°tico executar√° todas as c√©lulas e verificar√° a presen√ßa de erro nos `asserts`, depois haver√° a corre√ß√£o manual das quest√µes com aprecia√ß√£o da resposta e coment√°rios gerados em HTML. Se ele n√£o achar a c√©lula com os asserts, fica sem a nota da quest√£o, se ele n√£o achar a c√©lular com a quest√£o, fica sem os coment√°rios. Mas voc√™s podem escreve sim c√≥digo fora dos espa√ßo delemitado pelo `ESCREVA SEU C√ìDIGO AQUI` sem problemas, s√≥ n√£o altera a assinatura da fun√ß√£o. Esse espa√ßo foi pensado para facilitar a sua implementa√ß√£o.\n",
    "\n",
    "Os Notebooks foram programados para serem compat√≠veis com o Google Colab, instalando as depend√™ncias necess√°rias automaticamente a baixando os datasets necess√°rios a cada Lab. Os comandos que se inicial por ! (ponto de exclama√ß√£o) s√£o de bash e tamb√©m podem ser executados no terminal linux, que justamente instalam as depend√™ncias."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc2a98a9",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0156fd95",
   "metadata": {},
   "source": [
    "## Laborat√≥rio 3 - Neural Radiance Fields (NeRFs)\n",
    "\n",
    "Neste laborat√≥rio iremos treinar nossa pr√≥pria NeRF (Neural Radiance Field). Essa √© uma reprodu√ß√£o simplificada do trabalho de Ben Mildenhall, Pratul P. Srinivasan, Matthew Tancik, Jonathan T. Barron, Ravi Ramamoorthi e Ren Ng entitulado [Representing Scenes as Neural Radiance Fields for View Synthesis](https://www.matthewtancik.com/nerf), no qual uma rede neural √© treinada para reproduzir uma cena a partir de fotos com posi√ß√µes conhecidas. Essencialmente a rede neural aprende a cor e a densidade (opacidade por 'comprimento') de cada ponto X, Y, Z no espa√ßo.\n",
    "\n",
    "Dessa forma h√° diversos c√≥digos na internet sobre esse assunto, e pe√ßo que n√£o copiem. Podem assistir os v√≠deos e aulas do youtube a respeito, mas n√£o copiem o c√≥digo como resposta direta. O mesmo vale para as LLMs, n√£o pergunte diretamente a resposta para elas, mas pode us√°-las.\n",
    "\n",
    "Esse laborat√≥rio em 5 partes, que compreendem as opera√ß√µes b√°sicas com PyTorch (motivadas pelas aulas 1 e 2); o entendimento e a estrutura√ß√£o do conjunto de dados a ser utilizado na NeRF; a defini√ß√£o da arquitetura neural; o treinamento; e a infer√™ncia. Qualquer d√∫vida, n√£o hesitem de perguntar no grupo do WhatsApp. Esse deve ser um Lab bem divertido e recompensador. Eu tentei simplificar ao m√°ximo, sem que a NeRF 'parasse de funcionar'.\n",
    "\n",
    "**ATEN√á√ÉO**: √â recomend√°vel utilizar uma GPU Nvidia para o treinamento da NeRF neste laborat√≥rio. Habilite a GPU gratuitamente no Colab ([aula 1 slide 137](https://docs.google.com/presentation/d/1PYRArTIBh9vQ5r7DvEwGgbZp8FCkrwZnkQuFZ-z-snc)) ou utilize a imagem docker com GPU ([jupytergpu.yml](https://github.com/Gabrui/cm203))."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9be01ec6",
   "metadata": {},
   "source": [
    "**Exemplo de prompt que pode ser utilizado no ChatGPT**\n",
    "\n",
    "Pode usar LLMs desde que elas n√£o gerem nenhum c√≥digo e n√£o falem a resposta diretamente.\n",
    "\n",
    "Por favor, tenha cuidado para n√£o deix√°-lo falar nenhum c√≥digo, sen√£o ele vai dar a resposta de cara e voc√™ n√£o vai aprender nada ...\n",
    "\n",
    "```\n",
    "I am a computer vision graduate student and I am currently implementing the following code. Could you help me. My professor said that you shouldn't give me the answer right away, but it is okay if you guide me towards the answer so that I can discover it by myself. You should not output any code, as requested by the professor. Here is the function I need to implement (it is in portuguese but I hope you can understand):\n",
    "\n",
    "def entrada_senos_cossenos(x, escala=potencias_2) -> torch.Tensor:\n",
    "    \"\"\" Realiza a seguinte opera√ß√£o em um tensor de entrada:\n",
    "           x =[x1, ..., xn]\n",
    "        dado um vetor de escala de frequ√™ncias\n",
    "           escala = [f1, f2, ..., fe]\n",
    "        Calcula:\n",
    "           senos = [sin(f1*x1), sin(f2*x1), ..., sin(fe*x1), ... ... ... , sin(f1*xn), sin(f2*xn), ..., sin(fe*xn)]\n",
    "           cosse = [cos(f1*x1), cos(f2*x1), ..., cos(fe*x1), ... ... ... , cos(f1*xn), cos(f2*xn), ..., cos(fe*xn)]\n",
    "        e retorna a concatena√ß√£o do vetor de entrada e de seus senos e cossenos na escala de frequ√™ncias recebida, nessa ordem.\n",
    "           retorna [x, senos, cossenos]\n",
    "\n",
    "      Args:\n",
    "        x (torch.Tensor): Tensor de entrada a ser operado, tem shape (..., N)\n",
    "        escala (optional, torch.Tensor): Tensor com shape (E, ) que representa os fatores de escala \n",
    "            de frequ√™ncia a serem utilizados na transforma√ß√£o, (default: potencias_2).\n",
    "\n",
    "      Returns:\n",
    "        (torch.Tensor): Resultado da opera√ß√£o, tem shape (..., N + 2 * N * E)\n",
    "      \"\"\"\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f695ecd",
   "metadata": {},
   "source": [
    "## Imports e Baixar dados"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa2c0857",
   "metadata": {},
   "source": [
    "Temos as nossas bibliotecas conhecidas, j√° utilizadas nos labs 1 e 2, o NumPy e o Matplotlib. Al√©m dessas temos o tqdm (*te quiero demasiado*) para mostrar uma barra de progresso durante o treino.\n",
    "\n",
    "Em seguida temos a nossa estrela desse lab: O PyTorch ! Por enquanto vamos utiliz√°-lo em sua forma pura, mas nos pr√≥ximos lab, utilizaremos o FastAI que √© tem uma interface de mais alto n√≠vel. Mas √© importante saber fazer as opera√ß√µes de mais baixo n√≠vel no PyTorch para saber como o FastAI funciona de fato. Al√©m disso, voc√™s v√£o perceber que o PyTorch nada mais √© do que o nanoGrad que fizemos na aula passado, s√≥ que glorificado: tem muito mais opera√ß√µes, e consegue usar GPU eficientemente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "2e5dc47b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from typing import Callable, Tuple, List, Iterable\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "import torch\n",
    "from torch.nn.functional import sigmoid, relu, leaky_relu, softplus, pad\n",
    "\n",
    "from IPython.display import Image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13442bc7",
   "metadata": {},
   "source": [
    "Para n√£o ter d√∫vidas e nem dores de cabe√ßa de ficar trazendo tensores de CPU para GPU e vice-versa, j√° definimos a GPU como o dispositivo padr√£o para instanciarmos nossos tensores al√©m de nosso tipo padr√£o float com 32 bits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "1af40cfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "torch.set_default_device(device)\n",
    "torch.set_default_dtype(torch.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "490de53c",
   "metadata": {},
   "source": [
    "Verifica se j√° foram baixadas os dados do drive (ou do *fallback*), baixando-as e descompactando se necess√°rio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "41bc2fa5",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "e31039d5dc0a91a44a8ce8a7976ec8bc",
     "grade": false,
     "grade_id": "dataset_lab",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/home/fbaltor/fbaltor/ita/2023/CM-203/cm203/.venv/bin/gdown\", line 8, in <module>\n",
      "    sys.exit(main())\n",
      "  File \"/home/fbaltor/fbaltor/ita/2023/CM-203/cm203/.venv/lib/python3.10/site-packages/gdown/cli.py\", line 156, in main\n",
      "    filename = download(\n",
      "  File \"/home/fbaltor/fbaltor/ita/2023/CM-203/cm203/.venv/lib/python3.10/site-packages/gdown/download.py\", line 275, in download\n",
      "    for file in os.listdir(osp.dirname(output) or \".\"):\n",
      "FileNotFoundError: [Errno 2] No such file or directory: '/content'\n",
      "--2023-08-30 14:36:52--  http://ia.gam.dev/cm203/23/lab03/nerfs.zip\n",
      "Resolving ia.gam.dev (ia.gam.dev)... 89.117.75.63, 2605:a143:2138:6535::1\n",
      "Connecting to ia.gam.dev (ia.gam.dev)|89.117.75.63|:80... connected.\n",
      "HTTP request sent, awaiting response... 301 Moved Permanently\n",
      "Location: https://ia.gam.dev/cm203/23/lab03/nerfs.zip [following]\n",
      "--2023-08-30 14:36:53--  https://ia.gam.dev/cm203/23/lab03/nerfs.zip\n",
      "Connecting to ia.gam.dev (ia.gam.dev)|89.117.75.63|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 4260460 (4.1M) [application/zip]\n",
      "/content: Permission denied\n",
      "/content/nerfs.zip: No such file or directory\n",
      "\n",
      "Cannot write to ‚Äò/content/nerfs.zip‚Äô (Success).\n",
      "[Errno 2] No such file or directory: '/content'\n",
      "/home/fbaltor/fbaltor/ita/2023/CM-203/cm203/notebooks/nerfs_lab03/content/content/content/content\n"
     ]
    }
   ],
   "source": [
    "# dataset_lab read_only\n",
    "\n",
    "! [ ! -d \"/content/nerfs\" ] && gdown -O /content/nerfs.zip \"1VCtH2tt7h8XP8yKjYCKrIx8jQ0QiaCa-\" &&  unzip -q /content/nerfs.zip -d /content && rm /content/nerfs.zip\n",
    "! [ ! -d \"/content/nerfs\" ] && wget -P /content/ \"http://ia.gam.dev/cm203/23/lab03/nerfs.zip\" &&  unzip -q /content/nerfs.zip -d /content  && rm /content/nerfs.zip\n",
    "base_path = Path(\"/content/nerfs\")\n",
    "%cd /content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "5ce8966d-fe7c-4946-ac0e-11bc8df39bdb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/fbaltor/fbaltor/ita/2023/CM-203/cm203/notebooks/nerfs_lab03/content/content/content/content\n",
      "Downloading...\n",
      "From: https://drive.google.com/uc?id=1VCtH2tt7h8XP8yKjYCKrIx8jQ0QiaCa-\n",
      "To: /home/fbaltor/fbaltor/ita/2023/CM-203/cm203/notebooks/nerfs_lab03/content/content/content/content/nerfs.zip\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4.26M/4.26M [00:27<00:00, 156kB/s]\n",
      "/home/fbaltor/fbaltor/ita/2023/CM-203/cm203/notebooks/nerfs_lab03/content/content/content/content/content\n"
     ]
    }
   ],
   "source": [
    "# local data setup\n",
    "import os\n",
    "jupyter_path = os.path.abspath(\"\")\n",
    "print(jupyter_path)\n",
    "nerfs_path = jupyter_path + '/content/nerfs'\n",
    "\n",
    "! [ ! -d \"./content/nerfs\" ] && gdown -O nerfs.zip \"1VCtH2tt7h8XP8yKjYCKrIx8jQ0QiaCa-\" &&  unzip -q nerfs.zip -d ./content && rm nerfs.zip\n",
    "! [ ! -d \"./content/nerfs\" ] && wget -P ./content \"http://ia.gam.dev/cm203/23/lab03/nerfs.zip\" &&  unzip -q nerfs.zip -d ./content  && rm nerfs.zip\n",
    "base_path = Path(nerfs_path)\n",
    "%cd ./content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e325010",
   "metadata": {},
   "source": [
    "E vamos carregar os dados:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "ed75a8de",
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN = True\n",
    "\n",
    "dados = np.load(base_path / 'dados.npz')\n",
    "\n",
    "distancia_focal = float(dados['distancia_focal'])\n",
    "imagens_treino = torch.tensor(dados['imagens_treino'])\n",
    "poses_treino = torch.tensor(dados['poses_treino'])\n",
    "\n",
    "imagens_valida = torch.tensor(dados['imagens_valida'])\n",
    "poses_valida = torch.tensor(dados['poses_valida'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "971bd056-db00-44ee-832b-ae838a897aca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7fb8700d4af0>"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAaAAAAGgCAYAAADsNrNZAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8pXeV/AAAACXBIWXMAAA9hAAAPYQGoP6dpAABCm0lEQVR4nO3de3BcxZ03/N+Z+4w0mtHFutmSLcDBV4KxsTGwGwLasIQkkPhNNvvArkNSmzdZk2CoCoEksFXJgkm2asOyRWDhyTrk3RA2PBUgC1kSHnPZEMzNBINxkI2v8kWyZGlmJM1obqffP6icPt82BhvLtCx/P1WqOq0+c07P0aXn9K/Prx2llBIiIqIPWMB2A4iI6OTEDoiIiKxgB0RERFawAyIiIivYARERkRXsgIiIyAp2QEREZAU7ICIisoIdEBERWcEOiIiIrDhuHdCdd94ps2bNklgsJsuWLZMXX3zxeJ2KiIhOQM7xyAX3n//5n/K3f/u3cvfdd8uyZcvk9ttvlwcffFB6enqkubn5XV/ruq7s27dPksmkOI4z0U0jIqLjTCklIyMj0t7eLoHAu9znqONg6dKlatWqVV65Wq2q9vZ2tWbNmvd8bW9vrxIRfvGLX/zi1wn+1dvb+67/7yd8CK5UKsmGDRuku7vb+14gEJDu7m5Zv379IfsXi0XJ5XLel2JybiKiKSGZTL5r/YR3QIODg1KtVqWlpQW+39LSIn19fYfsv2bNGkmlUt5XZ2fnRDeJiIgseK8wivVZcDfeeKNks1nvq7e313aTiIjoAxCa6AM2NTVJMBiU/v5++H5/f7+0trYesn80GpVoNDrRzSAiokluwu+AIpGILF68WNatW+d9z3VdWbdunSxfvnyiT0dERCeoCb8DEhG57rrrZOXKlbJkyRJZunSp3H777TI2NiZXXXXV8TgdERGdgI5LB/RXf/VXMjAwIDfffLP09fXJmWeeKY8//vghExOIiOjkdVweRD0WuVxOUqmU7WYQEdExymazUldXd9h667PgiIjo5MQOiIiIrGAHREREVrADIiIiK9gBERGRFeyAiIjICnZARERkBTsgIiKygh0QERFZwQ6IiIisYAdERERWsAMiIiIr2AEREZEV7ICIiMgKdkBERGQFOyAiIrKCHRAREVnBDoiIiKxgB0RERFawAyIiIivYARERkRXsgIiIyAp2QEREZAU7ICIisoIdEBERWcEOiIiIrGAHREREVrADIiIiK9gBERGRFeyAiIjICnZARERkBTsgIiKygh0QERFZwQ6IiIisYAdERERWsAMiIiIr2AEREZEV7ICIiMiKkO0GEL0fM9riUL7yso7D7usq4xu+j12OwkpXHCgH/R/RlHkgVFX42oDvWErwtQHcFc6rjPME3+Vj4ns06RDlqj5POIgvVsaFcnxt7B+uQN1P/nPH0Z2Y6B3wDoiIiKxgB0RERFawAyIiIisYA6ITUqoWf3Vn+WJCo/kq1EVjQSjnx3V9LIp1pYqLrw3rz2gVI0YSC2EgJ1/G+pAv0OMawZqAEbtRQb1vtWLEgMJ4HuU7TzCCdeWi+Vo8T6Gg6xMx57B1IiKxqO+4blmIJhrvgIiIyAp2QEREZAWH4OiEpFwcPvKPjgVDQWNnLAbCerjOHFZTwQSUDzrLve1yZQTqWoProTxUWQ7lbCHmbXcmnoW6cKgE5bHyad723vw8qJsR+x8ohyK6jbnKYqiLOFugXFPaCmUJ6T/5crkAVSqAn0eVq4cjQ2F+VqWJx98qIiKygh0QERFZwQ6IiIisYAyITkjmtOagbxpzpYBpYyJh/DUvFBu97ac31kFdurEVyvWN9d52Nodzmh99oR/KZ104A8pOQJ/3wSeK2IYcvnbOYh0/qpuWhLpnjDZGI3p+dH0rvrfRwmwo9741BOUzF57ubZ/ehLGlagmnoItv+ne5fJQ5f4iOAO+AiIjICnZARERkBTsgIiKygjEgmhKqZR2/cBwjPU0JU/NUC4PedqKKsRgn0wtlN/kX3nZxBPc9o8s4z4jx2oB+3qgtlYe6aD3GVMLV/d52Kd8IdTXOAShHRMeAlIPPPFUyG6Gs8ruhLGOjejOC6XXcIP47GC/o62ZeU6KJwDsgIiKygh0QERFZwQ6IiIisYAyITggLTquB8tL5+KxMKOyLhYzjc0BBI49ZxNHxmHTCWGagjPGWQGXY265xMAZ0+gx8biY7+hiUM77UcY2N41CXTGCbSpWXvO2BfRjHaU2MQTkWb/K29+YzUNdWuwfKBwJ43saaAW87FMVlzV3jMaBgVP97iJXwmi6cg88m7dqjr2luFPclOhzeARERkRXsgIiIyAoOwdEJoT2Nn5VObzWGsIp62MetKqPOmIbtq6+6xmcwF/8knn/u/3rbbWmcSl2tx6UbImEczmufpstVhUOGJYXncZReGqGtCYfNAk4tlEeKeimHp5/5JdRdcCYe11xdtVDS6YSCo7gkhAphqiFV1e0vGal4TmmPQDmT1cfiEBwdKd4BERGRFeyAiIjIiqPqgNasWSNnn322JJNJaW5ulssvv1x6enpgn/HxcVm1apU0NjZKbW2trFixQvr7+w9zRCIiOlkdVQzomWeekVWrVsnZZ58tlUpFvvWtb8nHPvYx2bx5s9TUvD1N9tprr5XHHntMHnzwQUmlUnL11VfLZz7zGfn9739/XN4AnRx++zIuh903gqlhVl05zdsOGFOGw1H8nOUUdTwjimEPKVXwuAFHz02uS2AcJBIxYiYhjIsE4VB43JgRp6pE9TTzgLFMuBPC9scdHW8xlx8POPha1zivv7ZqxHxCAdy36lu+u5jH+NYjTw4K0bE6qg7o8ccfh/JPfvITaW5ulg0bNsif//mfSzablR//+Mdy//33y4UXXigiImvXrpW5c+fK888/L+ecc84hxywWi1Is6rVScrnc+3kfRER0gjmmGFA2mxURkYaGBhER2bBhg5TLZenu7vb2mTNnjnR2dsr69evf8Rhr1qyRVCrlfXV0dBxLk4iI6ATxvjsg13Vl9erVct5558mCBQtERKSvr08ikYik02nYt6WlRfr6+t7xODfeeKNks1nvq7e39x33IyKiqeV9Pwe0atUq2bRpkzz77LPH1IBoNCrRaPS9d6STWmsj/o5csDgF5dEDOmXOWB7jIMUQxjYqvhhQZRxjKNVxfDYmFm/2tsMyCnWVPKbICZqrVgf057uQwjw3TgDPG6z64lYB/LOsGKmF/LXJJC4hHnYHoBww8uu4Bf28kX+5BRGRQMQ4b1mft+xg3ZxTMRXPm9s4dE5H733dAV199dXy6KOPylNPPSUzZszwvt/a2iqlUkkymQzs39/fL62trUJERPQnR9UBKaXk6quvloceekiefPJJ6erqgvrFixdLOByWdevWed/r6emR3bt3y/LlyyemxURENCUc1RDcqlWr5P7775dHHnlEksmkF9dJpVISj8cllUrJl770JbnuuuukoaFB6urq5Gtf+5osX778HWfAER2peBjHt2pcTFcjVT0FOiQ4tOQfchMRWbBQZ5OeH8ShvR07cbirs+ZKb3twy39BnQq/AWUnaExjdnU7qgrrXGVMtQ7ooTLXLUKdwtndMpLXw4+XfOpv8bh9v4LyF5cfhHJzvZ5O/fSzmPm7WjCuqW8KdyiI1/SUdvzX0bPN114hOjJH1QHdddddIiJywQUXwPfXrl0rX/jCF0RE5Ic//KEEAgFZsWKFFItFufjii+VHP/rRhDSWiIimjqPqgJR67882sVhM7rzzTrnzzjvfd6OIiGjqYy44IiKygssx0AlhRx9Oj77n10NQvvGzerp01Zi2XK3iVOTmRp32pq4OP4MNZnHpg0FHx1vitU1Ql2zAJRYUzqyWwa06xhIy0utEjH39s6VDDbj6ayiJ5xl3dDtq65uhLhFoh/KyhTh1XETHvNY7GAMaM9LtOAGdqmdwFONDv/4dXn+i94N3QEREZAU7ICIisoIdEBERWcEYEJ2Qcnl8LqVc0eVKBeuCxvM5T/5mp7c9OIDP3DjtZ0A5G3jZ2w6rONTFohhbMtZfkJLv+SOzDVUjRY64vmdujI+FtQncdzzQ5m0P7H4N6moUvvefrsX1uiK+g+eMWJn5cVSVddxttIDtJ5oIvAMiIiIr2AEREZEVHIKjE1LZGD3al9HDXa3tC6CuEkxDOTy60dtubsHhrZxrTC8e1TlmolGcplwewmzYZWOV04Cjh61c18g8HcTPflXR7QgXcMp5oR/bWCju8rbzxvRoJ5yBcqMxzdzxpddRgVOgLhjGKeiS3+Jt7h3AFWmJJgLvgIiIyAp2QEREZAU7ICIisoIxIJoS7npMr4h6zQ1/DXVjg1uhPM3Z421nDmShrrYNp2Wf2rzb2w65mNZm9zaMiwQdjPP06JdK016M67xRwnjRDt807U+fjTGfuHHcZLv+3OgksA1hY9mK3H58bSyu/+R7q2dCXSTWAuU/btHxsLfe6heiicY7ICIisoIdEBERWcEOiIiIrGAMiKaESkU/GDR8YA/UVY1nhsplHedJN+G6CE44BuXp6YK3PWrEbQ5U8c+nUMITXZTT8ZfaIaxbYWS2+bLv0MbKDTJexphQR50+b43CGFBB1eGL64z35ztvFMNd4pYLUM5lzaUciCYW74CIiMgKdkBERGQFOyAiIrKCMSCacopjmM8tEGuAciGvn/3Jj2B8ZcYpw1CORfQSDG2tUaib1dwI5R19mJft3Kw+T88ufA4ob8R5rv9/p+t9B3DfFd3Tobx3ULd5cBjjNo7CwM5gxngOKKFjQrUJ/PMfNZabyBzcL0THE++AiIjICnZARERkBYfgaMrZs3cnlIeHX4HypWfoqdbdf5GEukQMP5OlmvSwVFd7BOoywzjk9qlZrVDOvbHZ255pLihqlJ1PNXnbjQtSUJffj+cp+kYN1683hhBn4TTsJxUO0e3uzXvbA329ULf/IK6eKg5O4SaaaLwDIiIiK9gBERGRFeyAiIjICsaAaMrZvPl1KJdd/Jy19Cs67rN4EcZbDvZj+plZcxLedrbfWP7amErtjhtLZ/+1jgnt25iDutpPNkM536+X+26cjQeuFHAqddkXQPr4JzHuNHwAU/58/GNtUL77nu3e9gsvvwR1gQguyV0qYvyIaKLxDoiIiKxgB0RERFawAyIiIisYA6Kpx8GHbNJJLC86Xcd1Xlh/AOq6TqmBcrmo4zHjRYzx1DRgap5qBWM37nT93FDn/zkL6uIh3Hcoq2M3VeMhoXAcn8cZzul2DAxjfOillw5CWWJhbHPK9yevsA2Ow8+j9MHibxwREVnBDoiIiKzgEBxNOdOacerxtDRmiD44oKdTt7ThMFqpisNSz72kM1pv3j4Gdd0fnQbl3u24Oulbe/R5LrqgCepeeR2ne+/s0xmwu2txVdatb+Bxd/qmWndMwz9hN4yfKQPGXHEnrN9vFE8jiRS2MXNgp7etjOE6oonAOyAiIrKCHRAREVnBDoiIiKxw1CQb3M3lcpJKpd57RyKfREKnkbnh2z+EunTtIJQ7Iv+hXxfEqdUHhjCVTc9+LPspF/90gmLEW3wf78zVGHIF/I5Suh2xKH4uDAaNz4m+l4aMc3bNwJhWOIpTuHeOXOJtD5fmQN2Onbuh/NuHf6Tbm8FrSHQkstms1NXVHbaed0BERGQFOyAiIrKCHRAREVnB54BoSmhp6/C2gxF8wGXP7n1QHi3r52pK+TLURSP4mcyJ6HQ6ZrC0ksHlCtIKl2sQpdPkBAJ43PuewjYW8voZo+7zcFmEWadieiB/Cp3BIXzGqf8AtiEawxjQroxeqmI8bKQsasAlIurqW7xtxoDoeOAdEBERWcEOiIiIrOAQHE0JHTP1lOK9u7ZCXa0xDFXyZa02Mu9IvmR8I6KHqcyp1CqAx90ygK/1D7u5VXx1tYJDf0FHT8Ou4szwQ87rz/btGm/AzMhdMsoBR7e5ODYMdZUY/jto65zvbe/Z8YbZCqJjxjsgIiKygh0QERFZwQ6IiIisYAyIpoTcwDZdKDRAXcWI1dQkfOUwxkgKRuYdjNTgvsOjWL7317nDti9kpNNpbcL0JGVf4EcZK7qa07/9tYEQHjcUwL3DxrRyN6unbQcF32x27x+gXDiIq8USTTTeARERkRXsgIiIyAp2QEREZAVjQDQlbN32lrftGMtQR8OYMufj5+gUOaOjFWNf4zNZ+PCrldTVt0L5rz/3MSifGvmdt12p4Hl+/jQusx30p+pxElDnGFEgJf7ngPChoYr53E8Q40lvvaWf58kW+qCuNlSC8t6+fiE6nngHREREVrADIiIiK9gBERGRFYwB0ZQwls9729t27oG6Oac1Qbnii6E4QXxGaMSICdWLjh+NuRibGRpNQ9ktZaGcSunzlFxzWW1j+W5fnGckj8svDGVw6Ya66Ki3HXUwviUhI3Oc8UxRwNF/8iNjY1C3u3+nEH2QeAdERERWsAMiIiIrOARHU04oHIFypYJTlWt9qXhCOAInlSIOwcXGdTKeSgCnKUerGXxxdQiKJV/6naCx+mjUSKFTHNdDcIEKrnIaKCfxPGU9dJZK4HEKxsqrFWMxh6DjX6XVePNEHzDeARERkRXsgIiIyIpj6oBuu+02cRxHVq9e7X1vfHxcVq1aJY2NjVJbWysrVqyQ/n4+UU1EROh9x4Beeukl+bd/+zc544wz4PvXXnutPPbYY/Lggw9KKpWSq6++Wj7zmc/I73//+2NuLNGRCIfDUHYVxkHyeR3nyeYw5lMxl7j2LVlQl8RY0swOnHadyWGMqLlRx26Gc1WoK5TwvImobuPcjlGom3sKvnYsq8uZcWzv2AgeNxDBOE/AP/378FmGiD4Q7+sOaHR0VK644gq59957pb6+3vt+NpuVH//4x/LP//zPcuGFF8rixYtl7dq18txzz8nzzz//jscqFouSy+Xgi4iIpr731QGtWrVKLr30Uunu7obvb9iwQcrlMnx/zpw50tnZKevXr3/HY61Zs0ZSqZT31dHR8X6aREREJ5ij7oAeeOABeeWVV2TNmjWH1PX19UkkEpF0Og3fb2lpkb6+vkP2FxG58cYbJZvNel+9vb1H2yQiIjoBHVUMqLe3V6655hp54oknJBaLvfcLjkA0GpVoNDohxyISEQmGMAZUMuItri89TTSBfwIqD0UJKP0ZbdYMTMXT3IjnebOKx5qW1OcZK5jP4+Bi3/mSjus01OFzTLNn4N/am2Xf8t0ljDslajHmU8awlQTguSCsNJ8Lcl2MPRFNtKO6A9qwYYMcOHBAzjrrLAmFQhIKheSZZ56RO+64Q0KhkLS0tEipVJJMJgOv6+/vl9bW1nc+KBERnZSO6g7ooosuktdffx2+d9VVV8mcOXPkm9/8pnR0dEg4HJZ169bJihUrRESkp6dHdu/eLcuXL5+4VhMR0QnvqDqgZDIpCxYsgO/V1NRIY2Oj9/0vfelLct1110lDQ4PU1dXJ1772NVm+fLmcc845E9dqIoPjG1ab1TUb6ob6eqBc5xt2yxdxmKlaMVLk+KZPj47jOfN9OLQ3XsI/p6GcHuLKG9Ol//I8zNC9e59Or1Os4tDeH7fjMFvYl+InbKT4qYsZ2b1LeN5IIuVtO4LjjRdcchWU1z/1C2+7kOfsVJp4E54L7oc//KEEAgFZsWKFFItFufjii+VHP/rRRJ+GiIhOcMfcAT399NNQjsVicuedd8qdd955rIcmIqIpjLngiIjICi7HQFNCU3Obtz1/3iKoG6jFOM/I2Fa9PYLToasuxkwqvjQ+1/zzdjypY65yaqxG6q86gu/8yfqNI9gGo00zp+nzXvGpaVB3cBiXchAjFU88nva22zumQ13VxWnZiaTOcsIYEB0PvAMiIiIr2AEREZEV7ICIiMgKxoBoSli0aKm3fe65H4G6bSGM3QSCOv4Sq8E/gaIRQhkc0M/KfOEvaqFuzixMkdPTiw8KLZ1T423/cTfWNaXwvGFfm159C5/PmdWCqap29eljjVcxlpSoxWeIChWMH7Wm9bGibRgr279jI5SDjpHHh2iC8Q6IiIisYAdERERWcAiOTgjmpOVf/hsOH0XrP+lt7xrDX+vpp58L5bHeDd52pYLDTKGAsUxoWA+jffhD2Irmevz89sX/NQPKPa9lvO0L/6wR6gLGlOc+37Da//p0G9Rt7cEp0H/YHfe2B3DxVAkGsY1VY2p4MabbEYzWQd15558P5U9+SU9f/5f7sb2PPrlXiI4V74CIiMgKdkBERGQFOyAiIrKCMSA6IXz8wmYoXzAXp0Df8WS/t71py2tQVxPDOMisiF5GYXwc0/SUjCUVhgs63rJ93xDUlV2cHr11WwHKubyOm0yP4XEreUwBFPalzKlrxBVRa4zXnnuWjkv9n99koC7vGqu/GtGz0XHfdSthHGdHAa/paKOevv53/w+m/GEMiCYC74CIiMgKdkBERGQFOyAiIrKCMSCyJhnHGEpbY9rbHsjjcyeX/2UHlPfsxGdj+vZs8bbdCh539CDGZmTrQd0G4zmgfDwB5drp+jNaMoGf13JjGD/a8AY+lDPsW+ohlMQ25McwBrS7Ty+7HW3AWMzruzCNTzKtn1WqbsfUOzUDB6BcNB6gCrTqNlYTeJ0CCvMQvb5Tn7frtBqoa2ysh3KhqNtUqeBxIoLXeLyE773iMuXPyYp3QEREZAU7ICIisoIdEBERWcEYEFnz1Y+dDeU9vpjJH/owZlIXxHjLGztwyYKOyBvetuvgM0OFQhbKpc1jettoUyRtrMfQos8bdnB5a0cwb9zBgxir8T+C89zLw1AVeJfVvB97vP+wdSIi+wd0G8tbMO5U2YfXBa+ayHhol7ftJvG1yTqMq+VLOjbzhy1Yl07j0hSBvP5XMrsF6/6sqwHKI+N41e/97YvedrlqtpimMt4BERGRFeyAiIjICg7B0QfmQzMwnUsigr9+Y77puLEYTofesWMAyi21OFTTntDDVhu3jOCJA7hMgn8AyFzmwVHGd5Qedisbq4uaL3bMYTV/2dzXPLHv0K65r/HigG8oMBiNQ9244BAc1opEk/pnkHNxuLE53ovH8l2oagWHzapGI5Wvja2duJxEcx1etxlNuAzEolPave0Xt2IbaGrjHRAREVnBDoiIiKxgB0RERFYwBkTH1bzOVm/7L8+aA3UHxzCukM3rcqVqLCVdxPQtThKnRJ82T0/9vXUR7lsa3QPlX5fS3vb4kLFvLR63WNbLGwQjuEzCWB7jUGHjrykY9MWPisa+CdzZVTpOoqqYmiZu7DtW1MdtXJiCuvGYMQ07iJ8xT2/b5m1XImNQFzXeQKWsfx6OMj+r4r7BoC7/4bXtUDf9dEzbE49jqqGWFE7bppMH74CIiMgKdkBERGQFOyAiIrKCMSA6rqYldRr/3gO4pPVoAZ9D6S/q5QFcI+ZQqWDZzODfdao+z++eOwh1USMhzfhcve9+IwYUieLyBvNCOgZUW4NtKBlxqlgU61VA18djGFuqM2JNyhcDSiWxDQ1JPO7QiH4/b0XwvQ3UYkwoFMLXxkQ/IxVJYkyrYMZ5fJfGNR6BUhI0vqF/IKMFjO0VFL6f3n348xktMf3OyYp3QEREZAU7ICIisoJDcHRczWtPe9sjecxwvdNYFbTsGwJylZnqBYdxqi4O8/x/j+mVQI0Zz1Kp4vhRf5/OWl2TxinBFRf/JNK+rDG19diGomD7Y2Fss//t5UvYhgrOloYxruZpOLzVN4xvKBXX5wmGjaG9Oty3aFzjSJMefjTXIY0Yw2xlX3od/xChiEgwiNfCcfR5nSBew7wxVJmswdVVgyH9s5wzoxXq3tzTJzR18Q6IiIisYAdERERWsAMiIiIrGAOiCRUNYUxif0anexnOYeqXccH4Szikfx3LZSM+5OJnpbIR1wlF9HnHK66xbxTKFUcfu1TGmEksgucdGNfxi1pj+Ygz5uBxy2Vs0/Z+faxgAc9TW4PHGh7R0737hipQF4jgNR0c0Md1jfnosZgROysa09l9zagqfK0R5pGq79hmTM4x15Nw9YGDAazbPYjLY7QksN6t6vc7q7UJ6rbsxdVhXbORdELjHRAREVnBDoiIiKxgB0RERFYwBkQTakHXdCgna/WDNNEoxnwGB/BhmGhQj+87Ru6XYgljM5UQLvs8nrhAH9dthrrhcYypVBr1sUYr+GxSrLoDyuK85m329GEMqx+Lh8RQxou+53VC+NzMKJ5WxPfsTF/JiK/gI09SqeprOlA4A4+ruqBcjuAzN9Vx3cigsXx3VHZDOeY+r8+pMI5TLY9DOejoeFE4hCl+Do6OQrnGuE7Tmxq97cYwxtWWnn4KlJ9/c5vQ1ME7ICIisoIdEBERWcEhODomAWM67twZLVA+MJzxtgeLuG/BWOU0m9NDNa7gOM1IcDmU36xiub+n19sORrP42twwlGv7H/O2hwMzoC5YNwtfqz7pbafGnoW60cIuKEeM6dL+xUgDjjHuZE5j9r1fZQ4/qk4o55xub3tvHw4Z5vqfgXI6gu99Wos+dtFNQl0mcBo2Mf45XcjicQ8e3ATlQEC/90gYhxvHjLc6f9o0KFeLeiyz7OBrO5sbofzSFr3aatVM0U0nHN4BERGRFeyAiIjICnZARERkBWNAdEzmdGDMJxzEOMiAb9HToTzGfFxjDN/xTVWeOQOn346FT4fyUw/9bygvm6ePtX0QYxuRKE5F/lBcTz++ZPZbUFdydkL5mf52b3s4fD7UqTGcxlzvDEA57FuN1ExP4xgxIf9yB8UKxkgOVP4M23jwt952aM9rULe8Da9/qqEWjxXQ06VrjKnVwxmMaa3/g57OPm/xx6CurhHjbMMDegq3GZkJGzGhF3bhyrh/PlP/vEbyuEpuIICfkVsa6r3tfYN4HDrx8A6IiIisYAdERERWsAMiIiIrGAOiYzK3A1Pi7B3MQLl/ROeRiUQwzUqxiLlsUmn9zMdp886BupfWPw7lvr1Gypy5KW+zfQampxkpY4r/oYM61vE/b2Ic4dQW/JNIRnRsw6m+DHUH3SVQjo//GspOzJdayPhLMx8DqvqWl8i4+IzT+NB6KDfX6mdhCjWYZijr4oE74xinGqgmvO2A+WhSED+P7ty22dsOGSly5i76CJQ3vvAbb1tVMdYXMOKC+QrmFuov6vqBg71QF4snoFyu4PulExvvgIiIyAp2QEREZAU7ICIisoIxIHpPiSim17/ywqXedkNNHOre3HcQXxvSz3VUjGWdkwkc30+m9PMvB4cxNjOSGYRyR2sdlBMx/VmqPrkf6kaHGqA8mJztbTcnXoe6raO4dHa0VgdKaoL4zFDBmQflXAGf3wkE9HNB+TFjSe5a/NMrVXWcamgMYya1zj4oj5d1vWrA6x9N4c9qUxavU9W3rHg0lMM64+dTk9D79vdhbGb2fIxTpRp1Tr3hAXyeyBF87/7lu0VEtg/qWOCMOnxuKZPHZR9a0r6lKDLYfjrx8A6IiIisYAdERERWcAiO3tMZxiqniaiekjs0itN8X9+Gq2qquB5SKeRxyCQcwxVSa+v0UFk2g2ltCkWculsp43BRvqQ/S9UaKXFcY/XOakyn9Rmt4NDScC8OH4Wj+k9k+ixM8VMa3QvlQCQF5dHsHm9782s4pPjhRTgsWI6k9XFCOKyWK+OU5/ExPRU5FHShrmjM93aj9VAuFPWQaI2Dw5rVBF7T8y/W7zdRm5Z3k/CtfDt0wEixZEw5N2Z7Q/3QmPGzqhopi/iReUrhj5OIiKxgB0RERFYcdQe0d+9eufLKK6WxsVHi8bgsXLhQXn5ZPyGulJKbb75Z2traJB6PS3d3t2zdunVCG01ERCe+o4oBDQ8Py3nnnScf/ehH5b//+79l2rRpsnXrVqmv1+PMP/jBD+SOO+6Q++67T7q6uuSmm26Siy++WDZv3iwxY8yfJqeQMUi/6JQZh9lTZOfePihXA5iypbNOT7XeG8Apwk0tuNxyuaxTuASN9C01MXxtQw2me4lHdCChovD3rHc3fgBK1Oh4RS2uJiHTT01DORTWx3WMJRXqUtjGahnLsYhehuDDZ+MU7XjMeG3VN7Xaxfe2563/wdfW62nk9dNw2YoDg8YSCwMboTw2qpdRaJ25AOrqGzBlUUOjvo4qiNe0b/8eKPvXYAhGcN+0sSTEyFAGyq7vutY3YBzt4DDGDeMxHR9LGP9P8uMYP6LJ76g6oO9///vS0dEha9eu9b7X1dXlbSul5Pbbb5fvfOc7ctlll4mIyE9/+lNpaWmRhx9+WD7/+c8fcsxisShFX2A0l+PcfiKik8FRDcH96le/kiVLlshnP/tZaW5ulkWLFsm9997r1e/YsUP6+vqku7vb+14qlZJly5bJ+vXr3+mQsmbNGkmlUt5XR0fH+3wrRER0IjmqDmj79u1y1113yezZs+U3v/mNfPWrX5Wvf/3rct9994mISF/f28MxLS04rtHS0uLVmW688UbJZrPeV29v7zvuR0REU8tRDcG5ritLliyRW2+9VUREFi1aJJs2bZK7775bVq5c+b4aEI1GJRqNvveO9IFpN8fhR/BZH6VGve1d+w9A3SkzjHiRb0nl9maMBWCkQ6RS1c+31DW0Ql00YKT4dzAtf7mi4y21ahjqFszF5bz3HtCvLVcxHZAEcAg4GPLHZoz1C0L4LI9TxOeCHF9gJBo1Yj7GsQK+a1ocxWs6swNjZdGAXo7Byb8EdUnjM2Wi+UwoV5p0nCepnoS67C5MkXPQ0bGmfAWfTUrV44fMim+JBcd48Md18VmlsBHPa2rVx+qahz+r7O8w/jXuex6sIY3POOX7MAUTTX5HdQfU1tYm8+Zh/qu5c+fK7t1vP3zY2vr2P43+/n7Yp7+/36sjIiISOcoO6LzzzpOenh743pYtW2TmzJki8vaEhNbWVlm3bp1Xn8vl5IUXXpDlyzF5IRERndyOagju2muvlXPPPVduvfVW+dznPicvvvii3HPPPXLPPfeIyNu33qtXr5Z//Md/lNmzZ3vTsNvb2+Xyyy8/Hu2n4yAewV+LQqEA5f0H9RBXKGTsWzGGgOJ6uKWxGYdtdu/PQHlkVKermdY6C+rqOz4MZZX/I5SHs3qYJxHBlVbrHdy30PxRb7s4iil+CgM4ZTvYpqdsVxUOuVUEy4EcrtLqTx8UNKZwG4N5opQeVgtEF0FduIgjCvGIHiYcG8bh0UDIyDgeeBbKjvjrsRUlYxbzgcyb3nZL1wVQFwkbq5z60iwFAlgXDOLvSCCEQ3DjBX3isbFRqKtJpaE8sEeneqqtqRE6sR1VB3T22WfLQw89JDfeeKN897vfla6uLrn99tvliiuu8Pa5/vrrZWxsTL785S9LJpOR888/Xx5//HE+A0REROCok5F+4hOfkE984hOHrXccR7773e/Kd7/73WNqGBERTW3MBUdERFZwOQY6xJIPdUF5KItTk9O+VVCbUzgOn1cYV/CnWYlH8fNOvlCEcsmXyqZ3x2ao6zr9LChn9+OU4aHcG7p9tdiGcAhjKNN8KYD2VzFmEnJfhfJoXv+JBFMXQp2b3QDlRATjSUFfSiPHXIIAi6KUfm06sAXqhsJ/BuVw9f/q9oYxcBMwYk1m+iC/irFQ6bgRvxPfz7KxAeNd27e9CeViQcdulPE7UCxiG4OhMJQrFT3FPm/EgDIZXMbCf+iAcVHNmNDoGMYCafLhHRAREVnBDoiIiKxgB0RERFYwBkRyWhsuFWB+KgkG8DtN03Q6l3QtprJ5ZRum6R8d188QxWtx7H9kCPMDFl1dP7QP4yCRIMYRZs3DeEw8cKp+7a7fQZ1bMmJY9b5jGWljDgbOxuMmZunjZLG98YIRB1GYrsZx9Z9XqYTxoZCx3ESxrJ9jcgKvQ53EMeVMruYybzvqvgJ1qohtdMtmnE0HfqoutqG9ay6UZ6UXe9s739oEdbu2vAjlQEj/HvjT8oiIhFyMzdSl6qAc9P0b6tuJS6KrCqbxcQJ632oF0zM1pPA6MQY0+fEOiIiIrGAHREREVnAIjuTDxoqnI8b06LoaHGZLJ3VWa2MES3J5TNszbYZOQhsK469bxZgHXHV9BzOm8u7dgSt75oZxqGnmh87R7Z1+GdSFg9jITZv+oPc1hm3iEWMq7x69b0JwtdGhPA65BY2VTItKD3GNjOCwVCyCn/3GinqoKV/E48QSL0A53TbXt70E6kIpzDguxnBYzDf0Fw5jFvr8KE553rbht952/z7MASk4MiZBX3qdchHTA/kzWIuIRIv4++X6Uz0ZU7bNocpIRJ8nHjaGhtNJKAdE/37t3MdM2ZMR74CIiMgKdkBERGQFOyAiIrKCMaCTVNQXjzmYxfQnuTEcw583C2NE/hUvC8Z4fqoRYypz533I2969G6dom8sB1NbojOnjjpnWBmNL1XFMrzO8/RFve/1TeNwPL8BVNtvqdFqZiMIl4IPj+N6zeT2Vt2IsU3HIqqZmQEx88QtjiQLHSEcT9MWL3HF8764RD8v0veVtjwxug7poDabMCUUwflfxrU66txenPDfVHITySF6f1wjfyZjxr8Nx9L4BI25TMuM6EZz271/SY9xYE6Imgtcp4ov7uMY07EgI3+tp09u97YFhXCV3rGCsPUFW8A6IiIisYAdERERWsAMiIiIrGAM6STXV6eddWlI4dh5QFSib6Xb8oY79AwNQZ6b4HxvVaXC2bN8LdXVJXCV33HfacBDjHkowriAO1h/I6NhGuYwxlGoJY1zVvD5RKWDEcYy0Q5WqL3WNwjplLqpgxHkCgYivCmNlThBjGyGl2x+PG88XGadxXX2sagXPWRjFOE7AwWd7Ar7Y03g+C3X7x/HhnvqUPnaxjI2IRYyYlu+9BwN4HPNnVy5hPE+U71oYP9eg8fOJ+86bL2KqHdd45kn5flGnGel/GAOaHHgHREREVrADIiIiK9gBERGRFYwBnaQ+doZevqApiTGekBFz2LIH82j5q/uHM1DXNHMWlDdu0s+p5HIYiymVMdYUielx+nwB64pGPjEzGZnre74lEjKWOijhsaJRX1xBmbEmM66jP6MpYwloZTz3c0hcJ6xjXIEwxoDCUczD5gZ0rClqtMGMS1WrvhiWEXQz9zWXx/Yv2R0O4r6Dw/hczXBOH9sx3nvQiNFFlH5+qmTksnNdjLcMHMD6aS06X2A4jr+LZrwo4yvHwvhzHhnJQLmtIeVtV6tGcJImBd4BERGRFeyAiIjICg7BnSQuWHgKlJed3ultm+n/8yUsF4zRC/+wWyiByxfMOx3T9jz4q53edtFI22N+/hkb1cd13YqxLw75OObwl2+IKGKkbwlHjOEu36Fc4zgBYxhNHN1mJ4THccrG8FcEp5Ur3xIFQaMNoSju6wb1sdwA/lmaK9JWKvralI3hrVA4AuVquWzU6/cXM4YBHQeXm3B811wpY2q18fOBVVCNUUxz+M4sh31DaclkGuoObMNlIOqTesmFzBi+9zltmAZqOK/r53e2QF3vAE5XJzt4B0RERFawAyIiIivYARERkRWMAU1RNTGMZXx2+ULcwRdXyIzhVNeBMRzfd4wUMw21Ou7TPOdUqDtv2QIo9+7XY+07d+2DuswIjuH7p8qWqxhzyOdx30jYjNXowENtHOtiNbhMddWfqif47sskSECXQ0aMp1TCqeHhGMbDxLfkdcSIq0WN6cbiiyepAO7rnzotIhLwTb0uGtOwIzGjjXgWCfuuW00S2xs3fh7+a2pOmQ8bU91dX4zIjKMFjEsqDv7bSaXT3na6AZeTyOzFJTxKVR2XShrxx/0ZnObf0aSPO5w3rwRNBrwDIiIiK9gBERGRFeyAiIjICsaApqg/n4/P/QSNWIc/Q8ur2zE2E45ifMJMXe/4ntvo/ujZUBcw1g64+kuf9rbXb9gMdZt7cEnocEQ/w7K9F5d5GBjEZQUaGvCZD/+zMY6RfsaMAY37ltmOxPBZGHNJhVBML40QNGJAgZCxfHRdEsrKFwvxpwoSEak1l7gY13GfYATjOubPzv9ex43nfGIJPK6ZiidWo+MmtdgkOb0W2x/yPSc0OIjPzdTVYvzF/17NtEn19SkoRxK4NEI0qn/u0Qg+x9TYis/vjPnaoRy8Lp3TGqHs/z2Y4YsHiYg0pfB3YtBYlp4+GLwDIiIiK9gBERGRFRyCm0L8U2MvXIjTo83UNUXftOADWVxZMhDGYZ24MSzS1tbsbbc047TZoUwOyomEHsJKp3Ho5bRTO6EcDutfxzFj1mzcmHJbn8bhokxWp5Exs2H72yAikvFdi3QdDsWUKuZ0Y92mlDFs4xgrxzY04FDTaFGPcVWMacxxY6ispHTKn7EipsSJG2l8gkHdpto6PGdNLbZRjLQ3Cd9wZCCMxzXT9iR9q4iGo3gN48Y0/6DvWCOjOK0/bVy3qrFCatLXpnpjaLW9vQ3KL/32CW+7UsxDXdHFf2dNCf1zNqf1f7hrOpTXvYopf+iDwTsgIiKygh0QERFZwQ6IiIisYAxoClk6W49r1yVi77KnyPZ9/d62Y0xFPmf5h6G8ceMWKC9cNM/b3t+P03NbpmFMqDCupyoPZTDW1NyE4/3jvum7yRps/7QGY4qwkTLHv7hngxFzGDdWRI2E9M4Nadx3yIiHRdM69mROEQ66uLxEbQ3GSUpKv59qHN9Pwti3UPEtfWBM2Q4baYcqvhVRG4y4WshIxWMeq863+m24hNO93TIG3tIp/d7NlWLjYSyHfFP3A8bPJhnHfzMlYxmLYFXHchKxZqiLhPG1dY065pUZwOu/d3AQyqfN7fC2q1Wcjp40/j6eMn7HXWP6Oh0fvAMiIiIr2AEREZEV7ICIiMgKxoCmkLTv2ZKXtvRCnfEYkOz1pTQ5/cwzoa5rNqbxWbr8LCg3+uIi23b14b5nYeqU1za95W2bS2XXJDD2NJrXY/qBIP5qmnGdzCiO//uXl641xvcLJYzrJH1pcMwlrJ0APsOSrNH7VoxnSSJGTChslP2htZCxpELCiAkVfWGqYeM4CeM6FUv6OZq6uLn0Ab72kBhQrT5vqIR1lRJec38czggXSTyE7ycW1zEtZTx7lDJiQGa6o3x22NuuljAlTqmCx+o4rcvbPvPsM6Cuksefs7Nnu7cdMZY1TxrXeM4MjD1t7u0XOv54B0RERFawAyIiIis4BDeFNCb1MMjMFpwO/eo2HJIbD+mhpaWLPgR1Lc1pKM8/fRaU9/fr6a6nzmqHuqqRyibmG2qqN4bRjNEh8c+UNYfcIoeko8G0MSnf6p4hI8VMKITDdf6hv4CRaTpmDqPFfSmAjGHMRNxoUwhfGwrrN+RP6fN2nbFqa0zv688OLYLDgG8fS7/3eAyPGzaukzmbOOobqgxG8L2PjxurzvquRSqJ51GukVrI93N2jRVPjdNIJIrDj/40UQHjuJE4TjOvqdVjgTVGOqO6ZsyGnStmve3M/gNQN1rE359Fp86AMofgPhi8AyIiIivYARERkRXsgIiIyArGgE5gXS2YymbJqToVz5t7cEXRFiPlTFuHnmq9YP5pUFcqYswkbKRWifhWBq2pwQH+oJkixxf7qK9PQ10mi+n0/asB1BspZsxYRtiYbhyP6vOaK4jGjfQ0Md/8aHOl0roEHjcS9cdUsBHKiOuEjVhHuVL11WGlMSsb9jWvoTmH3h9viRoBFvUu+759bN1m10hPc2hsRn8+DRrTrgPGkgqO79oEjeUwzFVyReGc7khMx9kiQWyTGVNMJvXvRdDB42QyuIxFcto0b7swgCmjcnn83UsZcbakL743UsC/B5o4vAMiIiIr2AEREZEV7ICIiMgKxoBOYN0fng3lnn36+ZyhPI5bJ4z0NOecoV+7c+ceqPvQ7JlQHs5mobxth95/8ZlzoK7/AKbE37NPx6Jmn4LPWpTGMe1NqaKDQMlabG8ogLGBwQyWa/3P9hj7GqEBiUd9S38b6VvMcEXIiN34KQfjE0a2F3GVvw4PbMap/DEucymAahXP4/riLzUx4zhGG82lD2qj/v1xb38cSkQkGNNvSLlYV3SN1EIRXa4ax3GMfzPGo0syNq6XgQgbz1ZFgngsf+wmXGM8h2Vc/5Kj68PGsg7T6swl0fH9LOhs9bbX9+wSOj54B0RERFawAyIiIivYARERkRWMAZ3Adg5koOxP+R8wUuIn2lqh7E+ZP1rEcfZp9bj89e839OBrA/rXJmQETbbtGYJyrW/pg1pj6QDzvE31+lmlGS0pqNs/iIEcs75tmn4+ZP8A7tvZloZyPKLbnxvFGFDZWHLhYNYIIPkZsRrHCAL5878Zh5XcGC5/HfM/x3TI0g24fHd+XMfKzJ+zuRy2c0icSu8fNeJQhWLB2FfXx6L4r2Ikj+33x7QSxs/Z314RkVAQ6xNx/X7LRmwpacSE4PEjo/3TfEuIi+AzXuO1WLfbyI2YyWMb07UYI6Ljg3dARERkBTsgIiKywlHKTHJiVy6Xk1Qq9d47knz68r+EMl43/LEmU5jaxp/yPxwyU9dgOpqRMUzT7x+OiRqrnGZzmOIk4UttEzFS1wzncPgr4Ttv0JhTmxvFNvhX6xTBbDXmvgnj/ZQrvqEZI+2QOYW4WMYhrKPh+NZvMP/MHCNljvL9vCrGcF3MWJ7Bf22C5lK3RtE1/rrNFEDQBmMqsn9E0Wy/Mj67+n9cZnojcYKH3VdEpFLV1zwUwiG3gODPwz9VPJnE9FKOi8No/t/bqIs/5y09O+TdDB7MeNv/8+Kr77ovHV42m5W6urrD1vMOiIiIrGAHREREVhxVB1StVuWmm26Srq4uicfjcuqpp8r3vvc9uD1XSsnNN98sbW1tEo/Hpbu7W7Zu3TrhDSciohPbUU3D/v73vy933XWX3HfffTJ//nx5+eWX5aqrrpJUKiVf//rXRUTkBz/4gdxxxx1y3333SVdXl9x0001y8cUXy+bNmyVmpMWnY7Nz524oT2tp8bbDxpLPB4cxnY5/Ou4hY/ZmWNCMM/gCDYdUGfzTc804QtU4r7/NZizGbGPACCT4mxExYiYBY3q0P/7iOOZnMOO9v0uI9L1eCyl1zMMcEro5/DUtG9Oj/dfGbF7IuC7mkuP+a3HIsgnGdfL/vA6JWRknDjiHj3cd8mbfFb7XdwtQ58cw3qjeZe9Ro/2NnZ3GHljfP1IQOv6OqgN67rnn5LLLLpNLL71URERmzZolP//5z+XFF18Ukbd/8W6//Xb5zne+I5dddpmIiPz0pz+VlpYWefjhh+Xzn//8IccsFotS9AWCc7nc+34zRER04jiqIbhzzz1X1q1bJ1u2bBERkY0bN8qzzz4rl1xyiYiI7NixQ/r6+qS7u9t7TSqVkmXLlsn69evf8Zhr1qyRVCrlfXV0dLzf90JERCeQo7oDuuGGGySXy8mcOXMkGAxKtVqVW265Ra644goREenr6xMRkRbfUNCfyn+qM914441y3XXXeeVcLsdOiIjoJHBUHdAvfvEL+dnPfib333+/zJ8/X1599VVZvXq1tLe3y8qVK99XA6LRqESj0ffekQ7xh1c3G98xy0TIHwqZyCcAg774kRmfqxjxvEPb5Lzj9tuwkf6YYtCIWfmfJxIx0g5FMC5YNfc1YmWjRoomOj6OqgP6xje+ITfccIMXy1m4cKHs2rVL1qxZIytXrpTW1rfzjfX390tbW5v3uv7+fjnzzDMnrtVERHTCO6oYUD6fP2SmTDAY9GYodXV1SWtrq6xbt86rz+Vy8sILL8jy5csnoLlERDRlqKOwcuVKNX36dPXoo4+qHTt2qF/+8peqqalJXX/99d4+t912m0qn0+qRRx5Rr732mrrssstUV1eXKhQKR3SObDar5O37bn7xi1/84tcJ/JXNZt/1//1RdUC5XE5dc801qrOzU8ViMXXKKaeob3/726pYLHr7uK6rbrrpJtXS0qKi0ai66KKLVE9PzxGfgx0Qv/jFL35Nja/36oCYjJSIiI4LJiMlIqJJiR0QERFZwQ6IiIisYAdERERWsAMiIiIr2AEREZEV7ICIiMgKdkBERGQFOyAiIrKCHRAREVnBDoiIiKxgB0RERFawAyIiIivYARERkRXsgIiIyAp2QEREZAU7ICIisoIdEBERWcEOiIiIrGAHREREVrADIiIiK9gBERGRFeyAiIjICnZARERkBTsgIiKygh0QERFZwQ6IiIisYAdERERWsAMiIiIr2AEREZEV7ICIiMgKdkBERGQFOyAiIrKCHRAREVnBDoiIiKxgB0RERFawAyIiIivYARERkRXsgIiIyAp2QEREZAU7ICIisoIdEBERWcEOiIiIrGAHREREVrADIiIiK9gBERGRFeyAiIjICnZARERkBTsgIiKygh0QERFZwQ6IiIisYAdERERWsAMiIiIr2AEREZEV7ICIiMgKdkBERGQFOyAiIrKCHRAREVnBDoiIiKxgB0RERFawAyIiIivYARERkRXsgIiIyIpJ1wEppWw3gYiIJsB7/T+fdB3QyMiI7SYQEdEEeK//546aZLccruvKvn37RCklnZ2d0tvbK3V1dbabNWnlcjnp6OjgdXoPvE5HhtfpyPA6vTullIyMjEh7e7sEAoe/zwl9gG06IoFAQGbMmCG5XE5EROrq6vgDPgK8TkeG1+nI8DodGV6nw0ulUu+5z6QbgiMiopMDOyAiIrJi0nZA0WhU/uEf/kGi0ajtpkxqvE5HhtfpyPA6HRlep4kx6SYhEBHRyWHS3gEREdHUxg6IiIisYAdERERWsAMiIiIr2AEREZEVk7YDuvPOO2XWrFkSi8Vk2bJl8uKLL9pukjVr1qyRs88+W5LJpDQ3N8vll18uPT09sM/4+LisWrVKGhsbpba2VlasWCH9/f2WWjw53HbbbeI4jqxevdr7Hq/T2/bu3StXXnmlNDY2Sjwel4ULF8rLL7/s1Sul5Oabb5a2tjaJx+PS3d0tW7dutdjiD161WpWbbrpJurq6JB6Py6mnnirf+973IMEmr9MxUpPQAw88oCKRiPr3f/939cYbb6i/+7u/U+l0WvX399tumhUXX3yxWrt2rdq0aZN69dVX1cc//nHV2dmpRkdHvX2+8pWvqI6ODrVu3Tr18ssvq3POOUede+65Fltt14svvqhmzZqlzjjjDHXNNdd43+d1UmpoaEjNnDlTfeELX1AvvPCC2r59u/rNb36j3nrrLW+f2267TaVSKfXwww+rjRs3qk996lOqq6tLFQoFiy3/YN1yyy2qsbFRPfroo2rHjh3qwQcfVLW1tepf/uVfvH14nY7NpOyAli5dqlatWuWVq9Wqam9vV2vWrLHYqsnjwIEDSkTUM888o5RSKpPJqHA4rB588EFvnz/+8Y9KRNT69ettNdOakZERNXv2bPXEE0+oj3zkI14HxOv0tm9+85vq/PPPP2y967qqtbVV/dM//ZP3vUwmo6LRqPr5z3/+QTRxUrj00kvVF7/4RfjeZz7zGXXFFVcopXidJsKkG4IrlUqyYcMG6e7u9r4XCASku7tb1q9fb7Flk0c2mxURkYaGBhER2bBhg5TLZbhmc+bMkc7OzpPymq1atUouvfRSuB4ivE5/8qtf/UqWLFkin/3sZ6W5uVkWLVok9957r1e/Y8cO6evrg+uUSqVk2bJlJ9V1Ovfcc2XdunWyZcsWERHZuHGjPPvss3LJJZeICK/TRJh02bAHBwelWq1KS0sLfL+lpUXefPNNS62aPFzXldWrV8t5550nCxYsEBGRvr4+iUQikk6nYd+Wlhbp6+uz0Ep7HnjgAXnllVfkpZdeOqSO1+lt27dvl7vuukuuu+46+da3viUvvfSSfP3rX5dIJCIrV670rsU7/Q2eTNfphhtukFwuJ3PmzJFgMCjValVuueUWueKKK0REeJ0mwKTrgOjdrVq1SjZt2iTPPvus7aZMOr29vXLNNdfIE088IbFYzHZzJi3XdWXJkiVy6623iojIokWLZNOmTXL33XfLypUrLbdu8vjFL34hP/vZz+T++++X+fPny6uvviqrV6+W9vZ2XqcJMumG4JqamiQYDB4yM6m/v19aW1sttWpyuPrqq+XRRx+Vp556SmbMmOF9v7W1VUqlkmQyGdj/ZLtmGzZskAMHDshZZ50loVBIQqGQPPPMM3LHHXdIKBSSlpYWXicRaWtrk3nz5sH35s6dK7t37xYR8a7Fyf43+I1vfENuuOEG+fznPy8LFy6Uv/mbv5Frr71W1qxZIyK8ThNh0nVAkUhEFi9eLOvWrfO+57qurFu3TpYvX26xZfYopeTqq6+Whx56SJ588knp6uqC+sWLF0s4HIZr1tPTI7t37z6prtlFF10kr7/+urz66qve15IlS+SKK67wtnmdRM4777xDpvFv2bJFZs6cKSIiXV1d0traCtcpl8vJCy+8cFJdp3w+f8hqnsFgUFzXFRFepwlhexbEO3nggQdUNBpVP/nJT9TmzZvVl7/8ZZVOp1VfX5/tplnx1a9+VaVSKfX000+r/fv3e1/5fN7b5ytf+Yrq7OxUTz75pHr55ZfV8uXL1fLlyy22enLwz4JTitdJqbenqIdCIXXLLbeorVu3qp/97GcqkUio//iP//D2ue2221Q6nVaPPPKIeu2119Rll1120k0vXrlypZo+fbo3DfuXv/ylampqUtdff723D6/TsZmUHZBSSv3rv/6r6uzsVJFIRC1dulQ9//zztptkjYi849fatWu9fQqFgvr7v/97VV9frxKJhPr0pz+t9u/fb6/Rk4TZAfE6ve2//uu/1IIFC1Q0GlVz5sxR99xzD9S7rqtuuukm1dLSoqLRqLroootUT0+Ppdbakcvl1DXXXKM6OztVLBZTp5xyivr2t7+tisWitw+v07HhekBERGTFpIsBERHRyYEdEBERWcEOiIiIrGAHREREVrADIiIiK9gBERGRFeyAiIjICnZARERkBTsgIiKygh0QERFZwQ6IiIis+P8BNN1kUXSDX1AAAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "imagens = dados['imagens_treino']\n",
    "\n",
    "def denormalize_image(image: np.ndarray):\n",
    "    denormalized = image.copy()\n",
    "    denormalized[:, :, [0, 1, 2]] = 255*denormalized[:, :, [0, 1, 2]]\n",
    "    return denormalized.astype(int)\n",
    "\n",
    "plt.imshow(denormalize_image(imagens[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a32cd9df",
   "metadata": {},
   "source": [
    "## Opera√ß√µes B√°sicas PyTorch (1 ponto)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32e3ee4e",
   "metadata": {},
   "source": [
    "**Explica√ß√£o sobre o assunto**\n",
    "\n",
    "O pyTorch tem a mesma ess√™ncia de opera√ß√£o que o nosso nanoGrad que implementamos na aula passada. Al√©m disso, ele implementa todas as opera√ß√µes que voc√™ tamb√©m consegue fazer em NumPy, sobre tudo as t√©cnicas de slicing, broadcasting e as opera√ß√µes que aprendemos na aula 1 sobre o NumPy. Dessa forma temos um framework com a mesma flexibilidade de NumPy que √© capaz de calcular os gradientes automaticamente al√©m de poder operar eficientemente em GPU.\n",
    "\n",
    "Veja com √© f√°cil criar um tensor, lembre-se que ele √© essencialmente a mesma coisa que o nosso N√≥ Computacional (`NoComp`) do lab passado, mas que ele tamb√©m tem muito mais opera√ß√µes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f69013be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 2, 3])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.tensor([1, 2, 3])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f16f61dc",
   "metadata": {},
   "source": [
    "O `torch.Tensor` mai√∫sculo √© a mesma coisa, s√≥ que ele usa o tipo especificado l√° em cima como padr√£o (`torch.float32`) e n√£o o tipo do dado de origem, na c√©lula acima era int64, mas aqui j√° √© float32."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "28280bba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1., 2., 3.])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.Tensor([1, 2, 3])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aadf6cf3",
   "metadata": {},
   "source": [
    "Voc√™ tamb√©m pode criar um tensor a partir de uma matriz numpy, sendo que ele herda o tipo e o mesmo shape da matriz numpy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b1b5d589",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.2271,  0.7856, -1.2149,  0.4869], dtype=torch.float64)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.tensor(np.random.randn(4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81ddefc8",
   "metadata": {},
   "source": [
    "Quando voc√™ cria um Tensor do PyTorch inicialmente, ele n√£o tem o c√°lculo dos gradientes habilitado por default, para poder habilitar esse funcionalidade, voc√™ tem que setar o par√¢metro `requires_grad`, pois dessa forma, todos os tensores resultantes de um que tem isso habilitado tamb√©m ter√° o c√°lculo dos gradientes habilitado. Veja que ele at√© aparede a `grad_fn`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1bd739f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([8.1000], grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.tensor([3.1], requires_grad=True) + torch.tensor([5])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b3ceffa",
   "metadata": {},
   "source": [
    "Assim a fun√ß√£o `backward` √© respons√°vel por implementar o `retropropaga` de todos os n√≥s que tem `requires_grad=True`. Os gradientes ficam armazenados no atributo `.grad` dos tensores, tal qual fizemos no Lab 2:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f62bc7db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([2.])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor0 = torch.tensor([3.1], requires_grad=True)\n",
    "tensor1 = 2 * tensor0 + 8\n",
    "tensor1.backward()\n",
    "tensor0.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b59f3845",
   "metadata": {},
   "source": [
    "E veja como o PyTorch te adverte quando voc√™ tenta acessar o gradiente de um tensor que n√£o √© ra√≠z da computa√ß√£o (folha para ele). Isso acontece pois ele √© esperto e nem guarda o gradiente para tensores intermedi√°rios pois a princ√≠pio eles n√£o s√£o par√¢metros otimiz√°veis (s√£o resultados internos de contas)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4b3fb4b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/site-packages/torch/utils/_device.py:62: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the .grad field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations. (Triggered internally at aten/src/ATen/core/TensorBody.h:486.)\n",
      "  return func(*args, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "tensor1.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff58d44c",
   "metadata": {},
   "source": [
    "Mas veremos essa quest√£o dos gradiente depois no treinamento.\n",
    "\n",
    "Agora vamos implementar uma nova opera√ß√£o que ser√° utilizada como camada inicial para a nossa NeRF, que √© expandir as nossas entradas, que s√£o simplesmente pontos 3d (X, Y, Z) em mais dimen√µes em frequ√™ncia, isto √©, aplicar senos e cossenos para obtermos uma codifica√ß√£o posicional (*positional encoding*) em uma dimens√£o maior que 3. Nas aulas futuras de Transformers, veremos mais uma outra motiva√ß√£o para essa opera√ß√£o (spoiler: √© como se fosse uma codifica√ß√£o bin√°ria s√≥ que cont√≠nua, por exemplo se em bin√°rio representamos o n√∫mero 99 como (0, 1, 1, 0,   0, 0, 1, 1) usamos os senos e cossenos com frequ√™ncia exponencialmente maiores para criar uma representa√ß√£o cont√≠nua disso).\n",
    "\n",
    "(E tudo isso depois do Gabriel ter falado que Deep Learning fazia um 'automatic feature engineering' sendo que temos que manualmente ajudar a rede neural com features que s√£o senos e cossenos dos valores de entrada ü§°)\n",
    "\n",
    "Primeiro vamos definir a quantidade de frequ√™ncias que iremos utilizar e calcular os seus valores com base na exponencia√ß√£o bin√°ria:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45bca9ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "qtd_frequencias = 6\n",
    "potencias_2 = 2.**torch.arange(qtd_frequencias)\n",
    "potencias_2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8baee63b",
   "metadata": {},
   "source": [
    "**Enunciado da Quest√£o**\n",
    "\n",
    "Implemente a fun√ß√£o `entrada_senos_cossenos` abaixo. Utilize as fun√ß√µes `torch.sin` e `torch.cos` para aplicar o seno e o cosseno. Fa√ßa uso de opera√ß√µes vetorizadas, n√£o use for-loops expl√≠citos que poderiam ser implementados de forma vetorizada (se n√£o depois a NeRF fica um pouco mais lenta). Veja a [aula 1 slide 100](https://docs.google.com/presentation/d/1PYRArTIBh9vQ5r7DvEwGgbZp8FCkrwZnkQuFZ-z-snc) sobre o broadcasting e tamb√©m um pouco antes sobre o uso de elipsis e `None` para indexar matrizes, √© a mesma coisa do NumPy no PyTorch.\n",
    "\n",
    "**N√ÉO** use LLMs (ChatGPT) para pegar a resposta pronta. **N√ÉO** pesquise a resposta pronta na internet.\n",
    "\n",
    "**Pode** olhar a documenta√ß√£o das biblitocas (PyTorch mas todas as fun√ß√µes que voc√™ precisa est√° nas **dicas**) e **pode** (aconselhado) olhar o material de aula (slides e refer√™ncias).\n",
    "\n",
    "<details><summary><b>Dica para a resposta</b></summary>\n",
    "<p>\n",
    "Utilize as fun√ß√£o torch.cat e torch.flatten al√©m de indexa√ß√£o com elipse e elemento None. Use √≠ndices negativos.\n",
    "\n",
    "A minha solu√ß√£o ficou com tr√™s linhas (mas poderia ter sido apenas uma), pois realizei primeiro o reshape do tensor de entrada para o formato correto, depois fiz a multiplica√ß√£o na dimens√£o correta (apenas com o operador *) e por √∫ltimo apliquei a concatena√ß√£o dos resultados calculados com o seno e o cosseno do produto, realizando o flatten para a dimens√£o correta.\n",
    "</p>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8f3bbc2",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "176d5fd24dc20f5c864060784c5a66d7",
     "grade": false,
     "grade_id": "basico_pytorch",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# basico_pytorch autograded_answer\n",
    "\n",
    "def entrada_senos_cossenos(x: torch.tensor, escala: torch.tensor = potencias_2) -> torch.tensor:\n",
    "    \"\"\" Realiza a seguinte opera√ß√£o em um tensor de entrada usando opera√ß√µes VETORIZADAS.\n",
    "           x =[x1, ..., xn]\n",
    "        dado um vetor de escala de frequ√™ncias\n",
    "           escala = [f1, f2, ..., fe]\n",
    "        Calcula:\n",
    "           senos = [sin(f1*x1), sin(f2*x1), ..., sin(fe*x1), ... ... ... , sin(f1*xn), sin(f2*xn), ..., sin(fe*xn)]\n",
    "           cosse = [cos(f1*x1), cos(f2*x1), ..., cos(fe*x1), ... ... ... , cos(f1*xn), cos(f2*xn), ..., cos(fe*xn)]\n",
    "        e retorna a concatena√ß√£o do vetor de entrada e de seus senos e cossenos na escala de frequ√™ncias recebida, nessa ordem.\n",
    "           retorna [x, senos, cossenos]\n",
    "\n",
    "      Args:\n",
    "        x (torch.Tensor): Tensor de entrada a ser operado, tem shape (..., N)\n",
    "        escala (optional, torch.Tensor): Tensor com shape (E, ) que representa os fatores de escala \n",
    "            de frequ√™ncia a serem utilizados na transforma√ß√£o, (default: potencias_2).\n",
    "\n",
    "      Returns:\n",
    "        (torch.Tensor): Resultado da opera√ß√£o, tem shape (..., N + 2 * N * E)\n",
    "      \"\"\"\n",
    "    # ESCREVA SEU C√ìDIGO AQUI (pode apagar este coment√°rio, mas n√£o apague esta c√©lula para n√£o perder o ID)\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7690a4b5",
   "metadata": {},
   "source": [
    "Se voc√™ usou uma LLM, escreva a sua conversa com ela aqui nesta pr√≥pria c√©lula de texto (copie a conversa inteira) ou exporte o link da conversa:\n",
    "\n",
    "**Escreva aqui**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d362f962",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "2cc2293c7824c68f41de90586eac7570",
     "grade": true,
     "grade_id": "testa_basico_pytorch",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# testa_basico_pytorch autograder_tests 1\n",
    "\n",
    "qtd_frequencias = 6\n",
    "potencias_2 = 2.**torch.arange(qtd_frequencias)\n",
    "x0 = torch.tensor([3.])\n",
    "assert entrada_senos_cossenos(x0).shape == (13,)\n",
    "assert entrada_senos_cossenos(x0).dtype == torch.float32\n",
    "assert entrada_senos_cossenos(x0).requires_grad == False\n",
    "assert torch.linalg.norm(entrada_senos_cossenos(x0) - torch.tensor([ 3,  0.1411, -0.2794, -0.5366, -0.9056, -0.7683,  0.9836, -0.9900, 0.9602,  0.8439,  0.4242, -0.6401, -0.1804])) < 0.001\n",
    "x1 = torch.tensor([[3.]])\n",
    "assert entrada_senos_cossenos(x1).shape == (1, 13)\n",
    "assert torch.linalg.norm(entrada_senos_cossenos(x1) - torch.tensor([[ 3,  0.1411, -0.2794, -0.5366, -0.9056, -0.7683,  0.9836, -0.9900, 0.9602,  0.8439,  0.4242, -0.6401, -0.1804]])) < 0.001\n",
    "x2 = torch.tensor([[3, 1, 0.],\n",
    "                   [0, 2, -1]])\n",
    "assert entrada_senos_cossenos(x2).shape == (2, 39)\n",
    "assert torch.linalg.norm(entrada_senos_cossenos(x2) - torch.tensor(\n",
    "    [[ 3.0000,  0.1411, -0.2794, -0.5366, -0.9056, -0.7683,  0.9836, -0.9900,  0.9602,  0.8439,  0.4242, -0.6401, -0.1804,\n",
    "       1.0000,  0.8415,  0.9093, -0.7568,  0.9894, -0.2879,  0.5514,  0.5403, -0.4161, -0.6536, -0.1455, -0.9577,  0.8342,\n",
    "       0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000],\n",
    "     [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,\n",
    "       2.0000,  0.9093, -0.7568,  0.9894, -0.2879,  0.5514,  0.9200, -0.4161, -0.6536, -0.1455, -0.9577,  0.8342,  0.3919,\n",
    "      -1.0000, -0.8415, -0.9093,  0.7568, -0.9894,  0.2879, -0.5514,  0.5403, -0.4161, -0.6536, -0.1455, -0.9577,  0.8342]])) < 0.001\n",
    "assert abs(entrada_senos_cossenos(torch.zeros(5, 2, 3)).sum().item() - 180) < 0.001\n",
    "assert entrada_senos_cossenos(torch.zeros(5, 2, 7)).shape == (5, 2, 91)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe5706ee",
   "metadata": {},
   "source": [
    "## Estrutura√ß√£o do Conjunto de Dados (2 pontos)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26caa5d6",
   "metadata": {},
   "source": [
    "**Explica√ß√£o sobre o assunto**\n",
    "\n",
    "Veja o in√≠cio do v√≠deo abaixo, pelo menos at√© o primeiro minuto 01:00.\n",
    "\n",
    "[![Video Explaining NeRFs](https://ia.gam.dev/cm203/23/lab03/nerfs_video.png)](https://youtu.be/JuH79E8rdKc \"Video Explaining NeRFs - Click to Watch!\")\n",
    "\n",
    "Veja que o nosso conjunto de dados s√£o apenas as imagens 2d tiradas de uma c√¢mera e as poses da c√¢mera, isto √©, a posi√ß√£o 6d no espa√ßo da c√¢mera: transla√ß√£o (x, y, z) e rota√ß√£o (os tr√™s √¢ngulos de rota√ß√£o  œà, Œ∏, œÜ por exemplo, mas essa rota√ß√£o j√° √© dada por uma matriz de rota√ß√£o para n√£o ter perigo de gimbal-lock).\n",
    "\n",
    "Voc√™ pode se perguntar, as fotos eu consigo capturar facilmente, mas como n√≥s conseguimos essa pose das c√¢meras? Isso n√≥s veremos no pr√≥ximo bimestre em vis√£o 3d, por enquanto voc√™ pode pensar que algu√©m mediu essas posi√ß√µes antes de tirar as fotos (um bra√ßo rob√≥tico por exemplo), ou no nosso caso, com um dataset sint√©tico que foi renderizado no Blender, √© justamente as coordenadas da c√¢mera.\n",
    "\n",
    "Mas a ideia central da NeRF √© ter uma rede neural cuja entrada √© um ponto 3d no espa√ßo, com as suas coordenadas X, Y, Z, e cuja sa√≠da √© o R, G, B desse ponto e a sua 'opacidade' ùúé (que na realidade √© uma densidade linear). <sub><sup>(OBS: Na formula√ß√£o completa da NeRF, al√©m do ponto 3d com X, Y, Z, temos os dois √¢ngulos de visada desse ponto (Œ∏, œÜ) que permite modelar o reflexo de luz (que depende de como voc√™ olha para o ponto). Mas para simplificar, deixamos de fora.)</sup></sub>\n",
    "\n",
    "Ent√£o temos que transformar esses pixels e essa pose da c√¢mera em uma sequ√™ncia de pontos 3d no espa√ßo que poder√° servir de entrada para a rede. E depois no final, temos que pegar essa sequ√™ncia de pontos e reduzir."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afdf0977",
   "metadata": {},
   "source": [
    "Vamos primeiro gerar duas matrizes de coordenadas X e Y para cada pixel da nossa imagem, chamei de `pos_x` e `pos_y`. Perceba que eles se iniciam do ponto superior esquerdo e crescem da esquerda para a direita e de cima para baixo, conforme a nossa conven√ß√£o de indexa√ß√£o da imagem, veja por exemplo o `pos_x` abaixo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "044170ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "H, W = imagens_treino.shape[1:3]\n",
    "pos_x, pos_y = torch.meshgrid(torch.arange(W), torch.arange(H), indexing='xy')\n",
    "pos_x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a6dc2f3",
   "metadata": {},
   "source": [
    "Se quiser ver o `pos_y`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53198e5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ef4583e",
   "metadata": {},
   "source": [
    "Ent√£o para cada pixel, vamos pegar um vetor que sai da origem do sistema de coordenadas da c√¢mera, tido como $(0, 0, 0)$, e passa pelo plano de forma√ß√£o da imagem da c√¢mera, tido como $(x-x_c, y-y_c, f)$, onde $x_c$ e $y_c$ s√£o o centro de forma√ß√£o da imagem em pixels e $f$ √© a dist√¢ncia focal da c√¢mera, tamb√©m medida em pixels. Estamos desprezando efeitos de distor√ß√£o da lente.  Essa conta que voc√™ n√£o precisa se preocupar agora (veremos a explica√ß√£o no pr√≥ximo bimestre em vis√£o 3d). <sub><sup>OBS: (Aqui a nossa implementa√ß√£o difere um pouco do paper, eles n√£o deixam norma 1, apenas dividem por $f$. A diferen√ßa √© que a nossa amostra de dist√¢ncia vai ficar um segmento de esfera, em vez de um frostum)</sup></sub>\n",
    "\n",
    "Mas atente para o `shape` do tensor resultante: para cada pixel temos uma dire√ß√£o com `dir_x, dir_y, dir_z` normalizada, tudo isso no referencial da pr√≥pria c√¢mera (mas cuja dire√ß√£o dos eixos j√° foi invertida para ficar compat√≠vel com a rota√ß√£o que ser√° efetuada posteriormente)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a06bd2f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "centro_x, centro_y = W / 2, H / 2\n",
    "raios_camera = torch.stack([pos_x-centro_x, -(pos_y-centro_y), -distancia_focal * torch.ones_like(pos_x)], -1)\n",
    "raios_camera = raios_camera / raios_camera.norm(dim=-1, keepdim=True)\n",
    "raios_camera.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b91049e7",
   "metadata": {},
   "source": [
    "Veja que √© bem semelhante ao `pos_y` acima, s√≥ que inverteu a dire√ß√£o, o centro ficou no meio e os valores m√°ximos e m√≠nimos dependem da dist√¢ncia focal da c√¢mera (o FoV, Field of View, √¢ngulo de abertura da c√¢mera)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94049174",
   "metadata": {},
   "outputs": [],
   "source": [
    "raios_camera[:, :, 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e20e97f",
   "metadata": {},
   "source": [
    "Para termos um tensor com os valores de dist√¢ncia amostrados linearmente basta usarmos a fun√ß√£o `torch.linspace` que a mesma do `np.linspace`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b27345e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "distancia_minima=2.\n",
    "distancia_maxima=6.\n",
    "n_amostras=64\n",
    "valores_distancia = torch.linspace(distancia_minima, distancia_maxima, n_amostras)\n",
    "valores_distancia"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beb07c0c",
   "metadata": {},
   "source": [
    "Ent√£o para cada raio normalizado nas coordenadas da c√¢mera, iremos lev√°-lo para as coordenadas globais por meio da pose da c√¢mera (√© uma simples multiplica√ß√£o pela matriz de rota√ß√£o). Tamb√©m a origem de cada um desses raios ser√° a origem da pr√≥pria c√¢mera nas coordenadas globais (da sua pose). \n",
    "\n",
    "Assim, basta amostrarmos os pontos ao longo dos valores de dist√¢ncia da origem da c√¢mera (j√° tudo em coordenadas globais).\n",
    "\n",
    "Uma quest√£o importante que me fez perder tempo no treinamento que n√£o convergia muito bem da NeRF era a necessidade de perturbar a dist√¢ncia desses pontos, para que eles n√£o sejam sempre os mesmos durante o treinamento, por isso temos o par√¢metro aleatoriza que √© o valor m√°ximo de uma distribui√ß√£o uniforme $U_{uniforme}[0, aleatoriza)$.\n",
    "\n",
    "Assim a sa√≠da final dessa etapa de pr√©-processamento s√£o os pontos 3d e tamb√©m a dist√¢ncia entre os pontos vizinho (pois n√≥s usaremos esse valor quando formos reduzir os pontos ao longo do raio na renderiza√ß√£o para calcular a transpar√™ncia).\n",
    "\n",
    "Para facilitar a implementa√ß√£o se voc√™s e tamb√©m o entendimento, separei a fun√ß√£o mostrada abaixo `amostra_pontos3d` em outra `amostra_pontos3d_inner`, para detalhar melhor o seu funcionamento que voc√™ ir√° implementar mais a diante (s√≥ o `inner`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9e10ddf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def amostra_pontos3d(raios_camera: torch.tensor, pose_camera: torch.tensor, valores_distancia: torch.tensor, aleatoriza: float = 0) -> Tuple[torch.tensor, torch.tensor]:\n",
    "    \"\"\" Amostra pontos 3d a valores de dist√¢ncia ao longo do raio especificado a partir da c√¢mera\n",
    "    com a sua pose definida. Retorna esses pontos 3d e as dist√¢ncias entre eles (pontos vizinhos).\n",
    "\n",
    "    Args:\n",
    "      raios_camera (torch.Tensor): Tensor que determina o ponto de origem do raio, tem shape (..., 3)\n",
    "      pose_camera (torch.Tensor): Tensor que determina a pose da c√¢mera [R | T], tem shape (..., 3, 4)\n",
    "      valores_distancia (torch.Tensor): Valores de dist√¢ncia entre os pontos, shape (..., n_amostras)\n",
    "      aleatoriza (float): valor aleat√≥rio m√°ximo a ser somado sobre os valores_dist√¢ncia\n",
    "\n",
    "    Returns:\n",
    "      (torch.Tensor): Pontos 3d amostrados, tem shape (..., n_amostras, 3)\n",
    "      (torch.Tensor): Dist√¢ncia entre o ponto atual e o anterior, tem shape (..., n_amostras)\n",
    "    \"\"\"\n",
    "    direcao_vetor = raios_camera @ pose_camera[..., :3, :3].T\n",
    "    origem_vetor = pose_camera[..., :3, -1]\n",
    "    if aleatoriza:\n",
    "        valores_distancia = valores_distancia + torch.rand(direcao_vetor.shape[:-1]+valores_distancia.shape[-1:]) * aleatoriza\n",
    "    return amostra_pontos3d_inner(origem_vetor, direcao_vetor, valores_distancia, 1e10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c7c637c",
   "metadata": {},
   "source": [
    "**Enunciado da Quest√£o**\n",
    "\n",
    "Implemente a fun√ß√£o `amostra_pontos3d_inner` abaixo de acordo com a sua documenta√ß√£o.\n",
    "\n",
    "**N√ÉO** pesquise a resposta pronta na internet (**N√ÉO** copie o c√≥digo de reposit√≥rios de NeRFs e seus derivados ...). Pode usar LLMs (ChatGPT) mas sem pedir para ele responder na cara dura, isto √©, voc√™ pode pedir uma explica√ß√£o mas n√£o a resposta para a LLM, utilize por exemplo prompt de exemplo l√° de cima (pode melhorar se quiser, mas nunca pe√ßa a resposta direta):\n",
    "\n",
    "**Pode** olhar a documenta√ß√£o das biblitocas (NumPy, PyTorch, FastAI, mas todas as fun√ß√µes que voc√™ precisa est√° nas **dicas**) e **pode** (aconselhado) olhar o material de aula (slides e refer√™ncias).\n",
    "\n",
    "<details><summary><b>Dica para a resposta</b></summary>\n",
    "<p>\n",
    "Use indexa√ß√£o com ..., com None, com :, com n√∫mero negativo. Utilize a fun√ß√£o `pad` j√° importada, para acrescentar um valor (argumento value) com pad vezes no pad=(n_come√ßo, n_final).\n",
    "\n",
    "A minha solu√ß√£o ficou com duas linhas, uma para calcular os pontos3d por meio das opera√ß√µes de adi√ß√£o, multiplica√ß√£o, e indexa√ß√£o inteligente; e a outra linha  para calcular a dist√¢ncia entre os pontos por meio da indexa√ß√£o inteligente e a fun√ß√£o pad para acrescentar o valor_infinito no final.\n",
    "</p>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8618bb9",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "b779f998574131fe34c304b59a967015",
     "grade": false,
     "grade_id": "estrutura_dados",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# estrutura_dados autograded_answer\n",
    "\n",
    "def amostra_pontos3d_inner(origem: torch.tensor, direcao: torch.tensor, valores_distancia: torch.tensor, valor_infinito: float) -> Tuple[torch.tensor, torch.tensor]:\n",
    "    \"\"\" Amostra pontos 3d a valores de dist√¢ncia ao longo do raio que parte de um ponto de origem\n",
    "        e que tem uma dire√ß√£o especificada. Retorna esses pontos 3d e as dist√¢ncias entre eles (pontos vizinhos).\n",
    "        Use opera√ß√µes VETORIZADAS.\n",
    "        Observe o seguinte exemplo com as dimens√µes maiores omitidas (broadcastable)\n",
    "           origem = [x0, y0, z0]\n",
    "           direcao = [dir_x, dir_y, dir_z]\n",
    "           valores_distancia = [dist0, dist1, ..., dist_n]\n",
    "        Calcula:\n",
    "           pontos3d = [[x0+dir_x*dist0,  y0+dir_y*dist0,  z0+dir_z*dist0],\n",
    "                       [x0+dir_x*dist1,  y0+dir_y*dist0,  z0+dir_z*dist0],\n",
    "                             ...              ...              ...\n",
    "                       [x0+dir_x*dist_n, y0+dir_y*dist_n, z0+dir_z*dist_n]]\n",
    "          distancia_entre_pontos = [dist1-dist0, dist2-dist1, ..., dist_n-dist_n1, valor_infinito]\n",
    "        e retorna tanto os pontos3d quando as dist√¢ncia entre os pontos vizinho, sendo que o √∫ltimo √© o valor infinito.\n",
    "\n",
    "      Args:\n",
    "        origem (torch.Tensor): Tensor que determina o ponto de origem do raio, tem shape (..., 3)\n",
    "        direcao (torch.Tensor): Tensor que determina a dire√ß√£o do raio que parte da origem, tem norma garantidamente\n",
    "            unit√°ria, tem shape (..., 3)\n",
    "        valores_distancia (torch.Tensor): Tensor que determina os valores de dist√¢ncia da origem que os pontos a serem\n",
    "            amostrado est√£o (na dire√ß√£o do raio), tem shape (..., n_amostras)\n",
    "        valor_infinito (float): Valor a ser utilizado na √∫ltima dist√¢ncia entre pontos\n",
    "\n",
    "      Returns:\n",
    "        (torch.Tensor): Pontos 3d amostrados, tem shape (..., n_amostras, 3)\n",
    "        (torch.Tensor): Dist√¢ncia entre o ponto atual e o anterior, tem shape (..., n_amostras)\n",
    "    \"\"\"\n",
    "    # ESCREVA SEU C√ìDIGO AQUI (pode apagar este coment√°rio, mas n√£o apague esta c√©lula para n√£o perder o ID)\n",
    "    raise NotImplementedError()\n",
    "    return pontos3d, distancia_entre_pontos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8b480c5",
   "metadata": {},
   "source": [
    "Se voc√™ usou uma LLM, escreva a sua conversa com ela aqui nesta pr√≥pria c√©lula de texto (copie a conversa inteira) ou exporte o link da conversa:\n",
    "\n",
    "**Escreva aqui**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de145e5c",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "ef4df290d50d4197267bba022993de3c",
     "grade": true,
     "grade_id": "testa_estrutura_dados",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# testa_estrutura_dados autograder_tests 2\n",
    "\n",
    "pts3d_test0, dists_test0 = amostra_pontos3d_inner(torch.tensor([9, 7, 0.]), torch.tensor([0, 0, 1.]), torch.tensor([0, 1, 2, 3, 8.]), 1e9)\n",
    "assert pts3d_test0.requires_grad == False\n",
    "assert pts3d_test0.dtype == torch.float32\n",
    "assert pts3d_test0.shape == (5, 3)\n",
    "assert torch.linalg.norm(pts3d_test0 - torch.tensor([[9., 7, 0], [9, 7, 1], [9, 7, 2], [9, 7, 3], [9, 7, 8]])) < 1e-5\n",
    "assert dists_test0.requires_grad == False\n",
    "assert dists_test0.dtype == torch.float32\n",
    "assert dists_test0.shape == (5,)\n",
    "assert torch.linalg.norm(dists_test0 - torch.tensor([1, 1, 1, 5, 1e9])) < 1e-5\n",
    "\n",
    "origem_testes = torch.tensor([0.0000, 2.7373, 2.9593])\n",
    "direcao_testes = torch.tensor([\n",
    "         [[ 0.3208, -0.8406, -0.4364],\n",
    "          [ 0.3150, -0.8424, -0.4373],\n",
    "          [ 0.3092, -0.8440, -0.4381],\n",
    "          [ 0.3034, -0.8457, -0.4390]],\n",
    "         [[ 0.3215, -0.8376, -0.4416],\n",
    "          [ 0.3157, -0.8394, -0.4425],\n",
    "          [ 0.3099, -0.8410, -0.4434],\n",
    "          [ 0.3040, -0.8427, -0.4443]]])\n",
    "val_dists_testes = torch.tensor([\n",
    "        [[ 0.4538,  3.3146,  6.8625,  8.1907, 10.6819],\n",
    "         [ 0.3811,  3.4473,  5.3367,  8.7293, 10.0798],\n",
    "         [ 0.8941,  2.6826,  6.9399,  8.8870, 10.7653],\n",
    "         [ 0.9996,  4.2814,  5.1150,  8.0507, 10.5449]],\n",
    "        [[ 0.3916,  4.3886,  6.9209,  7.5023, 10.9344],\n",
    "         [ 1.2990,  3.5211,  5.1325,  9.3391, 10.9775],\n",
    "         [ 1.3107,  3.8841,  6.1250,  9.2003, 10.8153],\n",
    "         [ 0.1952,  3.2972,  6.8838,  8.7427, 10.0683]]])\n",
    "pts3d_test1, dists_test1 = amostra_pontos3d_inner(origem_testes, direcao_testes, val_dists_testes, 1e8)\n",
    "assert pts3d_test1.dtype == torch.float32\n",
    "assert pts3d_test1.shape == (2, 4, 5, 3)\n",
    "assert torch.linalg.norm(pts3d_test1[:, :, [0,2,4], :] - torch.tensor([\n",
    "        [[[ 0.1456,  2.3558,  2.7613],\n",
    "          [ 2.2015, -3.0313, -0.0355],\n",
    "          [ 3.4268, -6.2419, -1.7023]],\n",
    "         [[ 0.1200,  2.4163,  2.7926],\n",
    "          [ 1.6811, -1.7583,  0.6256],\n",
    "          [ 3.1751, -5.7539, -1.4486]],\n",
    "         [[ 0.2765,  1.9827,  2.5676],\n",
    "          [ 2.1458, -3.1200, -0.0811],\n",
    "          [ 3.3286, -6.3486, -1.7570]],\n",
    "         [[ 0.3033,  1.8919,  2.5205],\n",
    "          [ 1.5519, -1.5885,  0.7138],\n",
    "          [ 3.1993, -6.1805, -1.6699]]],\n",
    "        [[[ 0.1259,  2.4093,  2.7864],\n",
    "          [ 2.2251, -3.0596, -0.0970],\n",
    "          [ 3.5154, -6.4214, -1.8693]],\n",
    "         [[ 0.4101,  1.6469,  2.3845],\n",
    "          [ 1.6203, -1.5709,  0.6882],\n",
    "          [ 3.4656, -6.4772, -1.8982]],\n",
    "         [[ 0.4062,  1.6350,  2.3781],\n",
    "          [ 1.8981, -2.4138,  0.2435],\n",
    "          [ 3.3517, -6.3584, -1.8362]],\n",
    "         [[ 0.0593,  2.5728,  2.8726],\n",
    "          [ 2.0927, -3.0637, -0.0992],\n",
    "          [ 3.0608, -5.7473, -1.5140]]]])) < 1e-3\n",
    "assert dists_test1.dtype == torch.float32\n",
    "assert dists_test1.shape == (2, 4, 5)\n",
    "assert torch.linalg.norm(dists_test1 - torch.tensor([\n",
    "        [[2.8608e+00, 3.5479e+00, 1.3282e+00, 2.4912e+00, 1.0000e+08],\n",
    "         [3.0662e+00, 1.8894e+00, 3.3926e+00, 1.3505e+00, 1.0000e+08],\n",
    "         [1.7885e+00, 4.2573e+00, 1.9471e+00, 1.8783e+00, 1.0000e+08],\n",
    "         [3.2818e+00, 8.3360e-01, 2.9357e+00, 2.4942e+00, 1.0000e+08]],\n",
    "        [[3.9970e+00, 2.5323e+00, 5.8140e-01, 3.4321e+00, 1.0000e+08],\n",
    "         [2.2221e+00, 1.6114e+00, 4.2066e+00, 1.6384e+00, 1.0000e+08],\n",
    "         [2.5734e+00, 2.2409e+00, 3.0753e+00, 1.6150e+00, 1.0000e+08],\n",
    "         [3.1020e+00, 3.5866e+00, 1.8589e+00, 1.3256e+00, 1.0000e+08]]])) < 1e-4\n",
    "\n",
    "pts3d, dists = amostra_pontos3d(raios_camera, poses_treino[0], valores_distancia)\n",
    "assert pts3d.shape == (100, 100, 64, 3)\n",
    "assert dists.shape == (64,)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "880c1438",
   "metadata": {},
   "source": [
    "Essa formula√ß√£o das amostra √© extremamente simplificada, o paper original realiza uma segunda amostragem baseada justamente no valor de densidade para poder amostrar os pontos de forma mais concentrada nos lugares de maior densidade (onde ele vai bater) pois n√£o adianta nada ter um monte de pontos que ficam no v√°cuo, ou atr√°s do objeto.\n",
    "\n",
    "Formula√ß√µes mais recentes, como a [instant-NGP](https://github.com/NVlabs/instant-ngp) utiliza formas ainda mais inteligentes de amostragem e uma implementa√ß√£o otimizada em CUDA para deixar o treinamento ainda 100x mais r√°pido.\n",
    "\n",
    "Fica a sugest√£o para o projeto do exame: implementar/aplicar uma NeRF original completa ou ainda modelos mais recentes de NeRFs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc3f3f19",
   "metadata": {},
   "source": [
    "## Defini√ß√£o da Arquitetura (3 pontos)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9264135a",
   "metadata": {},
   "source": [
    "**Explica√ß√£o sobre o assunto**\n",
    "\n",
    "![Arquitetura da nossa Pequena NeRF a ser implementada](https://ia.gam.dev/cm203/23/lab03/nerf.png)\n",
    "\n",
    "Neste lab implementaremos uma arquitetura menor da NeRF apresentada no artigo original (sem as entradas dos √¢ngulos de visada) e com menor largura e profundidade.\n",
    "\n",
    "A sua arquitetura √© extremamente simples, sendo apenas camadas densas (completamente conectadas) sequenciais, a menos de uma conex√£o intermedi√°ria que concatena o resultado da ativa√ß√£o anterior com a entrada codificada.\n",
    "\n",
    "Em PyTorch podemos definir a nossa rede neural como sendo uma `nn.Module` que apresenta algumas convi√™ncias simples de registar os par√¢metros trein√°veis.\n",
    "\n",
    "Abaixo, definimos uma PequenaNerf que conta com apenas 7 camadas trein√°veis. Escolhemos a fun√ß√£o de ativa√ß√£o `leaky_relu` pois o `relu` estava ficando por muitas vezes preso em regi√µes negativas e n√£o conseguia treinar (morria). Essa arquitetura tem algo interessante que √© reinjetar a codifica√ß√£oda entrada em uma camada posterior.\n",
    "\n",
    "Nessa nossa arquitetura, temos uma entrada com tamanho 3 no √∫ltimo eixo que representa o ponto 3d (X, Y, Z) e uma sa√≠da com tamanho 4 (que na realidade √© quebrada em 3 e 1) no √∫ltimo eixo que representa o valor (R, G, B, ùúé), nessa ordem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8fd16a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PequenaNerf(torch.nn.Module):\n",
    "    def __init__(self,\n",
    "                 largura: int = 128,\n",
    "                 codificador_posicional: Callable[[torch.tensor], torch.tensor] = entrada_senos_cossenos,\n",
    "                 funcao_ativacao: Callable[[torch.tensor], torch.tensor] = leaky_relu):\n",
    "        super().__init__()\n",
    "        self.largura = largura\n",
    "        self.codificador_posicional = codificador_posicional\n",
    "        self.funcao_ativacao = funcao_ativacao\n",
    "        dimensoes_apos_codifica = codificador_posicional(torch.zeros(3)).shape[0]\n",
    "        self.camada1 = torch.nn.Linear(dimensoes_apos_codifica, largura)\n",
    "        self.camada2 = torch.nn.Linear(largura, largura)\n",
    "        self.camada3 = torch.nn.Linear(largura, largura)\n",
    "        self.camada4 = torch.nn.Linear(largura + dimensoes_apos_codifica, largura)\n",
    "        self.camada5 = torch.nn.Linear(largura, largura)\n",
    "        self.camada6 = torch.nn.Linear(largura, largura)\n",
    "        self.camada_saida = torch.nn.Linear(largura, 4)\n",
    "        self.camadas = [self.camada1, self.camada2, self.camada3, self.camada4, self.camada5, self.camada6, self.camada_saida]\n",
    "\n",
    "    def forward(self, x: torch.tensor):\n",
    "        return executa_rede(x, self.codificador_posicional, self.funcao_ativacao, self.camadas)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc74cdbd",
   "metadata": {},
   "source": [
    "A esperteza do PyTorch com essa `nn.Module` se restringe a outros `nn.Module`s que sejam instanciados como atributos do primeiro ou ainda registrados manualmente por meio da fun√ß√£o `.add_module`.\n",
    "\n",
    "Perceba que tem a mesma ess√™ncia do m√©todo `.parametros()` da `RedeNeuralSequencial` que foi implementada no Lab 2.\n",
    "\n",
    "Veja por exemplo os par√¢metros (que na realidade est√£o internos a esses m√≥dulos) que o PyTorch detecta."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7af03c4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "p = PequenaNerf()\n",
    "p"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0819024",
   "metadata": {},
   "source": [
    "Na realidade, ele consegue acessar os tensores simplesmente acessando cada m√≥dulo recursivamente e agregando os seus tensores, por exemplo, o m√≥dulo `Linear` j√° termina com apenas dois par√¢metros `weight` e `bias` (√© s√≥ a combin√ß√£o linear, uma multiplica√ß√£o matricial.\n",
    "\n",
    "Veja abaixo todos os tensores que s√£o trein√°veis. O `state_dict` √© usado como uma forma de salvar e de carregar os par√¢metros de um `nn.Module`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6f47246",
   "metadata": {},
   "outputs": [],
   "source": [
    "p.state_dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d96f749",
   "metadata": {},
   "source": [
    "**Enunciado da Quest√£o**\n",
    "\n",
    "Implemente a fun√ß√£o `executa_rede` abaixo de acordo com a sua documenta√ß√£o.\n",
    "\n",
    "**N√ÉO** pesquise a resposta pronta na internet (**N√ÉO** copie o c√≥digo de reposit√≥rios de NeRFs e seus derivados ...). Pode usar LLMs (ChatGPT) mas sem pedir para ele responder na cara dura, isto √©, voc√™ pode pedir uma explica√ß√£o mas n√£o a resposta para a LLM, utilize por exemplo prompt de exemplo l√° de cima (pode melhorar se quiser, mas nunca pe√ßa a resposta direta):\n",
    "\n",
    "**Pode** olhar a documenta√ß√£o das biblitocas (NumPy, PyTorch, FastAI, mas todas as fun√ß√µes que voc√™ precisa est√° nas **dicas**) e **pode** (aconselhado) olhar o material de aula (slides e refer√™ncias).\n",
    "\n",
    "<details><summary><b>Dica para a resposta</b></summary>\n",
    "<p>\n",
    "Voc√™ deve chamar as fun√ß√µes, seja do codificador_posicional, seja da funcao_ativacao, ou seja um elemento da camadas_rede normalmente, tal qual chamamos as fun√ß√µes implementadas anteirormente, elas receber um tensor como entrada e geram um outro tensor de sa√≠da realizando diversas opera√ß√µes internamente, que s√£o todas diferenci√°veis e podem ser retropropagadas.\n",
    "\n",
    "Use `torch.cat` para concatenar os valores da codifica√ß√£o com o da camada anterior nessa ordem. Use as fun√ß√£o sigmoid e softplus j√° importadas.\n",
    "\n",
    "Essa eu acabei levando 8 linhas para implementar, mas s√£o todas bem simples que consistem apenas na aplica√ß√£o das fun√ß√µes da camada (camadas_rede que √© a combina√ß√£o linear) seguida pela fun√ß√£o de ativa√ß√£o. Voc√™ pode usar um for-loop ou descrever manualmente as opera√ß√µes com as camadas. Esse n√£o tem como vetorizar pois as opera√ß√µes ocorrem sequencialmente e n√£o em paralelo.\n",
    "    \n",
    "Um dos erros que cometi quando fui implementar pela primeira vez foi a de trocar as fun√ß√µes de ativa√ß√£o, usando uma sigmoide para a densidade, o que a limitava e deixava o trator semi-transparente. Muito aten√ß√£o nessa fun√ß√£o de ativa√ß√£o da √∫ltima camada, isso pode prejudicar bastante o treino.\n",
    "</p>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de474234",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "7001059d20e9ca804044aa61e0c6561c",
     "grade": false,
     "grade_id": "define_arquitetura",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# define_arquitetura autograded_answer\n",
    "\n",
    "def executa_rede(tensor_entrada: torch.tensor, codificador_posicional: Callable, funcao_ativacao: Callable, camadas_rede: List[Callable]):\n",
    "    \"\"\" Realiza a propaga√ß√£o direta da NeRF, que consiste primeiro na codifica√ß√£o da entrada (codificador_posicional)\n",
    "        depois na aplica√ß√£o sequencial das camadas da rede, cada qual com a sua pr√≥pria combina√ß√£o linear (camadas_rede)\n",
    "        no qual sempre devem passar pela funcao_ativacao, a menos da √∫ltima camada que deve ter uma fun√ß√£o de ativa√ß√£o\n",
    "        sigmoid para as tr√™s primeiras sa√≠das e fun√ß√£o de ativa√ß√£o softplus para o √∫ltimo elemento da sa√≠da.\n",
    "        A concatena√ß√£o dever√° ser realizada na quarta camada interna da rede, sendo que a codifica√ß√£o posicional deve\n",
    "        ser colocada antes do resultado da ativa√ß√£o da camada anterior, [x_codificado, atv_anterior] nessa ordem.\n",
    "\n",
    "      Args:\n",
    "        tensor_entrada (torch.tensor): Tensor que representa um ponto 3d no espa√ßo, tem shape (..., 3)\n",
    "        codificador_posicional (Callable): Fun√ß√£o que computa a codifica√ß√£o da entrada\n",
    "        funcao_ativacao (Callable): Fun√ß√£o de ativa√ß√£o a ser aplicada a toda camada na rede (menos a √∫ltima)\n",
    "        camadas_rede (List[Callable]): Lista com 7 fun√ß√µes que representam a combina√ß√£o linear de uma camada da rede\n",
    "\n",
    "      Returns:\n",
    "        (torch.Tensor): Sa√≠da de RGB, deve ser entre 0 e 1 (resultado de sigmoid), tem shape (..., 3)\n",
    "        (torch.Tensor): Sa√≠da de densidade ùúé, deve ser positivo (resultado de softplus), tem shape (..., )\n",
    "    \"\"\"\n",
    "    # ESCREVA SEU C√ìDIGO AQUI (pode apagar este coment√°rio, mas n√£o apague esta c√©lula para n√£o perder o ID)\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "321c260f",
   "metadata": {},
   "source": [
    "Se voc√™ usou uma LLM, escreva a sua conversa com ela aqui nesta pr√≥pria c√©lula de texto (copie a conversa inteira) ou exporte o link da conversa:\n",
    "\n",
    "**Escreva aqui**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65b05ef2",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "ead2ee102ddd168dfb2dbdcd36a20f16",
     "grade": true,
     "grade_id": "testa_define_arquitetura",
     "locked": true,
     "points": 3,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# testa_define_arquitetura autograder_tests 3\n",
    "\n",
    "pts3d_test0 = torch.tensor([[0, 0, 0], [-999, -999, -999], [999, 999, 999.], [1, 2, 3], [-1, 8, -6]])\n",
    "cores, densidades = executa_rede(pts3d_test0, lambda x: torch.cat([x, x], dim=-1), lambda x: x, [lambda x: x for i in range(6)]+[lambda x: x[..., :4]])\n",
    "assert cores.dtype == torch.float32\n",
    "assert cores.shape == (5, 3)\n",
    "assert torch.linalg.norm(cores - torch.tensor(\n",
    "        [[0.5000, 0.5000, 0.5000],\n",
    "         [0.0000, 0.0000, 0.0000],\n",
    "         [1.0000, 1.0000, 1.0000],\n",
    "         [0.7311, 0.8808, 0.9526],\n",
    "         [0.2689, 0.9997, 0.0025]])) < 1e-3\n",
    "assert densidades.dtype == torch.float32\n",
    "assert densidades.shape == (5,)\n",
    "assert torch.linalg.norm(densidades - torch.tensor([0.69315, 0, 999, 1.3133, 0.31326])) < 1e-4\n",
    "\n",
    "cores, densidades = executa_rede(pts3d_test0, lambda x: torch.cat([x/2, x], dim=-1), lambda x: x+1, [lambda x: x for i in range(6)]+[lambda x: x[..., :4]])\n",
    "assert torch.linalg.norm(cores - torch.tensor(\n",
    "        [[0.9526, 0.9526, 0.9526],\n",
    "         [0.0000, 0.0000, 0.0000],\n",
    "         [1.0000, 1.0000, 1.0000],\n",
    "         [0.9707, 0.9820, 0.9890],\n",
    "         [0.9241, 0.9991, 0.5000]])) < 1e-3\n",
    "assert torch.linalg.norm(densidades - torch.tensor([3.0486, 0.0000, 1002.0000, 4.0181, 2.1269])) < 1e-4\n",
    "\n",
    "modelo_nerf = PequenaNerf()\n",
    "modelo_nerf.load_state_dict(torch.load(base_path / 'aleatoria.pt', map_location=device))\n",
    "pts3d, dists = amostra_pontos3d(raios_camera[:2, :2],  poses_treino[0], torch.linspace(0, 5, 7))\n",
    "\n",
    "cores, densidades = modelo_nerf(pts3d)\n",
    "assert cores.shape == (2, 2, 7, 3)\n",
    "assert cores.requires_grad == True\n",
    "assert densidades.shape == (2, 2, 7)\n",
    "assert densidades.requires_grad == True\n",
    "assert torch.linalg.norm(cores - torch.tensor([\n",
    "         [[[0.7953, 0.4961, 0.5148],\n",
    "           [0.7108, 0.3646, 0.6647],\n",
    "           [0.8353, 0.3493, 0.5688],\n",
    "           [0.7661, 0.3283, 0.6333],\n",
    "           [0.6062, 0.4618, 0.6932],\n",
    "           [0.7543, 0.3719, 0.3533],\n",
    "           [0.8085, 0.4326, 0.4280]],\n",
    "          [[0.7953, 0.4961, 0.5148],\n",
    "           [0.7006, 0.3631, 0.6645],\n",
    "           [0.8324, 0.3365, 0.5950],\n",
    "           [0.7625, 0.3179, 0.6299],\n",
    "           [0.5739, 0.4561, 0.6880],\n",
    "           [0.7773, 0.3852, 0.3391],\n",
    "           [0.7382, 0.3707, 0.4519]]],\n",
    "         [[[0.7953, 0.4961, 0.5148],\n",
    "           [0.7031, 0.3730, 0.6575],\n",
    "           [0.8291, 0.3453, 0.5678],\n",
    "           [0.7587, 0.3482, 0.6528],\n",
    "           [0.5746, 0.4547, 0.7069],\n",
    "           [0.7038, 0.3423, 0.3991],\n",
    "           [0.8258, 0.4944, 0.4465]],\n",
    "          [[0.7953, 0.4961, 0.5148],\n",
    "           [0.6944, 0.3735, 0.6606],\n",
    "           [0.8263, 0.3322, 0.6081],\n",
    "           [0.7580, 0.3278, 0.6505],\n",
    "           [0.5700, 0.4707, 0.6980],\n",
    "           [0.7135, 0.3595, 0.3701],\n",
    "           [0.7903, 0.4171, 0.4178]]]])) < 1e-3\n",
    "assert torch.linalg.norm(densidades - torch.tensor([\n",
    "         [[0.1150, 0.1859, 0.1180, 0.3546, 0.3164, 0.3694, 0.2446],\n",
    "          [0.1150, 0.1861, 0.1298, 0.3080, 0.3537, 0.2966, 0.2130]],\n",
    "         [[0.1150, 0.1804, 0.1198, 0.3267, 0.2936, 0.3396, 0.3401],\n",
    "          [0.1150, 0.1794, 0.1301, 0.2792, 0.3456, 0.3108, 0.2732]]])) < 1e-3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18fd4e80",
   "metadata": {},
   "source": [
    "Pronto, se passar nos testes, a sua arquitetura est√° exatamente igual ao que implementamos, o que possibilita carregar os pesos j√° treinados e obter exatamente o mesmo resultado esperado.\n",
    "\n",
    "Perceba que voc√™ tamb√©m poderia implementar de formas diferentes e conseguir treinar uma NeRF at√© melhor, s√≥ que a arquitetura foi fixada por limita√ß√µes da corre√ß√£o."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcb72c3b",
   "metadata": {},
   "source": [
    "## Treinamento (2 pontos)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ca1c88a",
   "metadata": {},
   "source": [
    "**Explica√ß√£o sobre o assunto**\n",
    "\n",
    "Para podermos treinar a rede neural, temos que primeiro renderizar a imagem a partir das cores e densidades de cada ponto amostrados ao longo do raio. Para isso implementamos a fun√ß√£o `renderiza_imagem` abaixo. Ela recebe as cores e as densidades de cada ponto ao longo de um raio al√©m das dist√¢ncias lineares entre esses pontos vizinhos, calculadas mais facilmente no passo anterior.\n",
    "\n",
    "Como a densidade foi definida do ponto de vista da 'quantidade de mat√©ria por elemento de profundidade' linear, n√≥s temos que multiplic√°-la justamente pela nossa dist√¢ncia entre os pontos vizinhos para obter uma grandeza que seja correlacionada com a opacidade do material entre esses dois pontos vizinhos. Um detalhe aqui √© o uso da exponencial $(1-exp(-x))$ que garante que a opacidade esteja sempre entre 0 e 1 para valores de densidade positivos.\n",
    "\n",
    "Depois calculamos a transmit√¢ncia que a fra√ß√£o de luz que passa localmente entre os pontos, isto √©, a transpar√™ncia (o complementar da opacidade). N√≥s multiplicamos esse valor de transmit√¢ncia consecutivamente ao longo do raio para saber o quanto de luz de chega at√© a c√¢mera partindo daquele ponto por meio da fun√ß√£o `torch.cumprod` que calcula o produto cumulativo, exemplo: `[a, b, c, d] -> [a, a*b, a*b*c, a*b*c*d]` (aqui voc√™ pode pensar equivalentemente da fra√ß√£o de luz que se estivesse saindo da c√¢mera chegaria ao ponto). Para garantir que sempre pelo menos o primeiro ponto ser√° amostrado, n√≥s fazemos um padding com o valor unit√°rio para ficar da forma `[1, a, a*b, a*b*c]` e o valor que seria da opacidade amostrada no infinito vai embora tamb√©m.\n",
    "\n",
    "Finalmente podemos calcula a contribui√ß√£o de cada cor que √© justamente a sua pr√≥pria opacidade vezes o tanto de luz que chega at√© o observador partindo-se daquele ponto, isto √©, a transmit√¢ncia (transpar√™ncia acumulada). Todo esse processo na realidade √© uma discretiza√ß√£o do que seria uma integral da NeRF ao longo do raio (ray).\n",
    "\n",
    "Aqui estamos trabalhando com opera√ß√µes aditivas das cores ([ver slide 53 da aula 1](https://docs.google.com/presentation/d/1PYRArTIBh9vQ5r7DvEwGgbZp8FCkrwZnkQuFZ-z-snc)), s√≥ que ponderadas pela opacidade e pela 'transmit√¢ncia acumulada' do ponto de interesse at√© a c√¢mera. A intui√ß√£o aqui √© que s√£o os pontos que emitem luz.\n",
    "\n",
    "Ela foi dividia com a fun√ß√£o `reduz_cores` que voc√™ implementar√° na c√©lula pontuada desta se√ß√£o. Atente-se para o shape dos tensores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1448a32d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def renderiza_imagem(cores: torch.tensor, densidades: torch.tensor, distancia_entre_pontos: torch.tensor) -> torch.tensor:\n",
    "    \"\"\" Realiza a combina√ß√£o das cores de acordo com a densidade do ponto ao longo do raio (ray)\n",
    "\n",
    "      Args:\n",
    "        cores (torch.Tensor): Tensor que representa a cor de um ponto no espa√ßo em um mesmo\n",
    "            raio (ray), tem shape (..., N, 3) e valores entre 0 e 1\n",
    "        densidades (torch.Tensor): Tensor que representa a densidade amostrada em pontos \n",
    "            consecutivos em um raio (ray) no espa√ßo, tem shape (..., N) e valores positivos\n",
    "        distancia_entre_pontos (torch.Tensor): Tensor que representa a dist√¢ncia consecutiva \n",
    "            entre os pontos de um mesmo raio, tem shape (..., N)\n",
    "\n",
    "      Returns:\n",
    "        (torch.Tensor): Sa√≠da de RGB, com valores entre entre 0 e 1, tem shape (..., 3)\n",
    "    \"\"\"\n",
    "    opacidade = 1. - torch.exp(-densidades * distancia_entre_pontos)\n",
    "    transmitancia = 1 - opacidade + 1e-10\n",
    "    transmitancia_acumulada = torch.cumprod(transmitancia, -1)[..., :-1]\n",
    "    transmitancia_acumulada = pad(transmitancia_acumulada, (1,0), value=1)\n",
    "    pesos_de_cada_cor = opacidade * transmitancia_acumulada\n",
    "    return reduz_cores(cores, pesos_de_cada_cor)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6431371f",
   "metadata": {},
   "source": [
    "**Enunciado da Quest√£o**\n",
    "\n",
    "Implemente as fun√ß√µes `reduz_cores` e `train` abaixo de acordo com a sua documenta√ß√£o.\n",
    "\n",
    "**N√ÉO** pesquise a resposta pronta na internet (**N√ÉO** copie o c√≥digo de reposit√≥rios de NeRFs e seus derivados ...). Pode usar LLMs (ChatGPT) mas sem pedir para ele responder na cara dura, isto √©, voc√™ pode pedir uma explica√ß√£o mas n√£o a resposta para a LLM, utilize por exemplo prompt de exemplo l√° de cima (pode melhorar se quiser, mas nunca pe√ßa a resposta direta):\n",
    "\n",
    "**Pode** olhar a documenta√ß√£o das biblitocas (NumPy, PyTorch, FastAI, mas todas as fun√ß√µes que voc√™ precisa est√° nas **dicas**) e **pode** (aconselhado) olhar o material de aula (slides e refer√™ncias).\n",
    "\n",
    "<details><summary><b>Dica para a resposta</b></summary>\n",
    "<p>\n",
    "A fun√ß√£o `reduz_cores` pode ser implementada apenas com uma linha de c√≥digo. Use a fun√ß√£o `torch.sum` somando na dimens√£o `dim` correta. Basta fazer uma multiplica√ß√£o e uma indexa√ß√£o com elemento None (reshape).\n",
    "\n",
    "A fun√ß√£o `train` pode ser implementada com com apenas 4 linhas de c√≥digo. Sendo que as tr√™s primeiras s√£o apenas chamar as fun√ß√µes corretamente que foram recebidas como argumentos. Se tiver d√∫vida de como essas fun√ß√µes s√£o chamadas, observe os testes do exerc√≠cio anterior (TESTES TAMB√âM S√ÉO DOCUMENTA√á√ÉO).\n",
    "\n",
    "√â important√≠ssimo voc√™ chamar o m√©todo `.backward()` do seu tensor custo para poder retropropagar os gradientes, lembra do lab2 da fun√ß√£o retropropaga?\n",
    "</p>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83518ba8",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "bae0e8710b90a8c6997c4f09aa47e465",
     "grade": false,
     "grade_id": "treinamento",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# treinamento autograded_answer\n",
    "\n",
    "def reduz_cores(cores: torch.tensor, pesos_de_cada_cor: torch.tensor) -> torch.tensor:\n",
    "    \"\"\" Realiza a m√©dia ponderada das cores ao longo do raio (ray) de acordo com a sua import√¢ncia\n",
    "        que j√° foi calculada anteriormente pela sua opacidade e pela fra√ß√£o da luz que chega daquele ponto.\n",
    "\n",
    "      Args:\n",
    "        cores (torch.Tensor): Tensor que representa a cor de um ponto no espa√ßo em um mesmo raio (ray),\n",
    "            tem shape (..., N, 3) e valores entre 0 e 1\n",
    "        pesos_de_cada_cor (torch.Tensor): Tensor que representa os pesos a ser usado na combina√ß√£o linear das\n",
    "            cores consecutivas em um raio (ray) no espa√ßo, tem shape (..., N) e valores entre 0 e 1, soma unit√°ria\n",
    "\n",
    "      Returns:\n",
    "        (torch.Tensor): Sa√≠da de RGB da imagem formada, com valores entre entre 0 e 1, tem shape (..., 3)\n",
    "    \"\"\"\n",
    "    # ESCREVA SEU C√ìDIGO AQUI (pode apagar este coment√°rio, mas n√£o apague esta c√©lula para n√£o perder o ID)\n",
    "    raise NotImplementedError()\n",
    "\n",
    "def train(neural_model: Callable, funcao_custo: Callable, dataloader: Iterable, optimizer: torch.optim.Optimizer, scheduler: torch.optim.lr_scheduler.LRScheduler) -> List[float]:\n",
    "    \"\"\" Realiza o treinamento da rede neural, \n",
    "\n",
    "      Args:\n",
    "        neural_model (Callable): Modelo da rede neural a ser treinada\n",
    "        funcao_custo (Callable): Fun√ß√£o custo que recebe dois tensores e retorna um tensor de rank 0.\n",
    "            Passe a sa√≠da esperada como primeiro argumento e a sa√≠da estimada pela rede como segundo argumento.\n",
    "        dataloader (Iterable): Objeto iter√°vel que retorna uma tupla com valores de entrada e de sa√≠da esperada\n",
    "        optimizer (torch.optim.Optimizer): Tem refer√™ncia os par√¢metros trein√°veis da rede e os atualiza\n",
    "        scheduler (torch.optim.lr_scheduler.LRScheduler): Controla os hiper-par√¢metros do optimizer\n",
    "\n",
    "      Returns:\n",
    "        (List[float]): Lista com os custos que s√£o MSE, mean-squared error\n",
    "    \"\"\"\n",
    "    custos: List[float]  = []\n",
    "    dataloader = tqdm(dataloader) # Para imprimir a barra de progresso verde\n",
    "    for (pts3d, dists), imagem_rgb in dataloader:\n",
    "        optimizer.zero_grad() # Zera os gradientes, por causa do += do lab2\n",
    "        # ESCREVA SEU C√ìDIGO AQUI (pode apagar este coment√°rio, mas n√£o apague esta c√©lula para n√£o perder o ID)\n",
    "        raise NotImplementedError()\n",
    "        optimizer.step() # Aqui que os par√¢metros da rede s√£o atualizados\n",
    "        scheduler.step()\n",
    "        dataloader.set_postfix(Custo=custo.item())\n",
    "        custos.append(custo.item())\n",
    "    return custos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6801e04",
   "metadata": {},
   "source": [
    "Se voc√™ usou uma LLM, escreva a sua conversa com ela aqui nesta pr√≥pria c√©lula de texto (copie a conversa inteira) ou exporte o link da conversa:\n",
    "\n",
    "**Escreva aqui**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad0e8db8",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "800d79a11d99b387509ca01b6f223ce9",
     "grade": true,
     "grade_id": "testa_treinamento",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# testa_treinamento autograder_tests 2\n",
    "\n",
    "\n",
    "imagem_gerada = reduz_cores(torch.tensor([[0.3, 0.8, 0.6],[0, 0, 0]]), torch.tensor([0.5, 0.5]))\n",
    "assert imagem_gerada.dtype == torch.float32\n",
    "assert imagem_gerada.shape == (3,)\n",
    "assert torch.linalg.norm(imagem_gerada - torch.tensor([0.15, 0.4, 0.3])) < 1e-5\n",
    "imagem_gerada = reduz_cores(torch.tensor([[0.3, 0.8, 0.6],[0, 0, 0]]), torch.tensor([0, 1]))\n",
    "assert torch.linalg.norm(imagem_gerada - torch.tensor([0, 0., 0])) < 1e-5\n",
    "imagem_gerada = reduz_cores(torch.tensor([[0.3, 0.8, 0.6],[0, 0, 0]]), torch.tensor([1, 0]))\n",
    "assert torch.linalg.norm(imagem_gerada - torch.tensor([0.3, 0.8, 0.6])) < 1e-5\n",
    "imagem_gerada = reduz_cores(torch.ones((55, 66, 10, 3)), torch.ones((55, 66, 10))*0.1)\n",
    "assert imagem_gerada.shape == (55, 66, 3)\n",
    "assert torch.linalg.norm(imagem_gerada -  torch.ones((55, 66, 3))) < 1e-5\n",
    "\n",
    "modelo_nerf = PequenaNerf()\n",
    "modelo_nerf.load_state_dict(torch.load(base_path / 'aleatoria.pt', map_location=device))\n",
    "pts3d, dists = amostra_pontos3d(raios_camera[:2, :2],  poses_treino[0], torch.linspace(0, 5, 7))\n",
    "cores, densidades = modelo_nerf(pts3d)\n",
    "imagem_gerada = renderiza_imagem(cores, densidades, dists)\n",
    "assert imagem_gerada.requires_grad == True\n",
    "assert imagem_gerada.shape == (2, 2, 3)\n",
    "assert torch.linalg.norm(imagem_gerada -  torch.tensor([\n",
    "        [[0.7585, 0.4017, 0.5384],\n",
    "         [0.7306, 0.3832, 0.5476]],\n",
    "        [[0.7543, 0.4230, 0.5489],\n",
    "         [0.7389, 0.4010, 0.5410]]])) < 1e-3\n",
    "\n",
    "\n",
    "torch.manual_seed(42)\n",
    "\n",
    "funcao_custo_mse = lambda ≈∑, y: torch.mean((≈∑ - y)**2)\n",
    "\n",
    "amostra_pts0 = amostra_pontos3d(raios_camera, poses_treino[0], torch.linspace(2, 6, 16))\n",
    "dataloader0 = [(amostra_pts0, imagens_treino[0]) for i in range(10)]\n",
    "\n",
    "optim0 = torch.optim.Adam(modelo_nerf.parameters())\n",
    "sched0 = torch.optim.lr_scheduler.OneCycleLR(optim0, 1e-3, 10)\n",
    "\n",
    "custos = train(modelo_nerf, funcao_custo_mse, dataloader0, optim0, sched0)\n",
    "assert custos[-1] <= 0.027\n",
    "assert torch.linalg.norm(modelo_nerf.camada_saida.bias - torch.tensor([-0.0037, -0.0036, -0.0034, -0.0042])) < 1e-3\n",
    "assert torch.linalg.norm(modelo_nerf.camada_saida.bias.grad - torch.tensor([0.0023, 0.0036, 0.0013, 0.0008])) < 1e-3\n",
    "del dataloader0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8747fa3a",
   "metadata": {},
   "source": [
    "Esse n√£o foi o treinamento de verdade, foi apenas para testar o seu c√≥digo (a mesma imagem 10 vezes com poucas amostras do ray). Voc√™ ir√° treinar de fato a NeRF mais embaixo, depois do pr√≥ximo exerc√≠cio pontuado."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72bacf98",
   "metadata": {},
   "source": [
    "## Valida√ß√£o e S√≠ntese de Novas Cenas (2 pontos)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40ee6b93",
   "metadata": {},
   "source": [
    "**Explica√ß√£o sobre o assunto**\n",
    "\n",
    "Parab√©ns, se voc√™ chegou at√© aqui √© porque praticamente j√° est√° acabando a parte avaliada do lab e logo ap√≥s voc√™ treinar√° a sua pr√≥pria NeRF do zero!\n",
    "\n",
    "Agora vamos apenas chamar a rede com as fun√ß√µes apropriadas que j√° utilizamos anteriormente.\n",
    "\n",
    "Mas como tamb√©m a infer√™ncia n√£o faz parte do treinamento, n√£o precisamos calcular os gradiente, o que economiza mem√≥ria, assim usamos a fun√ß√£o `torch.no_grad()` para desabilitar o c√°lculo dos gradientes dentro de um escopo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "506a9341",
   "metadata": {},
   "outputs": [],
   "source": [
    "t0 = torch.tensor([3.], requires_grad=True)\n",
    "with torch.no_grad():\n",
    "    t1 = 2*t0 + 9\n",
    "t1, t1.requires_grad"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77125214",
   "metadata": {},
   "source": [
    "**Enunciado da Quest√£o**\n",
    "\n",
    "Implemente as fun√ß√µes `sintetiza_nova` de acordo com a sua documenta√ß√£o.\n",
    "\n",
    "**N√ÉO** pesquise a resposta pronta na internet (**N√ÉO** copie o c√≥digo de reposit√≥rios de NeRFs e seus derivados ...). Pode usar LLMs (ChatGPT) mas sem pedir para ele responder na cara dura, isto √©, voc√™ pode pedir uma explica√ß√£o mas n√£o a resposta para a LLM, utilize por exemplo prompt de exemplo l√° de cima (pode melhorar se quiser, mas nunca pe√ßa a resposta direta):\n",
    "\n",
    "**Pode** olhar a documenta√ß√£o das biblitocas (NumPy, PyTorch, FastAI, mas todas as fun√ß√µes que voc√™ precisa est√° nas **dicas**) e **pode** (aconselhado) olhar o material de aula (slides e refer√™ncias).\n",
    "\n",
    "<details><summary><b>Dica para a resposta</b></summary>\n",
    "<p>\n",
    "\n",
    "A fun√ß√£o `train` pode ser implementada com com apenas 4 linhas de c√≥digo. Sendo que s√£o apenas chamar as fun√ß√µes corretamente que foram implementada anteriormente. N√£o se esque√ßa do `torch.no_grad()`\n",
    "\n",
    "</p>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a818ae69",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "b64c1fd9703b93e2bef270e5aa0c9bd7",
     "grade": false,
     "grade_id": "validacao_sintese",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# validacao_sintese autograded_answer\n",
    "\n",
    "def sintetiza_nova(modelo_nerf: Callable, raios_camera: torch.tensor, pose_camera: torch.tensor, valores_distancias: torch.tensor) -> torch.tensor:\n",
    "    \"\"\" Amostra pontos 3d a valores de dist√¢ncia ao longo do raio especificado a partir da c√¢mera\n",
    "    com a sua pose definida. Retorna esses pontos 3d e as dist√¢ncias entre eles (pontos vizinhos).\n",
    "\n",
    "    Args:\n",
    "      modelo_nerf (Callable): Rede Neural\n",
    "      raios_camera (torch.Tensor): Tensor que determina o ponto de origem do raio, tem shape (..., 3)\n",
    "      pose_camera (torch.Tensor): Tensor que determina a pose da c√¢mera [R | T], tem shape (..., 3, 4)\n",
    "      valores_distancia (torch.Tensor): Valores de dist√¢ncia entre os pontos, shape (..., n_amostras)\n",
    "\n",
    "    Returns:\n",
    "        (torch.Tensor): Sa√≠da de RGB, com valores entre entre 0 e 1, tem shape (..., 3)\n",
    "    \"\"\"\n",
    "    # ESCREVA SEU C√ìDIGO AQUI (pode apagar este coment√°rio, mas n√£o apague esta c√©lula para n√£o perder o ID)\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bea3f9e",
   "metadata": {},
   "source": [
    "Se voc√™ usou uma LLM, escreva a sua conversa com ela aqui nesta pr√≥pria c√©lula de texto (copie a conversa inteira) ou exporte o link da conversa:\n",
    "\n",
    "**Escreva aqui**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83566d4f",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "a1ff3a07d3e431a5d14c4bfeb46df0a8",
     "grade": true,
     "grade_id": "testa_validacao_sintese",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# testa_validacao_sintese autograder_tests 2\n",
    "\n",
    "\n",
    "modelo_nerf = PequenaNerf()\n",
    "modelo_nerf.load_state_dict(torch.load(base_path / 'treinada.pt', map_location=device))\n",
    "n_amostras0 = 256 # Diminua essa linha se a sua GPU reclamar de mem√≥ria\n",
    "img0 = sintetiza_nova(modelo_nerf, raios_camera, poses_valida[1], torch.linspace(2, 6, n_amostras0))\n",
    "assert img0.requires_grad == False\n",
    "assert img0.shape == (H, W, 3)\n",
    "assert torch.linalg.norm(img0[50:55, 50:55] - torch.tensor([\n",
    "        [[0.2988, 0.2144, 0.0685],\n",
    "         [0.3396, 0.2407, 0.0739],\n",
    "         [0.3654, 0.2630, 0.0777],\n",
    "         [0.3996, 0.2907, 0.0830],\n",
    "         [0.4343, 0.3110, 0.0801]],\n",
    "        [[0.3106, 0.2353, 0.0742],\n",
    "         [0.3208, 0.2316, 0.0709],\n",
    "         [0.3539, 0.2520, 0.0723],\n",
    "         [0.3644, 0.2723, 0.0739],\n",
    "         [0.3935, 0.2955, 0.0727]],\n",
    "        [[0.3630, 0.2703, 0.0834],\n",
    "         [0.3621, 0.2685, 0.0743],\n",
    "         [0.3768, 0.2754, 0.0747],\n",
    "         [0.3994, 0.2906, 0.0766],\n",
    "         [0.3657, 0.2873, 0.0737]],\n",
    "        [[0.3786, 0.2704, 0.0733],\n",
    "         [0.3903, 0.2794, 0.0764],\n",
    "         [0.4062, 0.2888, 0.0841],\n",
    "         [0.4312, 0.3090, 0.0858],\n",
    "         [0.4103, 0.3089, 0.0849]],\n",
    "        [[0.3643, 0.2552, 0.0677],\n",
    "         [0.3884, 0.2796, 0.0762],\n",
    "         [0.4132, 0.2855, 0.0860],\n",
    "         [0.4202, 0.3040, 0.0951],\n",
    "         [0.4596, 0.3417, 0.1223]]]\n",
    ")) < 0.2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f53da30",
   "metadata": {},
   "source": [
    "A imagem resultante deve ser parecida com a segunda imagem abaixo do conjunto de valida√ß√£o."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef06ca81",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(img0.cpu())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b5659e7",
   "metadata": {},
   "source": [
    "## Juntando tudo e treinando de verdade!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebf67969",
   "metadata": {},
   "source": [
    "Vamos dar uma olhada no nosso conjunto de treinamento, s√£o 100 imagens com o tamanho de 100 pixels de altura e largura de um trator lego:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65177628",
   "metadata": {},
   "outputs": [],
   "source": [
    "if TRAIN:\n",
    "    fig, axs = plt.subplots(10, 10, figsize=(12,12))\n",
    "    for ax, img in zip(axs.flatten(), imagens_treino.cpu().numpy()):\n",
    "        ax.imshow(img)\n",
    "        ax.axis('off')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "imagens_treino.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f186687",
   "metadata": {},
   "source": [
    "J√° as imagens de valida√ß√£o s√£o apenas 5 (poderiam ter tido mais, mas acabou sendo uma limita√ß√£o desse dataset que j√° peguei formatado):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b20716a",
   "metadata": {},
   "outputs": [],
   "source": [
    "if TRAIN:\n",
    "    fig, axs = plt.subplots(1, 6, figsize=(18,6))\n",
    "    for ax, img in zip(axs.flatten(), imagens_valida.cpu().numpy()):\n",
    "        ax.imshow(img)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51f827ca",
   "metadata": {},
   "source": [
    "E se f√¥ssemos visualizar a sa√≠da de nossa rede:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecc75133",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mostra_predicao_valida(modelo_nerf, n_amostras = 128, plots = None):\n",
    "    valores = torch.linspace(2, 6, n_amostras)\n",
    "    fig, axs = plt.subplots(1, 6, figsize=(12,3))\n",
    "    for i, ax in enumerate(axs.flatten()):\n",
    "        if plots is not None and i == 5:\n",
    "            plt.plot(plots, 'x-')\n",
    "            break\n",
    "        img = sintetiza_nova(modelo_nerf, raios_camera, poses_valida[i], valores)\n",
    "        ax.imshow(img.cpu())\n",
    "        ax.set_title('PSNR %.4f'%(-10 * torch.log10(torch.nn.functional.mse_loss(img, imagens_valida[i])).item()))\n",
    "        ax.axis('off')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "if TRAIN:\n",
    "    mostra_predicao_valida(modelo_nerf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f08337f",
   "metadata": {},
   "source": [
    "Ent√£o s√≥ para juntar todas as contas que tinham ficadas espalhadas pelas quest√µes do notebook temos essa classe do Dataset, que gera a entrada e a sa√≠da esperada para a rede neural da NeRF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "099a4e87",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset:\n",
    "    def __init__(self, imagens, poses, focal, valores_distancias):\n",
    "        self.imagens = imagens\n",
    "        self.poses = poses\n",
    "        self.focal = focal\n",
    "        self.valores_distancias = valores_distancias\n",
    "        self.aleatoriza = (valores_distancias.max().item() - valores_distancias.min().item()) / len(valores_distancias)\n",
    "        H, W = imagens_treino.shape[1:3]\n",
    "        pos_x, pos_y = torch.meshgrid(torch.arange(W), torch.arange(H), indexing='xy')\n",
    "        centro_x, centro_y = W / 2, H / 2\n",
    "        raios_camera = torch.stack([pos_x-centro_x, -(pos_y-centro_y), -focal * torch.ones_like(pos_x)], -1)\n",
    "        self.raios_camera = raios_camera / raios_camera.norm(dim=-1, keepdim=True)\n",
    "\n",
    "    def dataloaders(self, batch_size=1):\n",
    "        return torch.utils.data.DataLoader(self, batch_size=batch_size)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.imagens)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        if idx > len(self.imagens):\n",
    "            raise IndexError(\"Index out of range\")\n",
    "        pts3d_dists = amostra_pontos3d(self.raios_camera, self.poses[idx], self.valores_distancias, self.aleatoriza)\n",
    "        return pts3d_dists, self.imagens[idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32eba8e2",
   "metadata": {},
   "source": [
    "Ent√£o vamos instanciar o nosso dataset de treino e o nosso dataloader, lembra do lab1? Aqui o dataloader simplesmente aglutinaria v√°rias imagens para formar um tamanho de batch maior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "014a496b",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_treino = Dataset(imagens_treino, poses_treino, distancia_focal, torch.linspace(2, 6, 64))\n",
    "dataloader_treino = dataset_treino.dataloaders()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc37321a",
   "metadata": {},
   "source": [
    "Assim podemos instanciar o modelo e carregar pesos aleat√≥rios (s√≥ porque o PyTorch gosta de iniciar os bias com valores aleat√≥rios e eu gosto de zeros, al√©m do desvio padr√£o dos pesos um pouco maior)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2d3cb5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "if TRAIN:\n",
    "    modelo_nerf = PequenaNerf()\n",
    "    modelo_nerf.load_state_dict(torch.load(base_path/'aleatoria.pt', map_location=device))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "722367d0",
   "metadata": {},
   "source": [
    "Veja como √© o modelo aleat√≥rio:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "868d4ee6",
   "metadata": {},
   "outputs": [],
   "source": [
    "if TRAIN:\n",
    "    mostra_predicao_valida(modelo_nerf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2298fa5",
   "metadata": {},
   "source": [
    "E agora fazemos o loop final do treinamento, sinta-se a vontade para ficar treinando mais e ver como os resultados evoluem, altere a quantidade de √©pocas e lembre-se de diminuir a learning_rate se voc√™ for rodar novamente por mais √©pocas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "113011d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "epocas = 10\n",
    "learning_rate = 2e-3\n",
    "\n",
    "optim0 = torch.optim.Adam(modelo_nerf.parameters())\n",
    "sched0 = torch.optim.lr_scheduler.OneCycleLR(optim0, learning_rate, epochs=epocas, steps_per_epoch=len(dataloader_treino), pct_start=0.01)\n",
    "psnrs = []\n",
    "\n",
    "for epoca in range(epocas):\n",
    "  if TRAIN:\n",
    "    custos = train(modelo_nerf, torch.nn.functional.mse_loss, dataloader_treino, optim0, sched0)\n",
    "    psnrs.append(-10 * np.log10(np.mean(custos)))\n",
    "    mostra_predicao_valida(modelo_nerf, plots=psnrs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba0298d3",
   "metadata": {},
   "source": [
    "Para salvar a sua NeRF:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d962b0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "if TRAIN:\n",
    "    torch.save(modelo_nerf.state_dict(), 'nova_treinada.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44c46e6e",
   "metadata": {},
   "source": [
    "Para carregar:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03e6e13d",
   "metadata": {},
   "outputs": [],
   "source": [
    "if TRAIN:\n",
    "    modelo_nerf.load_state_dict(torch.load('nova_treinada.pt', map_location=device))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d52acec",
   "metadata": {},
   "source": [
    "Agora mexa nas barrinhas abaixo para mudar a sua posi√ß√£o de renderiza√ß√£o (n√£o se preocupe com o c√≥digo abaixo):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d13be2e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial.transform import Rotation\n",
    "from ipywidgets import interactive, widgets\n",
    "\n",
    "n_amostrasx = 128\n",
    "\n",
    "def pose_de_angulos(phi, theta, distancia):\n",
    "    rot = Rotation.from_euler('ZYX', (theta, 0, phi), degrees=True).as_matrix()\n",
    "    pos = (np.array([0, 0, distancia]) @ rot.T)[:,None]\n",
    "    return torch.tensor(np.hstack([rot, pos]), dtype=torch.float32)\n",
    "\n",
    "def mostra(**kwargs):\n",
    "    pose_camera = pose_de_angulos(**kwargs)\n",
    "    img = sintetiza_nova(modelo_nerf, raios_camera, pose_camera, torch.linspace(1, 8, n_amostrasx)).cpu()\n",
    "    plt.figure(9, figsize=(6,6))\n",
    "    plt.axis('off')\n",
    "    plt.imshow(img)\n",
    "    plt.show()\n",
    "\n",
    "interactive_plot=None\n",
    "if TRAIN:\n",
    "    interactive_plot = interactive(mostra, \n",
    "          phi = widgets.FloatSlider(value=40, min=-90, max=90, step=.01),\n",
    "          theta = widgets.FloatSlider(value=60, min=0, max=360, step=.01),\n",
    "          distancia = widgets.FloatSlider(value=3, min=2, max=6, step=.01))\n",
    "interactive_plot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "750e007b",
   "metadata": {},
   "source": [
    "E agora vamos gerar um v√≠deo dessa visualiza√ß√£o girando em torno do objeto:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf9d9184",
   "metadata": {},
   "outputs": [],
   "source": [
    "html = None\n",
    "if TRAIN:\n",
    "    from IPython.display import HTML\n",
    "    from base64 import b64encode\n",
    "    import imageio\n",
    "\n",
    "    video = imageio.get_writer('output.mp4')\n",
    "    for theta in tqdm(np.linspace(0., 360, 120, endpoint=False)):\n",
    "        pose_camera = pose_de_angulos(60, theta, 3.5)\n",
    "        img = sintetiza_nova(modelo_nerf, raios_camera, pose_camera, torch.linspace(2, 6, n_amostrasx)).cpu().numpy()\n",
    "        video.append_data((255*img).astype(np.uint8))\n",
    "    video.close()\n",
    "\n",
    "    mp4 = open('output.mp4','rb').read()\n",
    "    data_url = \"data:video/mp4;base64,\" + b64encode(mp4).decode()\n",
    "    html = HTML(\"\"\"\n",
    "    <video width=400 controls autoplay loop>\n",
    "          <source src=\"%s\" type=\"video/mp4\">\n",
    "    </video>\n",
    "    \"\"\" % data_url)\n",
    "html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ba8745c",
   "metadata": {},
   "source": [
    "Usando essa forma bronca de amostragem precisariam-se muito tempo (v√°rias horas) para poder treinar uma NeRF igual ao do paper original com a resolu√ß√£o maior. Mas essas imagens pequena e com essa abordagem simplificada d√° para ver bem o que a rede consegue treinar nesse tempo."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "123782df",
   "metadata": {},
   "source": [
    "# Seus dados e feedback aqui:\n",
    "\n",
    "Coloque o seu feedback sobre o lab aqui para podermos melhor√°-lo para as pr√≥ximas turmas e tamb√©m 'calibrar' os pr√≥ximos labs (idealmente os 80% dos alunos terminar em bem menos de 3h)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a48dd344",
   "metadata": {},
   "source": [
    "Preencha as seguintes vari√°veis com a quantidade de horas gasta no lab, a dificuldade percebida e a nota esperada (pode apagar o `raise` e o coment√°rio):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e09baf2b",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "bd7bcb89e16451eac905de60a4e9033b",
     "grade": true,
     "grade_id": "meta_eval",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# meta_eval manual_graded_answer 0\n",
    "\n",
    "horas_gastas = None    # 1.5   - N√∫mero float com a quantidade de horas \n",
    "dificuldade_lab = None # 0     - N√∫mero float de 0 a 10 (inclusive)\n",
    "nota_esperada = None   # 10    - N√∫mero float de 0 a 10 (inclusive)\n",
    "\n",
    "# ESCREVA SEU C√ìDIGO AQUI (pode apagar este coment√°rio, mas n√£o apague esta c√©lula para n√£o perder o ID)\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c7890cc",
   "metadata": {},
   "source": [
    "Escreva abaixo (na c√©lula discursiva) outros coment√°rios e feedbacks sobre o lab, pode ser em termos gerais, ou espec√≠fico sobre alguma quest√£o. Se tiver alguma d√∫vida que restou tamb√©m pode colocar aqui.\n",
    "\n",
    "Quaisquer erros, por menor que forem (portugu√™s, o jupyter n√£o tem corretor gramatical para portugu√™s e a extens√£o do navegador n√£o pega na c√©lula), pode comentar abaixo para podermos melhorar e corrigir."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bff57274",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "1f2631795cfc36bf2f1ec9ad84af8d32",
     "grade": true,
     "grade_id": "meta_eval_discursivo",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "ESCREVA A SOLU√á√ÉO ABAIXO (n√£o mude essa primeira linha):\n",
    "**ATEN√á√ÉO**\n",
    "\n",
    "**ATEN√á√ÉO**\n",
    "\n",
    "**ATEN√á√ÉO**\n",
    "\n",
    "**ATEN√á√ÉO**\n",
    "\n",
    "**QUEST√ÉO DISCURSIVA**\n",
    "\n",
    "ESCREVA SUA RESPOSTA AQUI (n√£o apague esta c√©lula para n√£o perder o ID)\n",
    "\n",
    "**ATEN√á√ÉO**\n",
    "\n",
    "**ATEN√á√ÉO**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bc1f42a",
   "metadata": {},
   "source": [
    "Fim do laborat√≥rio."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
